{"text": "This study aimed to create a scalable and accurate method for predicting individual patient costs by automatically identifying hidden temporal patterns within multivariate time series data from patient insurance claims using a convolutional neural network (CNN).  Utilizing three years of medical and pharmacy claims data (2013-2016), the model was trained on the first two years to predict costs in the third year.  The data, comprising multivariate time series of cost, visit, and medical features, was transformed into images representing patients' health status. These images were then processed by a CNN with a customized architecture consisting of three convolution and pooling blocks employing LReLU activation and tailored kernel sizes. The learned temporal patterns served as input to a fully connected layer. Findings indicated that the proposed CNN architecture significantly enhanced individual-level healthcare cost prediction by effectively learning temporal patterns, outperforming methods reliant on predefined pattern shapes. The integration of temporal patterns derived from medical, visit, and cost data proved crucial for prediction accuracy, with three-month data patterns yielding the highest accuracy. The study also highlighted the need for unique CNN designs to accommodate the distinct characteristics of patient images extracted from multivariate time series data."}
{"text": "Accurate prediction of a dynamical system's future states, its response to shocks, and its underlying causal network necessitates the incorporation of nonlinearity, a capability lacking in prevalent linear methods like Vector Autoregression (VAR). To address this limitation, we propose a vector autoencoder nonlinear autoregression neural network (VANAR) that automatically extracts time series features and estimates functional forms. VANAR's performance is evaluated through three metrics: forecast accuracy, causality detection, and impulse response modeling, using both a simulated nonlinear chaotic system and empirical Philippine macroeconomic data. Results demonstrate VANAR's superior performance compared to VAR in forecasting and causality tests, consistently outperforming state-of-the-art models like SARIMA and TBATS. While both VANAR and VAR struggled to predict shocked trajectories in the nonlinear chaotic system, VANAR exhibited robustness in modeling diverse dynamics, encompassing chaotic, high-noise, low-data, and macroeconomic systems."}
{"text": "This thesis proposes novel generator architectures for Boundary Equilibrium Generative Adversarial Networks, inspired by Learning from Simulated and Unsupervised Images through Adversarial Training. These architectures eliminate the reliance on a noise-based latent space, instead functioning as refiner networks to enhance the photorealism of provided synthetic images. By removing noise injection and substituting it with an image-based concept, this approach aims to address the ambiguities surrounding latent space properties. The resulting generator architecture offers greater flexibility and simplicity, enabling control over the balance between refinement constraints and expressive capabilities. Unlike existing methods, this architecture does not necessitate paired or unpaired datasets of real and synthetic images for training; a comparatively smaller set of real images is sufficient."}
{"text": "While autonomous robotic skill learning promises to equip general-purpose robots with diverse behavioral repertoires without extensive manual programming, existing methods often compromise by relying on manually designed representations, human demonstrations, specialized environments, or lengthy training durations. This paper introduces a novel reinforcement learning algorithm for acquiring manipulation skills, enabling the training of versatile neural network policies with minimal human intervention while ensuring rapid and efficient learning in unpredictable environments. Our method extends the guided policy search (GPS) framework, transforming reinforcement learning into supervised learning guided by a computational teacher (without human demonstrations). Unlike previous GPS approaches that necessitate consistent initial states for episode resets, our method accommodates randomized initial states, broadening its applicability to environments lacking deterministic resets. Through simulation experiments, we demonstrate that our method achieves comparable sample efficiency to prior GPS techniques when training high-dimensional neural network policies, and we present real-world results obtained with a PR2 robotic manipulator."}
{"text": "This paper introduces a hierarchical image segmentation technique that constructs a disaffinity graph on the input image, performs over-segmentation using watershed basins, creates a new graph based on these basins, and finally merges basins using a size-aware modification of single linkage clustering. The method's quasilinear runtime enables efficient segmentation of large images, and its effectiveness is demonstrated through application to the complex task of segmenting 3D electron microscopic brain images."}
{"text": "This paper investigates the challenge of identifying outlier arms in multi-armed bandit scenarios, a problem with significant implications for fields like finance, healthcare, and online advertising.  Unlike previous research, our focus is on detecting generic outlier arms or groups, characterized by expected rewards that can be higher, lower, or comparable to those of typical arms. We begin by formally defining these generic outlier arms and groups, and then introduce GOLD, a novel pulling algorithm designed to pinpoint them. GOLD constructs a dynamic neighborhood graph using upper confidence bounds to distinguish the behavioral patterns of outliers from normal arms. We conduct a thorough performance analysis of GOLD and demonstrate its effectiveness through experiments on both synthetic and real-world datasets. Our results show that GOLD achieves 98% accuracy while reducing exploration costs by an average of 83% compared to existing state-of-the-art methods."}
{"text": "This work introduces a novel approach for simultaneously learning a 3D face parametric model and reconstructing 3D faces from multiple data sources. Existing methods typically focus on learning 3D face models from a single source, such as scanned data or images captured in uncontrolled environments. While 3D scans provide precise facial geometry, the acquisition process is costly and datasets are often limited in size. Conversely, in-the-wild images are abundant but lack explicit geometric information. To address this, we propose a method that leverages scanned face data, in-the-wild images, and a large collection of RGB-D images acquired using an iPhone X to bridge the gap between these sources. Our experimental findings indicate that incorporating training data from diverse sources leads to a more robust and powerful face model."}
{"text": "Differentiable Neural Architecture Search (NAS), a popular method known for its efficiency and simplicity, jointly optimizes model weights and architecture parameters within a weight-sharing supernet using gradient-based algorithms.  Traditionally, operations with the largest architecture parameters are selected to form the final architecture, assuming parameter magnitude reflects operation strength. However, our empirical and theoretical analysis demonstrates that this assumption is flawed, as architecture parameter magnitude does not necessarily correlate with an operation's contribution to supernet performance. We propose a perturbation-based architecture selection method that directly measures each operation's influence on the supernet.  Re-evaluating several differentiable NAS methods with this new selection approach consistently yields significantly improved architectures. Moreover, our findings suggest that the proposed method mitigates several failure modes of DARTS, indicating that the poor generalization often observed in DARTS may be largely attributed to the limitations of magnitude-based architecture selection rather than solely the supernet optimization process."}
{"text": "Accurate edge detection is fundamental to object detection in image processing because edges delineate object boundaries, distinguishing objects from backgrounds and separating overlapping objects. Precise edge identification enables object localization and the measurement of basic properties such as area, perimeter, and shape. As computer vision relies on object identification and classification, edge detection is a crucial tool. This study compared two edge detectors employing distinct methodologies, evaluating their performance across various scenarios to ascertain the optimal detector for different conditions."}
{"text": "Traditional causal discovery methods require training a new model for each distinct causal graph encountered, neglecting the shared information present in samples, such as the dynamics governing causal relationships. To address this limitation, we introduce Amortized Causal Discovery, a framework that exploits these shared dynamics to learn causal relations from time-series data. This approach enables the training of a single, amortized model capable of inferring causal relationships across samples originating from different causal graphs, effectively utilizing the shared information. Through experimental validation using a variational model implementation, we demonstrate significant performance enhancements in causal discovery and showcase the framework's extensibility to handle hidden confounding."}
{"text": "Neural attention (NA) is a crucial element in sequence-to-sequence models, achieving top performance in demanding tasks like abstractive document summarization (ADS) and video captioning (VC). NA mechanisms generate context vectors, which are weighted combinations of deterministic input sequence encodings, dynamically selected across extended temporal ranges. Drawing inspiration from recent advancements in amortized variational inference (AVI), this work proposes treating the context vectors produced by soft-attention (SA) models as latent variables, with approximate finite mixture model posteriors inferred using AVI. We hypothesize that this approach may enhance generalization capabilities, consistent with the findings of existing AVI applications in deep networks. To demonstrate our method, we implement and empirically evaluate it on challenging ADS, VC, and machine translation (MT) benchmarks, showcasing its superior performance compared to state-of-the-art methods."}
{"text": "Optimizing information gain through strategic measurements is crucial for understanding unknown states. This thesis presents a novel dynamic programming algorithm, derived from fundamental principles, that identifies a sequence of highly informative measurements by iteratively maximizing the entropy of potential measurement outcomes. This algorithm empowers autonomous agents or robots to determine the most advantageous locations for subsequent measurements, effectively planning a path that corresponds to an optimal sequence of informative measurements. Its versatility extends to states and controls that are either continuous or discrete, and agent dynamics that encompass both stochastic and deterministic models, including Markov decision processes. Leveraging recent advancements in approximate dynamic programming and reinforcement learning, such as on-line approximations like rollout and Monte Carlo tree search, enables real-time solutions to the measurement task. The resulting near-optimal solutions, encompassing non-myopic paths and measurement sequences, consistently outperform conventional greedy heuristics, such as maximizing the entropy of individual measurement outcomes. This superiority is exemplified in a global search problem, where on-line planning coupled with extended local search achieves a 50% reduction in the number of required measurements."}
{"text": "This research focuses on creating and implementing an automated anomaly detection algorithm specifically designed for meteorological time-series data. The proposed approach involves constructing an ensemble of anomaly detectors and incorporating adaptive threshold selection based on synthetically generated anomalies. The effectiveness of this method is demonstrated through its integration into the \"Minimax-94\" road weather information system."}
{"text": "This research investigates Salient Object Subitizing, which involves predicting the presence and quantity of salient objects within an image by utilizing global visual cues. Motivated by the human capacity for rapid and precise enumeration of objects within the subitizing range (1-4), we introduce a dataset of approximately 14,000 everyday images annotated with salient object counts through crowdsourcing. Our end-to-end trained Convolutional Neural Network (CNN) model achieves human-level accuracy in identifying images containing zero or one salient object. Furthermore, the model demonstrates significantly above-chance performance for images with multiple salient objects without requiring object localization. We also propose a technique to enhance CNN subitizing model training by incorporating synthetic images. Experimental results validate the accuracy and generalizability of our CNN subitizing model, highlighting its potential applications in salient object detection and image retrieval."}
{"text": "This paper investigates a dynamic assortment planning problem over a finite selling season of length T, where a seller offers a set of substitutable products to arriving customers who make purchase decisions based on a nested logit choice model. The seller aims to maximize expected revenue, equivalent to minimizing worst-case expected regret, while facing the challenge of unknown product utilities.  Departing from the prevalent multinomial logit model, this work leverages the nested logit model's ability to capture hierarchical choice behavior. By exploiting the revenue-ordered structure within each nest, a novel upper confidence bound (UCB) policy with an aggregated estimation scheme is developed. This policy simultaneously learns customer preferences and dynamically adjusts assortments. The policy achieves an accumulated regret of  \\tilde(), where M is the number of nests and N is the number of products per nest. A lower bound of \\Omega() is established, demonstrating the near-optimality of the upper bound when T is significantly larger than M and N. For scenarios with large N, a discretization heuristic is proposed to enhance algorithm performance. Numerical experiments validate the empirical effectiveness of the proposed algorithms."}
{"text": "This work presents a novel plug-in estimator for conditional average treatment effects (CATEs) in settings where treatments exhibit graph structures, such as molecular graphs representing drugs. Under a mild assumption regarding the treatment effect, our estimator breaks down the CATE estimation task into a series of tractable optimization subproblems. This approach offers two key advantages: (a) it effectively isolates the causal estimands, thereby mitigating regularization bias, and (b) it accommodates the use of diverse models for learning the underlying relationships. Through experiments conducted on small-world and molecular graphs, we demonstrate the superiority of our method over existing approaches and its robustness to varying degrees of selection bias. The implementation of our estimator is publicly available online."}
{"text": "While previous action recognition research has focused on activities as singular events within videos, recent work exploring actions as compositions of atomic actions has demonstrated potential for enhanced understanding, facilitated by datasets with such annotations. Despite this progress, there is a scarcity of research investigating action composition across multiple viewpoints and modalities. To address this gap, we present Home Action Genome (HOMAGE), a multi-view, multi-modal action dataset annotated with hierarchical activity and atomic action labels, along with dense scene composition labels. Utilizing this rich dataset, we propose Cooperative Compositional Action Understanding (CCAU), a cooperative learning framework for hierarchical action recognition that incorporates compositional action elements. CCAU consistently outperforms existing methods across all modalities. Moreover, we showcase the effectiveness of co-learning compositions in few-shot action recognition, achieving a 28.6% mAP with only a single sample."}
{"text": "The reconstruction of seismic data with missing traces is a persistent challenge in seismic data processing, often addressed using rank reduction techniques that necessitate a priori knowledge of the seismic data rank.  Determining this rank for field data is typically time-consuming and involves manual approximation. While deep learning methods offer an alternative, they require extensive training datasets, which can be difficult to acquire due to practical limitations. To overcome these challenges, this work introduces a novel unsupervised learning method leveraging the intrinsic properties of a U-net convolutional neural network without the need for training datasets. This approach utilizes a single undersampled seismic data input, allowing the network to autonomously exploit the deep seismic prior of the data, thereby simplifying the reconstruction process.  Furthermore, the method is capable of handling both irregular and regular seismic data. The performance of the proposed DSPRecon algorithm was evaluated using synthetic and field data, and its advantages were assessed by comparing it to the singular spectrum analysis (SSA) method for irregular data reconstruction and the de-aliased Cadzow method for regular data reconstruction. Experimental results demonstrated that DSPRecon outperformed both SSA and Cadzow methods, achieving recovered signal-to-noise ratios (SNRs) of 32.68 dB and 35.91 dB, respectively, compared to 19.11 dB for SSA and 15.32 dB for Cadzow."}
{"text": "This paper presents an end-to-end deep learning framework for generating high-quality bokeh effects in images. The framework leverages a monocular depth estimation network to blend the original image with smoothed versions, creating the desired out-of-focus effect.  Performance is evaluated against a saliency detection baseline and other methods from the AIM 2019 Challenge on Bokeh Effect Synthesis, with extensive experiments analyzing different algorithm components. The proposed lightweight network processes HD images in 0.03 seconds and achieved second place in the AIM 2019 Bokeh effect challenge-Perceptual Track."}
{"text": "While Domain Adaptation (DA) techniques have shown promise in various machine learning and computer vision applications, including classification, detection, and segmentation, their application to 3D point cloud data remains limited. The unique characteristics of point cloud data, particularly its rich spatial geometric information and the contribution of regional geometric structures to object semantics, pose a challenge for existing DA methods that primarily focus on global feature alignment and neglect local geometric details. To address this gap, we introduce PointDAN, a novel 3D Domain Adaptation Network for point cloud data. PointDAN leverages a multi-level approach to align both global and local features. For local alignment, we propose a Self-Adaptive (SA) node module with an adjustable receptive field to capture discriminative local structures across domains. A node-attention module is further incorporated to weigh the relationships between SA nodes, enabling hierarchical feature representation. Global alignment is achieved through an adversarial-training strategy that learns and aligns global features. Recognizing the lack of a standardized benchmark for 3D point cloud DA, we construct PointDA-10, a general benchmark derived from ModelNet, ShapeNet, and ScanNet datasets, for cross-domain 3D object classification. Comprehensive experiments on PointDA-10 demonstrate the superior performance of our model compared to state-of-the-art general-purpose DA methods."}
{"text": "Discrete attention mechanisms, while effective in identifying salient features in computer vision tasks, struggle to capture the complexity of regions of interest in images. Continuous attention models, though offering a more natural representation, often rely on unimodal densities, limiting their ability to handle complex or multi-part regions. This paper proposes a novel continuous attention mechanism that utilizes multimodal densities, specifically mixtures of Gaussians, to address this limitation. By employing the EM algorithm for region clustering and a description length penalty for component selection, our model effectively captures complex attention patterns. The resulting multimodal densities, expressed as a linear combination of unimodal mechanisms, allow for efficient backpropagation. Experiments on the VQA-v2 dataset demonstrate competitive performance and a more human-like attention selection compared to unimodal approaches. Furthermore, the multimodal attention maps provide enhanced interpretability, enabling the model to automatically distinguish objects from background in intricate scenes."}
{"text": "This research introduces a new 3D face recognition algorithm that combines a deep convolutional neural network (DCNN) with a 3D augmentation technique. While 2D face recognition has benefited greatly from the power of deep neural networks and extensive labeled datasets, training effective deep features for 3D face recognition is challenging due to the scarcity of large-scale 3D face data. This paper demonstrates that transfer learning, by fine-tuning a CNN pre-trained on 2D face images with a smaller set of 3D facial scans, can be successfully applied to 3D face recognition. Additionally, a novel 3D face augmentation technique is proposed, capable of generating various facial expressions from a single 3D scan. The proposed method achieves impressive recognition accuracy on the Bosphorus, BU-3DFE, and 3D-TEC datasets, eliminating the need for hand-crafted features. Furthermore, 3D identification using these deep features exhibits scalability for large databases."}
{"text": "While deep learning has advanced automatic colorization, these methods struggle with few-shot learning due to their reliance on extensive training datasets. To address this limitation, we introduce MemoPainter, a novel memory-augmented colorization model capable of generating high-quality colorizations from limited data, including rare instances. Furthermore, we propose a threshold triplet loss that facilitates unsupervised training of memory networks without requiring class labels. Our experimental results demonstrate MemoPainter's superior performance in both few-shot and one-shot colorization tasks."}
{"text": "Although generative adversarial networks (GANs) have excelled in realistic image generation, their potential for tasks beyond synthesis remains largely untapped. This work investigates whether GANs acquire meaningful structural knowledge of objects during the generation process and proposes a novel approach for semantic part segmentation using GANs. This method leverages a pre-trained GAN to extract pixel-wise representations from input images, which are then used as feature vectors for a segmentation network, requiring only a single labeled example alongside an unlabeled dataset. Experimental results demonstrate the discriminative power of GAN representations, achieving segmentation performance comparable to supervised baselines trained on significantly larger labeled datasets. This repurposing of GANs suggests a new paradigm for unsupervised representation learning with broad applicability to various tasks. Further details and results are available at https://repurposegans.github.io/."}
{"text": "This thesis presents a novel, efficient hyperparameter optimization algorithm drawing inspiration from Boolean function analysis. Designed for high-dimensional settings, such as training neural networks with numerous hyperparameters, the algorithm iteratively applies compressed sensing techniques for orthogonal polynomials, relying solely on uniform hyperparameter sampling for easy parallelization. Experiments on Cifar-10 demonstrate that our algorithm surpasses state-of-the-art tools like Hyperband and Spearmint, achieving significantly better solutions, sometimes even outperforming manual tuning. In terms of overall runtime, our method is at least ten times faster than Hyperband and Bayesian Optimization, and eight times faster than Random Search. Furthermore, our approach offers provable guarantees and delivers the first sample complexity improvements for learning decision trees in over two decades, including the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity."}
{"text": "This thesis introduces Layer Saturation, a metric quantifying the proportion of eigenvalues required to capture 99% of the variance within a neural network layer's latent representations. Leveraging spectral analysis, Layer Saturation offers an efficient method for real-time analysis of learned representations during training. The study explores the behavior of Layer Saturation across diverse neural architectures and tasks, highlighting its potential applications. Furthermore, a correlation between Layer Saturation and the generalization and predictive capabilities of neural networks is demonstrated."}
{"text": "Recent research has extensively explored situational awareness as a crucial aspect of connected and autonomous vehicles (CAVs), recognizing its direct impact on driver safety, which hinges on the robustness, reliability, and scalability of these systems. Cooperative mechanisms leveraging high-speed wireless vehicular networks have emerged as a solution to enhance situational awareness by addressing challenges such as occlusion and limited sensor range. However, network capacity constraints the amount of information shared among cooperative entities. Building upon our previous work on feature sharing, which aimed to balance computational and communication loads, this work introduces a mechanism for flexible adaptation to varying communication channel capacities and a novel decentralized shared data alignment method to further improve cooperative object detection performance. The efficacy of the proposed framework is validated through experiments on the Volony dataset, demonstrating superior average precision compared to our previous cooperative object detection method (FS-COD)."}
{"text": "This work presents a novel and efficient deep structured model learning approach for tasks such as semantic image segmentation. Our method leverages deep Convolutional Neural Networks (CNNs) to estimate messages within message passing inference for Conditional Random Fields (CRFs), eliminating the need to learn or evaluate traditional potential functions. This significantly enhances learning efficiency by avoiding expensive inference calculations during stochastic gradient descent. Moreover, our CNN message estimators have a reduced number of parameters compared to conventional CNN potential functions in CRFs, making them more scalable for problems with numerous classes. We demonstrate the effectiveness of our approach by achieving a state-of-the-art intersection-over-union score of 73.4 on the PASCAL VOC 2012 test set using only the VOC training images."}
{"text": "This thesis introduces Earliness-Aware Deep Convolutional Networks (EA-ConvNets), a novel end-to-end deep learning framework designed for early time series classification. Unlike conventional methods that rely on pre-defined features, EA-ConvNets jointly learns a deep hierarchy of features to capture salient characteristics within each time series, while simultaneously employing a dynamic truncation model to prioritize early segments. This enables EA-ConvNets to achieve highly accurate early predictions, surpassing existing state-of-the-art early time series classification methods and demonstrating competitive performance against standard time series classification algorithms. To our knowledge, EA-ConvNets is the first framework to leverage data-driven (deep) feature learning for early time series classification. Extensive experiments on benchmark datasets confirm the superior predictive capabilities of our method compared to existing state-of-the-art early time series classification techniques. Furthermore, the learned deep shapelet-based features exhibit high interpretability, providing valuable insights into the underlying characteristics of time series data."}
{"text": "While instance-based interpretation methods are well-studied for supervised learning, their application to unsupervised learning remains under-explored. This paper investigates influence functions, a prevalent instance-based interpretation method, within the context of variational auto-encoders (VAEs), a class of deep generative models. We formally define the counterfactual question addressed by influence functions in this setting and, through theoretical analysis, explore their insights into the influence of training samples on traditional unsupervised learning methods. We then present VAE-TracIn, a computationally efficient and theoretically grounded solution based on Pruthi et al. [28], for VAEs. Finally, we evaluate VAE-TracIn on several real-world datasets using extensive quantitative and qualitative analysis."}
{"text": "While deep generative models excel at modeling continuous data, generating discrete structures with formal grammars and semantics, such as computer programs and molecules, remains a challenge.  The problem of generating data that is both syntactically and semantically correct is largely unsolved. Drawing inspiration from compiler theory, where syntax and semantics are checked through syntax-directed translation (SDT), we introduce a novel syntax-directed variational autoencoder (SD-VAE) that incorporates stochastic lazy attributes. This method integrates the offline SDT check into real-time guidance, constraining the decoder. Unlike existing methods, our approach enforces constraints on the output space, ensuring that the generated output is not only syntactically valid but also semantically meaningful. We evaluate our model's performance in programming language and molecule applications, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness of incorporating syntactic and semantic constraints into discrete generative models, significantly outperforming current state-of-the-art approaches."}
{"text": "This study revisits the NOTEARS framework for Bayesian network learning through continuous optimization.  We extend existing algebraic acyclicity characterizations to encompass matrix polynomials. Focusing on a one-parameter-per-edge setting, we demonstrate that the Karush-Kuhn-Tucker (KKT) optimality conditions for the standard NOTEARS formulation are unsatisfiable except in a trivial scenario, elucidating the observed behavior of the corresponding algorithm. Subsequently, we derive the KKT conditions for an equivalent reformulation, proving their necessity and linking them to explicit constraints on edge absence. When the score function is convex, these KKT conditions also guarantee local minimality despite the non-convexity of the constraint. Leveraging these KKT conditions, we propose a local search post-processing algorithm that significantly and consistently enhances the structural Hamming distance of all evaluated algorithms, typically by a factor of 2 or more. Notably, certain combinations of local search with NOTEARS achieve both higher accuracy and improved efficiency compared to the original NOTEARS method."}
{"text": "While Braille has enabled literacy for the visually impaired, its limited accessibility to non-Braille readers has created a communication barrier. To bridge this gap, researchers have explored Optical Braille Recognition techniques for converting Braille documents into natural language. This work focuses on addressing this communication disparity within academic settings by developing an affordable and efficient method for translating the personal documents of blind students using a smartphone camera. A dot detection mechanism based on the Hough transform, robust against skewness, noise, and other distortions, is proposed for processing Braille images. Subsequently, a distance-based clustering algorithm groups detected dots into Braille cells, and standard physical parameters are estimated for feature extraction and classification as natural language characters. Evaluation of this technique on a dataset of 54 Braille scripts achieved an accuracy of 98.71%."}
{"text": "This work introduces a novel approach for relocalization in large-scale point clouds, seamlessly integrating global place recognition and local 6DoF pose refinement. A Siamese network is designed to jointly learn 3D local feature detection and description directly from raw 3D points, leveraging FlexConv and Squeeze-and-Excitation (SE) modules to capture multi-level geometric information and channel-wise relations within the learned local descriptor. Unsupervised discriminativeness prediction of local descriptors facilitates 3D keypoint detection. A global descriptor is generated by aggregating learned local descriptors using an effective attention mechanism, enabling the inference of both local and global 3D descriptors in a single forward pass. Extensive experiments on various benchmarks demonstrate the method's competitive performance in both global point cloud retrieval and local point cloud registration, surpassing state-of-the-art approaches. The generalizability and robustness of the learned 3D keypoints are further validated through successful registration of point clouds generated by a visual SLAM system without fine-tuning. Code and related materials are accessible at https://vision.in.tum.de/research/vslam/dh3d."}
{"text": "This paper introduces GeoCLR, a self-supervised method for training deep-learning Convolutional Neural Networks (CNNs) using georeference information. GeoCLR leverages the spatial proximity of images by contrasting similar image pairs from nearby locations with dissimilar pairs from distant locations, capitalizing on the assumption that images taken in close proximity often share visual similarities, a premise well-suited to seafloor robotic imaging applications with limited image footprints and overlapping coverage. This computationally efficient method enables on-mission training during multi-day AUV deployments using readily available resources. We demonstrate GeoCLR's effectiveness on a dataset of ~86k AUV images for habitat classification, showcasing its ability to guide human annotation efforts and improve classification accuracy by an average of 11.8% compared to state-of-the-art transfer learning with the same CNN and equivalent human annotation input."}
{"text": "Effective video understanding requires models to grasp the intricate relationship between static scene elements and their temporal evolution.  Such models should be capable of predicting future scene progressions from a single image and, conversely, explaining videos by decomposing them into their static content and residual dynamic information. This suggests a bijective mapping between videos and the combination of static content and residual information, contrasting with stochastic image-to-video synthesis methods that generate arbitrary video continuations. Our approach leverages a conditional invertible neural network (cINN) to model static and dynamic video characteristics independently, enabling a one-to-one mapping between residual vectors and videos with stochastic sampling outcomes. Experiments across four diverse video datasets validate the effectiveness of our method in generating high-quality and diverse synthetic videos. Our project page is available at https://bit.ly/3t66bnU."}
{"text": "The burgeoning interest in transformers has highlighted their potential as versatile models for computer vision tasks, including classification, detection, and segmentation.  Departing from the prevalent focus on discriminative models, this study investigates transformers in the context of generative adversarial networks (GANs). This work presents TransGAN, a pioneering GAN architecture devoid of convolutions, relying solely on transformer-based components. TransGAN comprises a memory-efficient transformer-based generator that progressively enhances feature resolution and a multi-scale discriminator designed to capture both semantic context and low-level textures. To address memory constraints and enable high-resolution generation, a novel grid self-attention module is introduced. Furthermore, a specialized training regimen incorporating data augmentation, modified normalization, and relative position encoding is developed to mitigate training instability. TransGAN achieves competitive performance against state-of-the-art GANs employing convolutional backbones, attaining a new state-of-the-art inception score of 10.43 and FID of 18.28 on STL-10, surpassing StyleGAN-V2. On higher-resolution datasets like CelebA-HQ and LSUN-Church, TransGAN generates diverse, high-fidelity images with intricate texture details.  In addition, the study delves into the unique characteristics of transformer-based generation models by visualizing training dynamics and comparing them to convolutional counterparts. The code is accessible at https://github.com/VITA-Group/TransGAN."}
{"text": "This paper presents SalsaNext, a real-time uncertainty-aware semantic segmentation method for full 3D LiDAR point clouds. Building upon SalsaNet [1], SalsaNext employs an encoder-decoder architecture with a novel context module, replacing ResNet encoder blocks with residual dilated convolution stacks featuring gradually increasing receptive fields and incorporating a pixel-shuffle layer in the decoder.  Furthermore, stride convolution is replaced with average pooling, and central dropout is applied. To directly optimize the Jaccard index, a weighted cross-entropy loss is combined with the Lovasz-Softmax loss [2].  Bayesian treatment is integrated to compute epistemic and aleatoric uncertainties for each point.  Extensive quantitative evaluation on the Semantic-KITTI dataset [3] demonstrates SalsaNext's superior performance compared to state-of-the-art semantic segmentation networks, achieving a top ranking on the Semantic-KITTI leaderboard. The source code is publicly available at https://github.com/TiagoCortinhal/SalsaNext."}
{"text": "Achieving optimal glycemic control in critical care settings is crucial but complex due to the lack of personalized strategies. This research leverages data-driven policies to determine optimal target blood glucose levels for severely ill septic patients, providing clinicians with personalized guidance. Patient states were encoded using a sparse autoencoder, and a reinforcement learning framework with policy iteration was employed to learn the optimal policy from existing glycemic data. Analysis of the expected return following the learned policy revealed a correlation between actual blood glucose values and 90-day mortality rates. This suggests that implementing the learned optimal policy could potentially decrease the estimated 90-day mortality rate by 6.3%, from 31% to 24.7%. These findings demonstrate the potential of reinforcement learning, combined with appropriate patient state encoding, to identify optimal glycemic trajectories and facilitate personalized glycemic control strategies for septic patients."}
{"text": "This thesis proposes Feature Selection Explore and Exploit (FS-EE), an algorithm designed to automatically identify and utilize only the essential features from a feature vector representation of the real world in reinforcement learning. FS-EE operates within the framework of Factored Markov Decision Processes and, under reasonable assumptions, exhibits a sample complexity that scales with the in-degree of the dynamics associated solely with the necessary features, rather than the in-degree of all features. This targeted approach can lead to significantly improved sample complexity when the in-degree of the necessary features is smaller than the in-degree of the complete feature set."}
{"text": "Depth estimation using a single camera is crucial for autonomous systems but often relies on expensive multi-beam LiDARs or suffers from limitations like scale ambiguity when using camera-only methods. This paper presents LiDARTouch, a novel framework that leverages a monocular camera and a lightweight LiDAR (e.g., with 4 beams) to estimate dense depth maps without requiring dense ground-truth depth. LiDARTouch utilizes the sparse LiDAR input in three ways: as an additional input to the model, in a self-supervised LiDAR reconstruction objective, and for pose estimation.  Experiments on the KITTI dataset demonstrate that LiDARTouch achieves state-of-the-art performance in self-supervised depth estimation, highlighting the effectiveness of integrating sparse LiDAR data with visual features. Furthermore, the use of a few-beam LiDAR mitigates scale ambiguity and infinite-depth problems inherent in camera-only approaches, and the framework shows the adaptability of supervised depth-completion methods to a self-supervised setting with minimal LiDAR input."}
{"text": "This thesis presents a reinforcement learning approach to the soccer dribbling problem, where an agent must navigate a region while maintaining ball possession against an opponent.  The opponent employs a fixed policy, while the dribbler learns optimal actions through reinforcement learning.  The state space is defined using relevant variables, and domain knowledge is incorporated through high-level macro-actions. We utilize a reinforcement learning algorithm with CMAC for function approximation. Experimental results demonstrate that, following the training phase, the dribbler successfully completes the task against a challenging adversary approximately 58% of the time."}
{"text": "Remote photoplethysmography (rPPG) holds significant promise for contactless heart activity monitoring, particularly in fields like remote healthcare. However, current end-to-end rPPG and heart rate (HR) estimation methods from facial videos struggle in less controlled environments characterized by head movement and poor lighting. This study investigates the limitations of existing end-to-end networks in such challenging conditions and proposes AutoHR, a robust end-to-end baseline for remote HR measurement using neural architecture search (NAS). AutoHR comprises three key components: a powerful searched backbone incorporating Temporal Difference Convolution (TDC) to capture rPPG-specific temporal information, a hybrid loss function leveraging constraints from both time and frequency domains, and spatio-temporal data augmentation strategies to enhance representation learning. Extensive experiments conducted on three benchmark datasets demonstrate the superior performance of AutoHR in both intra- and cross-dataset evaluations."}
{"text": "The rapidly evolving field of reinforcement learning (RL), particularly deep RL (DRL), presents significant advancements while facing ongoing scientific and technical challenges, such as action abstraction and environmental exploration, which intrinsic motivation (IM) aims to address. This article surveys the role of IM in DRL, categorizing different types of IM and analyzing their advantages and limitations in tackling these challenges. Furthermore, it delves into prominent research questions within DRL, focusing on task achievement. By examining these challenges, the article proposes a potential developmental architecture comprising RL algorithms and IM modules for information compression, suggesting a comprehensive framework for addressing a wide range of tasks."}
{"text": "Video super-resolution (VSR) seeks to generate high-resolution (HR) video frames from corresponding low-resolution (LR) frames and neighboring frames.  Temporal alignment is crucial in VSR due to misalignment caused by camera or object motion. Existing methods often rely on optical flow for frame warping, making their performance susceptible to inaccuracies in flow estimation. To address this, we introduce a temporal deformable alignment network (TDAN) that achieves feature-level alignment between the reference and supporting frames without explicit optical flow computation. TDAN dynamically predicts sampling convolution kernel offsets using features from both the reference and supporting frames, effectively transforming supporting frames to align with the reference. A reconstruction network then utilizes the aligned frames and the reference frame to predict the HR video frame. Experimental results validate the efficacy of our TDAN-based VSR model."}
{"text": "Current deep learning methods for graph representation learning rely on aggregating information from neighboring nodes. This work investigates key characteristics of these models and proposes a solution to address their limitations. Specifically, the scope of \"neighboring\" nodes considered for representation learning is heavily influenced by the graph structure, similar to the propagation of a random walk. To accommodate variations in local neighborhood characteristics and tasks, we introduce jumping knowledge (JK) networks, an architecture that dynamically selects different neighborhood ranges for each node, facilitating more effective structure-aware representation learning. Experiments conducted on social, bioinformatics, and citation networks demonstrate that our model achieves state-of-the-art results. Moreover, integrating the JK framework with existing models such as Graph Convolutional Networks, GraphSAGE, and Graph Attention Networks consistently enhances their performance."}
{"text": "This paper presents our submission to the Recognizing Families In the Wild Data Challenge (4th Edition), held in conjunction with the FG 2020 Forum. While automatic kinship recognition holds significant potential, it remains a challenging task due to the limited cues available for determining blood relationships between individuals. Building upon existing methods, we explored various approaches, including deep metric learning techniques to extract feature embeddings from images and subsequently assess kinship based on Euclidean distance or class-based methods. Our experiments revealed that strategies such as increasing negative sample size and utilizing high-resolution images can enhance performance. Ultimately, we achieved our best results across all tasks by employing a symmetric network architecture coupled with a binary classification approach."}
{"text": "This work presents a novel approach to video understanding by reframing video recognition as an image recognition problem, eliminating the need for temporal modeling. Our method leverages a simple yet universal strategy: constructing a super image from input frames and training an image classifier to perform action recognition, mirroring the process of image classification. We validate the effectiveness of this approach by achieving strong results on four benchmark datasets—Kinetics400, Something-to-something (V2), MiT, and Jester—using a state-of-the-art vision transformer. Furthermore, we demonstrate the generalizability of our idea by employing widely used ResNet image classifiers, achieving competitive performance on Kinetics400 comparable to leading spatio-temporal CNN approaches. Our code and models are publicly accessible at https://github.com/IBM/sifar-pytorch."}
{"text": "Experimental observations indicate that the efficiency of distributed stochastic gradient descent (SGD) training is significantly influenced by batch size and, in asynchronous implementations, by gradient staleness. Notably, speedup plateaus beyond a specific batch size and/or when delays become excessive. We introduce a data-dependent parameter that elucidates this speedup saturation in both scenarios. Our comprehensive theoretical analysis, encompassing strongly convex, convex, and non-convex settings, consolidates and extends previous research that often addressed only one of these aspects. Specifically, our framework enables the derivation of enhanced speedup results under common sparsity assumptions. Our findings provide theoretically grounded guidelines for adjusting learning rates in practice. We demonstrate the tightness of our results and validate key insights through numerical experiments."}
{"text": "The increasing prevalence of camera networks has led to a surge in research on multi-camera video analytics, encompassing tasks like object detection, attribute recognition, and cross-camera tracking of vehicles and individuals. However, existing frameworks are primarily designed for closed datasets with limited camera variability and well-defined surveillance environments, focusing on offline analysis with human guidance for forensic purposes. This paper introduces a teamed classifier framework for video analytics in heterogeneous, large-scale camera networks operating under challenging conditions, including multi-scale, multi-resolution cameras, occlusion, blur, and varying orientations. The framework is demonstrated for vehicle tracking and re-identification, employing a zero-shot learning (ZSL) system for continuous, automated vehicle tracking. Evaluations on VeRi-776 and Cars196 datasets demonstrate the robustness of the teamed classifier framework against adversarial conditions, its adaptability to evolving video characteristics such as new vehicle types/brands and cameras, and its real-time performance advantage over conventional offline video analytics methods."}
{"text": "Existing Siamese network trackers often lack model updates, limiting their ability to adapt to target-specific variations. Furthermore, their reliance on axis-aligned bounding boxes for object state inference can introduce background noise and hinder accurate estimation of rotation and scale transformations, potentially compromising tracking accuracy. To address these limitations, this paper introduces RSINet, a novel Rotation-Scale Invariant Network. RSINet comprises a target-distractor discrimination branch and a rotation-scale estimation branch, enabling explicit learning of rotation and scale information through a multi-task learning approach. Moreover, the tracking model is adaptively optimized and updated under spatio-temporal energy control, ensuring model stability, reliability, and high tracking efficiency. Extensive evaluations on OTB-100, VOT2018, and LaSOT benchmarks demonstrate that RSINet achieves state-of-the-art performance compared to recent trackers while maintaining a real-time speed of approximately 45 FPS."}
{"text": "This research investigates the application of slope difference distribution (SDD) for object recognition. SDD, a robust method for curve partitioning and clustering center calculation, has demonstrated superior performance in image segmentation compared to existing techniques, as verified by publicly available Matlab code comparisons. Leveraging the similarity between object contours and image histograms, SDD features are extracted from object contours to create a sparse representation. These SDD features form the basis for reference models, enabling model matching for online object recognition. Experimental results demonstrate the effectiveness of SDD, achieving 100% accuracy in gesture recognition on the NUS and near-infrared datasets, and 100% accuracy in object recognition on the Kimia 99 dataset."}
{"text": "The development of machine learning models for EEG analysis outside controlled laboratory environments necessitates methods capable of handling noisy data and randomly missing channels, a challenge particularly pronounced when using sparse EEG montages common in consumer-grade devices. Existing classical machine learning and deep learning approaches often lack robustness to such data corruption. While some strategies address missing channels, they are often impractical for sparse montages and resource-constrained devices. To address this, we introduce dynamic spatial filtering (DSF), a multi-head attention module that, when integrated before the first layer of a neural network, learns to prioritize reliable channels while mitigating the impact of corrupted ones. Evaluated on public EEG datasets with simulated corruption (~4,000 recordings) and a private dataset of at-home mobile EEG recordings (~100 recordings) with natural corruption, DSF achieves comparable performance to baseline models in noise-free conditions but outperforms them by up to 29.4% accuracy under significant channel corruption. Furthermore, DSF provides interpretable outputs, enabling real-time monitoring of channel importance. This approach holds promise for enabling EEG analysis in challenging settings where channel corruption hinders brain signal interpretation."}
{"text": "This study aims to accelerate image classification in imaging AI by automating label extraction from clinical reports and extending 2D classification to 3D image volumes.  Utilizing the SBERT natural language processing approach, Part 1 automatically extracts class labels from 90 radiology reports, which are then used in Part 2 to train a Deep-Q Network (DQN) for 3D image classification via reinforcement learning.  The DQN employs multi-step image classification, incorporating 3D convolutions and TD(0) Q learning.  Training and testing were conducted on separate sets of 90 and 61 images, respectively, with labels predicted from patient reports by the trained SBERT.  For comparison, a supervised deep learning classification network was also trained and tested on the same dataset.  SBERT achieved 100% accuracy in predicting normal and metastasis-containing scans.  While the supervised approach overfit the training data, resulting in 66% accuracy on the testing set, the reinforcement learning approach achieved 92% accuracy, a statistically significant improvement (p-value = 3.1 x 10^-5)."}
{"text": "This paper introduces Impressions2Font (Imp2Font), a novel approach for generating font images that evoke specific impressions. Imp2Font builds upon conditional generative adversarial networks (GANs) and accepts an arbitrary number of impression words as input. An impression embedding module, leveraging word embedding techniques, transforms these words into a soft-constraint vector, guiding the font generation process. Both qualitative and quantitative evaluations demonstrate that Imp2Font surpasses existing methods in generating high-quality font images, effectively incorporating multiple impression words and even handling previously unencountered words."}
{"text": "Joint learning of monocular depth estimation and semantic segmentation has gained traction due to the benefits of task interaction. However, existing methods often underutilize semantic labels, treating them solely as supervision for segmentation, and neglecting their contextual richness. To address this, we propose CI-Net, a network augmented with contextual information. CI-Net incorporates a self-attention block in the encoder to generate an attention map, guided by an ideal attention map derived from semantic labels. This injection of contextual information enhances scene understanding and facilitates accurate predictions by leveraging correlated features. Furthermore, a feature sharing module enables deep fusion of task-specific features, while a consistency loss encourages mutual guidance between them. Experiments on the NYU-Depth-v2 and SUN-RGBD datasets demonstrate that CI-Net significantly improves the accuracy of both semantic segmentation and depth estimation."}
{"text": "Advancements in deep learning have led to increasingly realistic image colorization, making it harder to distinguish colorized images from natural ones. This work introduces a novel convolutional neural network (CNN)-based forensic method for identifying natural images (NIs) and colorized images (CIs). The proposed method achieves high classification accuracy and effectively handles blind detection scenarios, where training samples from unknown colorization algorithms are unavailable. A base network is designed and implemented, demonstrating superior performance in terms of classification accuracy and generalization compared to existing methods.  Furthermore, a new branch analyzing smaller feature regions is integrated into the base network, enhancing both classification accuracy and generalization. To further improve blind detection performance, negative samples are automatically generated through linear interpolation of paired natural and colorized images and progressively incorporated into the training dataset. Experimental results confirm the method's ability to achieve stable and high generalization performance across various state-of-the-art colorization algorithms."}
{"text": "Due to the health risks associated with elevated Particulate Matter (PM) levels, the World Health Organization (WHO) has established guidelines for PM management, necessitating accurate measurement procedures. While Tapered Element Oscillating Microbalance (TEOM)-based PM sensors offer a cost-effective alternative to Beta Attenuation Monitor (BAM)-based sensors, they exhibit a higher susceptibility to malfunction. This paper addresses the challenge of anomaly detection in PM measuring sensors, which we define as any overall malfunction, to facilitate timely maintenance. To achieve this, we introduce a novel architecture called Hypothesis Pruning Generative Adversarial Network (HP-GAN) and demonstrate its superior performance through experimental comparisons with other anomaly detection architectures."}
{"text": "This work introduces methods for training convolutional neural networks (CNNs) with binarized weights and activations, resulting in quantized models optimized for mobile devices with constrained power and computational resources. Unlike previous quantization approaches that focus on \"value approximation\" by representing floating-point values with discrete sets, this research adopts a novel \"structure approximation\" perspective, hypothesizing that architectures specifically designed for low-bit networks can achieve superior performance.  A \"network decomposition\" strategy called Group-Net is proposed, dividing the network into groups, each reconstructable from a set of homogeneous binary branches.  Learned connections between groups enhance representation capability. Group-Net demonstrates strong generalization to other tasks, including semantic segmentation through context embedding within the binary structure and, for the first time, object detection. Experiments across classification, semantic segmentation, and object detection tasks showcase the superior accuracy and computational efficiency of the proposed methods compared to existing quantized networks, outperforming previous state-of-the-art binary neural networks."}
{"text": "Dense video captioning, which seeks to identify and describe significant events within unedited videos, has primarily relied on visual features, overlooking the potential of audio information. While some studies have attempted to integrate both modalities, their results have been subpar or limited to specific domains. This paper presents Bi-modal Transformer, a novel architecture that extends the Transformer framework to handle bi-modal input. We demonstrate the efficacy of our model in dense video captioning by leveraging both audio and visual modalities, highlighting its versatility for any sequence-to-sequence task involving two modalities. Furthermore, we showcase the utility of the pre-trained bi-modal encoder within Bi-modal Transformer as a feature extractor for a straightforward proposal generation module. Our model achieves state-of-the-art performance on the challenging ActivityNet Captions dataset. The code is accessible at v-iashin.github.io/bmt."}
{"text": "Deep learning has significantly advanced computer vision tasks like object detection and segmentation, but the high risk associated with inaccurate predictions in real-world applications, such as autonomous driving, necessitates addressing the issue of model uncertainty.  Standard object detection models, like YOLO, often exhibit overconfidence and fail to account for uncertainty in out-of-distribution data. This work proposes MC-DropBlock, an efficient and effective method for modeling uncertainty in object detection and segmentation using Monte-Carlo DropBlock inference. By applying drop-block during both training and testing on convolutional layers of models like YOLO, MC-DropBlock creates a Bayesian convolutional neural network capable of capturing epistemic uncertainty. Aleatoric uncertainty is additionally modeled using a Gaussian likelihood. The effectiveness of MC-DropBlock in modeling uncertainty for object detection and segmentation is demonstrated through out-of-distribution experiments. Results indicate that MC-DropBlock enhances the generalization, calibration, and uncertainty modeling capabilities of YOLO models."}
{"text": "While deep neural networks (DNNs) have demonstrated exceptional performance in various low-level vision tasks, their reliance on deep architectures with millions of parameters poses challenges for deployment on resource-constrained platforms. To address this trade-off between performance and efficiency, this paper introduces a novel activation unit tailored for image restoration. Unlike conventional per-pixel activation units such as ReLUs and sigmoids, our unit employs a learnable nonlinear function incorporating spatial connections, enabling the capture of more intricate features. Consequently, fewer layers are required to achieve comparable performance. We validate the efficacy of our unit through experiments on state-of-the-art networks for denoising, de-raining, and super-resolution, which are already considered compact. Our approach facilitates a nearly 50% reduction in model size without compromising performance."}
{"text": "Self-supervised learning typically involves training a model on a pretext task without human annotations, with the ultimate goal of transferring this knowledge to a target domain and task. While fine-tuning is the prevailing transfer strategy, it necessitates using the same or similar model architecture for both pretext and target tasks. This paper introduces a novel framework that addresses the limitations of designing, comparing, and transferring knowledge across diverse tasks, models, and data domains. By decoupling the self-supervised model's structure from the task-specific fine-tuned model, our framework enables: 1) quantitative evaluation of previously incompatible models, including handcrafted features; 2) demonstration that deeper neural networks can learn superior representations from the same pretext task; and 3) transfer of knowledge from a deep model to a shallower one, thereby enhancing its learning. Leveraging this framework, we propose a novel self-supervised task that achieves state-of-the-art performance on PASCAL VOC 2007, ILSVRC12, and Places benchmarks. Our learned features reduce the mean average precision (mAP) gap between self-supervised and supervised learning models in object detection on PASCAL VOC 2007 from 5.9% to 2.6%."}
{"text": "The traditional design of deep neural network architectures for tasks like medical image segmentation relies on a time-consuming, resource-intensive trial-and-error process driven by human expertise. To overcome this challenge, we introduce a novel algorithm that leverages policy gradient reinforcement learning to automatically optimize hyperparameters for deep network architectures, specifically focusing on medical image segmentation. Our reward function is based on a segmentation evaluation utility, namely the Dice index. We demonstrate the effectiveness of our method, highlighting its low computational cost compared to existing state-of-the-art medical image segmentation networks.  We propose a densely connected encoder-decoder CNN as a robust baseline architecture and apply our hyperparameter search algorithm to each layer. As a practical application, we train our system on cine cardiac MR images from the Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. Starting from the baseline architecture, the resulting network achieves state-of-the-art accuracy without requiring manual architecture design or close supervision of hyperparameter adjustments."}
{"text": "While graph neural networks (GNNs) show promise across various graph-based applications, the lack of standardized training protocols hinders fair comparisons of novel architectures and data augmentation strategies. To address this, we propose a reproducible benchmark for node classification, encompassing nine diverse datasets and seven GNN models. Our benchmark employs a k-fold assessment strategy for smaller datasets and a unified training procedure for all datasets, establishing a standardized experimental pipeline for GNNs. Through data augmentation using node2vec and Laplacian eigenvectors, we explore the impact of input features on model performance, revealing the significance of topological information for node classification. Our findings indicate that increasing model layers does not consistently enhance performance, except for disconnected graphs like PATTERN and CLUSTER. Notably, data augmentation, particularly with node2vec, significantly improves baseline performance."}
{"text": "Advancements in fields such as smart homes, autonomous vehicles, healthcare, and robotics, coupled with stricter regulations, have significantly impacted academic research on explainable machine learning. While numerous researchers have developed methods to explain any black-box classification model, a limitation of agnostic explanators is their reliance on a universal neighborhood generation process that may not ensure true adjacency between generated neighbors and the instance. This paper proposes a methodology for locally explaining neural network decisions by incorporating the network's architecture into the neighborhood generation process, thereby guaranteeing adjacency between the generated neighbors and the instance."}
{"text": "Rolling shutter LiDARs, widely used in robotics due to their detailed geometric data, capture the environment by sequentially scanning with an array of lasers. Conventional perception algorithms process data only after a complete 360-degree sweep is acquired, introducing latency, typically around 100ms for 10Hz LiDARs. This delay hinders real-time robotics applications demanding swift reactions to dynamic situations. To address this, we introduce StrObe, a novel method that minimizes latency by processing LiDAR packets incrementally, generating a continuous stream of detections without waiting for the full sweep. StrObe leverages a latent spatial representation of the scene, updated iteratively with incoming packets, effectively acting as a memory to maintain accuracy despite the low latency. Our evaluation on a large-scale real-world dataset demonstrates StrObe's superior performance compared to state-of-the-art methods when latency is considered, while achieving comparable accuracy in traditional settings."}
{"text": "This work introduces a weakly supervised top-down saliency framework that leverages binary image labels to predict object presence or absence.  Instead of relying on pixel-level annotations, the proposed method computes the probabilistic contribution of each image region to the confidence of a convolutional neural network (CNN) classifier through a backtracking strategy, generating top-down saliency.  A suitable bottom-up saliency map is selected from a set of maps produced by fast bottom-up approaches and combined with the top-down saliency.  Features with high combined saliency are used to train a linear support vector machine (SVM) classifier for feature saliency estimation, which is then integrated with the combined saliency and refined through multi-scale superpixel averaging.  The performance of the weakly supervised approach is evaluated on seven challenging datasets and compared with 40 related methods across four applications, demonstrating comparable results to fully supervised techniques."}
{"text": "While 2D object detection in clean images has been extensively researched, its susceptibility to adversarial attacks remains a concern. Although adversarial training has enhanced the robustness of object detectors, it often leads to a significant decrease in average precision (AP) on clean images. This paper proposes that feature alignment of intermediate layers can improve both clean AP and robustness in object detection. Building upon adversarial training, we introduce two feature alignment modules: Knowledge-Distilled Feature Alignment (KDFA) and Self-Supervised Feature Alignment (SSFA), which guide the network to generate more effective features. We conduct comprehensive experiments on PASCAL VOC and MS-COCO datasets to validate the effectiveness of our proposed approach. The code for our experiments is accessible at https://github.com/grispeut/Feature-Alignment.git."}
{"text": "Although word2vec, a word embedding method introduced by Mikolov (2013), has achieved widespread adoption in natural language processing, a robust theoretical foundation remains elusive. This paper presents a rigorous analysis of word2vec's highly nonlinear functional, revealing that a spectral method may underpin its performance. This finding holds the potential to facilitate the derivation of provable guarantees for word2vec. Numerical simulations corroborate these results. A compelling open question is whether the nonlinear aspects of word2vec not accounted for by the spectral method contribute positively to its effectiveness, and if so, through what mechanisms."}
{"text": "Reinforcement learning has seen significant advancements, with model-free approaches demonstrating broader applicability compared to model-based methods. This work introduces a novel reinforcement learning architecture called \"neural network iterative linear quadratic regulator (NNiLQR),\" which leverages iterative linear quadratic regulator (iLQR) principles without requiring prior system knowledge. By relying solely on measurement data, NNiLQR establishes an optimal policy through iterative neural network refinements, effectively providing a non-parametric approach. Notably, NNiLQR surpasses the classical iLQR method in terms of the objective function due to its incorporation of enhanced exploration. The superiority of NNiLQR is evident in the results obtained from two illustrative examples."}
{"text": "This work expands the field of composed image retrieval, where queries combine an image with textual instructions for modification, by moving beyond narrow domains and non-complex images. Recognizing the limitations of existing methods, we introduce the Compose Image Retrieval on Real-life images (CIRR) dataset, comprising over 36,000 crowd-sourced image-text pairs from diverse, open-domain sources. To address the challenges of open-domain composed image retrieval, we propose CIRPLANT, a transformer-based model that utilizes pre-trained vision-and-language knowledge to modify visual features according to natural language input. Retrieval is then performed through nearest neighbor search on these modified features. We demonstrate that CIRPLANT, despite its relatively simple architecture, surpasses existing methods on open-domain images while achieving state-of-the-art accuracy on established narrow-domain datasets like fashion.  The release of CIRR, coupled with our proposed model, aims to stimulate further research in composed image retrieval."}
{"text": "Malicious URLs pose a significant cybersecurity threat, hosting harmful content and deceiving users into financial loss, data theft, and malware infection, resulting in substantial annual financial damage. While blacklists are traditionally used for detection, their incompleteness and inability to identify newly created malicious URLs necessitate alternative solutions. Machine learning techniques have emerged as a promising approach to enhance the effectiveness of malicious URL detection. This article offers a comprehensive review of machine learning-based malicious URL detection techniques, formally defining the problem and categorizing existing research contributions across various dimensions, including feature representation and algorithm design.  Targeting a broad audience encompassing machine learning researchers, cybersecurity professionals, and practitioners, this survey aims to provide a current understanding of the field, fostering further research and practical applications. Additionally, the article addresses practical system design considerations, highlights open research challenges, and suggests promising directions for future research."}
{"text": "This paper introduces FFA-Net, an end-to-end feature fusion attention network designed for direct haze-free image restoration. FFA-Net comprises three core components: a novel Feature Attention (FA) module that integrates Channel and Pixel Attention mechanisms to differentially weight channel-wise features and pixel-wise haze distribution; a basic block structure incorporating Local Residual Learning and FA to bypass less crucial information and focus on salient features; and an Attention-based different levels Feature Fusion (FFA) structure that adaptively learns feature weights from the FA module, emphasizing important features while preserving information from shallow layers. Experimental results demonstrate that FFA-Net significantly outperforms existing single image dehazing methods, achieving a substantial improvement in PSNR from 30.23db to 36.39db on the SOTS indoor test dataset. The code is publicly available on GitHub."}
{"text": "Team Applied Robotics' vision-based robotic picking system, designed for the Amazon Picking Challenge 2016, is detailed in this paper. The competition required teams to create a robotic system capable of picking a diverse range of products from shelves or totes. This paper outlines the team's design considerations and strategy, including the implementation of a high-resolution 3D vision system, the utilization of texture and shape-based object detection algorithms, and the development of robot path planning and object manipulation techniques."}
{"text": "This research investigates open-set deep face recognition, aiming to achieve face feature representations where intra-class distances are minimized while inter-class distances are maximized within a chosen metric space. Hyperspherical face recognition has emerged as a promising approach in this domain. While SphereFace, an early hyperspherical method, sought to learn embeddings with large angular margins between classes, it suffered from training instability. To address this, a unified framework is introduced to analyze large angular margins in hyperspherical face recognition, leading to the development of SphereFace-R, an improved variant with enhanced training stability. SphereFace-R incorporates novel multiplicative margin implementations and explores three feature normalization schemes. Additionally, a \"characteristic gradient detachment\" strategy is proposed to further stabilize training.  Experimental results demonstrate that SphereFace-R consistently outperforms or matches state-of-the-art methods."}
{"text": "Graph Convolutional Networks (GCNs) have shown promise in human action recognition from skeleton data due to their ability to model non-Euclidean structures. However, many existing GCN methods rely on pre-defined, static graphs, potentially overlooking implicit joint correlations. Furthermore, the common spectral GCN approach, based on one-order hop approximations, may not fully capture higher-order connections. To address these limitations, we leverage Neural Architecture Search (NAS) to develop an automatically designed GCN for skeleton-based action recognition. Our search space incorporates dynamic graph modules to capture spatial-temporal correlations and multiple-hop modules to overcome the limitations of one-order approximations. We also propose a sampling- and memory-efficient evolution strategy for architecture search. The resulting architecture demonstrates the effectiveness of higher-order approximations and dynamic graph modeling with temporal interactions, aspects rarely explored previously. Extensive experiments on two large-scale datasets confirm the state-of-the-art performance of our model."}
{"text": "While attention-based scene text recognizers, which employ RNN-based encoder-decoder architectures to learn 1d- or 2d- attention from compact intermediate representations, have achieved significant success, they are susceptible to attention drift due to feature similarity and suffer from low efficiency due to limited parallelization. To address these limitations, we introduce MASTER, a self-attention based scene text recognizer that not only captures input-output attention but also learns self-attention to model feature-feature and target-target relationships within the encoder and decoder. This approach enables MASTER to learn a more robust intermediate representation resistant to spatial distortion, while also benefiting from high training parallelization and an efficient memory-cache mechanism for accelerated inference.  Extensive evaluations on various benchmarks demonstrate MASTER's superior performance on both regular and irregular scene text. Pytorch code can be found at https://github.com/wenwenyu/MASTER-pytorch, and Tensorflow code can be found at https://github.com/jiangxiluning/MASTER-TF."}
{"text": "While learning-based approaches have excelled in single-image super-resolution, video super-resolution seeks to leverage temporal information from multiple frames, often through optical flow and image warping. This paper presents an end-to-end video super-resolution network that integrates optical flow estimation within its architecture. Our analysis reveals that conventional image warping techniques limit the benefits of optical flow for video super-resolution. Consequently, we propose a novel motion compensation operation that directly warps from low to high resolution. This network configuration enables video super-resolution to effectively utilize optical flow, achieving state-of-the-art performance on benchmark datasets. Furthermore, we demonstrate that processing entire images, rather than independent patches, significantly enhances accuracy."}
{"text": "Histopathological image annotation is a laborious process demanding expert pathologists to meticulously examine extensive whole-slide images. While transfer learning has shown promise in image understanding tasks with limited annotations, its application to histology image analysis often suffers from performance degradation due to domain discrepancies between source and target datasets, including variations in tissues, staining, and imaging devices. To address this challenge, we propose a novel unsupervised domain adaptation method for histopathological image analysis. Our approach utilizes a backbone network to embed input images into a feature space and a graph neural layer to propagate supervision signals from labeled images. A graph model connects images based on their proximity in the embedded space, and a graph neural network synthesizes new feature representations. During training, target samples with confident predictions are assigned pseudo labels. A cross-entropy loss function guides predictions for both source samples with manual labels and target samples with pseudo labels. To further enhance domain-invariant feature extraction and category discrimination, we incorporate maximum mean diversity and contrastive learning. Our method achieves state-of-the-art performance in unsupervised domain adaptation for histopathological image classification across four public datasets."}
{"text": "Recent advancements in geometric representation learning have demonstrated significant potential across diverse machine learning domains, including relational learning, language processing, and generative models. This work investigates manifold-valued regression onto hyperbolic space as a key component for various machine learning applications. Specifically, by framing the task of predicting tree nodes as a manifold regression problem within hyperbolic space, we introduce a novel approach to address two challenging problems: hierarchical classification through label embeddings and taxonomy extension of hyperbolic representations. To tackle the regression problem, we explore existing methods and propose two computationally efficient novel approaches: a parametric deep learning model incorporating geodesic information from the target space and a non-parametric kernel method with proven excess risk bounds. Our experimental results indicate the effectiveness of leveraging hyperbolic geometry, particularly in taxonomy expansion, where hyperbolic-based estimators significantly outperform Euclidean space-based regression methods."}
{"text": "Ensuring the safe deployment of machine learning (ML) models in critical applications necessitates addressing vulnerabilities such as learned shortcuts, which are spurious correlations exploited by networks for decision-making without semantic relevance to the task. These shortcuts can hinder generalization to unseen data. While explainability methods can reveal such vulnerabilities, their applicability is limited in black-box setups, common when using third-party ML components. To overcome this, we propose using an interpretable-by-design network, specifically a BagNet that relies on local image patches for decisions, as a proxy to detect learned shortcuts in a black-box model. By leveraging the proxy's interpretability, we automatically extract potential shortcut candidates and validate their transferability to the black box. Our experiments on the autonomous driving dataset A2D2 demonstrate the significant influence of extracted patch shortcuts on the black-box model. This approach efficiently identifies patch-based vulnerabilities, contributing to the development of safer ML models."}
{"text": "Skin detection, a crucial aspect of object detection in human-computer interaction, has found applications in various fields such as face recognition, human motion analysis, and content filtering. While existing research on skin detection has primarily focused on individuals of African, Mongolian, and Anglo-Saxon descent, the specific characteristics of Indian sub-continental skin tones have received limited attention. This study aims to compare three image segmentation techniques for detecting skin in images of Indian sub-continental individuals, optimize the detection criteria, and identify efficient parameters for accurate skin area detection. Experimental results indicate that the HSV color model-based approach demonstrates superior performance, achieving a true positive rate of 91.1% and a true negative rate of 88.1%."}
{"text": "This thesis introduces DeepV2D, a novel end-to-end deep learning framework designed for video depth estimation. DeepV2D leverages the power of neural networks to learn image representations while incorporating geometric principles of image formation.  The architecture integrates a series of classical geometric algorithms, transformed into trainable modules and seamlessly combined into a fully differentiable structure. DeepV2D employs an iterative process, alternating between motion estimation and depth estimation stages until convergence to accurate depth maps. The source code for DeepV2D is publicly accessible at https://github.com/princeton-vl/DeepV2D."}
{"text": "The proliferation of highly realistic fake images created using Deepfake or GAN technologies poses a significant societal threat. While numerous methods have been developed to detect these fabricated images, they remain susceptible to adversarial perturbations—deliberately introduced noise that can lead to misclassifications. Current attack methods typically generate adversarial perturbations that affect nearly the entire image, resulting in redundancy and increased perceptibility. This paper introduces a novel approach to disrupt fake image detection by identifying key pixels crucial to the detector's operation and targeting only these pixels for attack. This strategy significantly reduces the L_0 and L_2 norms of adversarial perturbations compared to existing methods. Experiments conducted on two publicly available datasets using three different fake image detectors demonstrate that our proposed method achieves state-of-the-art performance in both white-box and black-box attack scenarios."}
{"text": "Autonomous driving necessitates robust 3D perception, driving the adoption of MEMS LiDAR due to its affordability, durability, and suitability for mass production. Despite its advantages, MEMS LiDAR's limited field of view (FoV) hinders its widespread implementation. This paper introduces LEAD, a LiDAR Extender for Autonomous Driving, which expands the FoV and range of MEMS LiDAR by integrating image data. LEAD employs a multi-stage propagation strategy leveraging depth distributions and uncertainty maps for effective depth extension.  A teacher-student training framework ensures accurate depth transfer to the completion network, mitigating scale errors.  A high-precision laser scanner-generated ground-truth dataset validates LEAD's performance, demonstrating superior results compared to state-of-the-art methods. We anticipate that LEAD and the accompanying dataset will contribute significantly to depth research within the community."}
{"text": "Driven by the computational demands of contemporary deep models, this work focuses on transferring knowledge from a large, accurate model to a smaller one. Our contributions encompass three key areas: (i) we present an adversarial network compression method that enables the training of a compact student network to emulate a larger teacher network without requiring labeled data; (ii) we introduce a regularization technique to prevent the emergence of an overly powerful discriminator while preserving network capacity; and (iii) our approach demonstrates generalizability across diverse teacher-student model configurations. Through comprehensive evaluations on five benchmark datasets, we demonstrate that our student network exhibits minimal accuracy degradation, outperforms other knowledge transfer techniques, and surpasses the performance of the same network trained using labels. Furthermore, we achieve state-of-the-art results when compared to alternative compression strategies."}
{"text": "This research investigates conversational fashion image retrieval using multiturn natural language feedback, addressing the limitations of previous single-turn approaches and traditional multiturn models. A novel framework is proposed that effectively leverages encoded reference image, feedback text, and conversation history to identify candidate images.  Furthermore, a mutual attention strategy incorporates image fashion attribute information. Due to the lack of a suitable multiturn fashion dataset, a large-scale dataset was created through manual annotation of an existing single-turn dataset. Experimental results demonstrate that the proposed model significantly outperforms current state-of-the-art methods."}
{"text": "Accurate pose tracking, which involves identifying and temporally matching unique human poses across video frames, remains a challenge. Existing methods struggle to effectively model temporal relationships and often demand substantial computational resources, typically requiring offline processing. This work introduces KeyTrack, an efficient multi-person pose tracking method that leverages only keypoint information, eschewing RGB or optical flow data, to achieve real-time human keypoint tracking. Our Pose Entailment method forms the core of KeyTrack, where pose estimates from different frames are sampled, tokenized, and fed into a Transformer-based network for binary classification of temporal succession. To enhance keypoint accuracy for the Pose Entailment step, we incorporate a novel, parameter-free keypoint refinement technique into our top-down pose estimation method. KeyTrack achieves state-of-the-art performance on the PoseTrack'17 and PoseTrack'18 benchmarks while significantly reducing the computational cost compared to most other tracking methods."}
{"text": "Due to the critical need for minimizing prediction delay in time-sensitive fields like healthcare and finance, early time series classification has received significant attention. This approach aims to classify incomplete time series with a desired accuracy level as quickly as possible.  Numerous early classification methods have emerged in recent years, each addressing different aspects of the problem.  Therefore, a comprehensive review of existing solutions is crucial to understand the current state of the field. These solutions have shown promising results across diverse applications, including human activity recognition, gene expression-based health diagnostics, and industrial monitoring. This paper presents a systematic review of early classification approaches for both univariate and multivariate time series, categorizing them into four distinct groups: prefix-based, shapelet-based, model-based, and miscellaneous approaches. The review also explores the applications of early classification in various domains, such as industrial monitoring, intelligent transportation, and medicine. Finally, it summarizes the current literature and outlines future research directions."}
{"text": "Optimizing the visual appeal of designs involves strategically positioning text over images. Automating this process necessitates algorithms capable of determining suitable text placement, orientation, and style based on the image content. This task, termed \"copyspace detection,\" focuses on identifying aesthetically pleasing text parameters within an image, distinct from traditional foreground-background segmentation. Our research explores copyspace detection solutions utilizing both one- and two-stage object detection methodologies trained on a manually annotated dataset. This workshop will delve into these algorithms, showcasing their integration into generative design models and pipelines, such as Einstein Designer."}
{"text": "The rapid dissemination of fashion trends through social media has accelerated the fashion design and manufacturing cycle, but the sheer volume of user-generated content makes it challenging for designers to identify emerging trends. This highlights the need for automated methods to analyze fashion photos and extract information about trending items. However, creating large labeled datasets for fast fashion items is difficult, and existing object detectors lack the capability to leverage the abundance of unlabeled data on social media for fine-tuning. This work demonstrates the effectiveness of a generic object detector, pre-trained in an unsupervised manner on a large collection of unlabeled social media images, followed by fine-tuning on a smaller labeled dataset. This approach achieves a 72.7% mAP on a test dataset, outperforming state-of-the-art detectors by 11% to 17%, primarily due to the architecture's ability to learn from unlabeled data and effectively identify small objects."}
{"text": "Previous research indicates that effective face anti-spoofing relies on identifying subtle image patterns known as \"spoof traces,\" which include color distortion, 3D mask edges, Moire patterns, and others. Developing a generic face anti-spoofing model capable of estimating these spoof traces can enhance both the generalization of spoof detection and the interpretability of the model's decisions. However, this task is challenging due to the diversity of spoofing methods and the absence of ground truth for spoof traces. This work proposes a novel adversarial learning framework to separate spoof faces into their spoof traces and corresponding live counterparts. Guided by physical properties, spoof generation is modeled as a combination of additive and inpainting processes. The additive process represents spoofing as the introduction of extra patterns (e.g., Moire patterns) by spoofing material, where the live counterpart can be recovered by removing these patterns. The inpainting process describes spoofing as spoofing material completely covering certain regions, requiring the live counterpart of those regions to be \"guessed.\" Three additive components and one inpainting component are used to represent traces at different frequency bands. The disentangled spoof traces can be used to synthesize realistic new spoof faces after appropriate geometric correction, and these synthetic spoofs can be used for training to improve the generalization of spoof detection. Our approach achieves superior spoof detection performance across three testing scenarios: known attacks, unknown attacks, and open-set attacks. Furthermore, it provides a visually convincing estimation of the spoof traces. Source code and pre-trained models will be made publicly available upon publication."}
{"text": "This paper introduces a novel access control method for safeguarding semantic segmentation models, leveraging spatially invariant permutation of feature maps with a secret key. During training and testing, selected feature maps are permuted using this key. This approach enables authorized users possessing the correct key to utilize the model's full capabilities while simultaneously degrading performance for unauthorized users. Unlike conventional access control methods, which have been primarily focused on image classification tasks, this method is specifically designed for semantic segmentation. Experimental results demonstrate that protected models allow authorized users to achieve performance comparable to unprotected models while effectively resisting unauthorized access attempts. Furthermore, the study verifies that a conventional method employing block-wise transformations exhibits degraded performance when applied to semantic segmentation models."}
{"text": "Unlike traditional convolutional layers with static filters, our proposed Dynamic Filter Network generates filters dynamically based on the input. This adaptive architecture enhances flexibility without significantly increasing model parameters, enabling the learning of diverse filtering operations, including spatial transformations, selective blurring/deblurring, and adaptive feature extraction.  The modularity of the framework allows for stacking multiple layers, such as in recurrent architectures. We demonstrate the effectiveness of the Dynamic Filter Network on video and stereo prediction tasks, achieving state-of-the-art performance on the moving MNIST dataset with a smaller model. Visualization of learned filters reveals the network's ability to capture flow information from unlabeled data, suggesting its potential for unsupervised pretraining of supervised tasks like optical flow and depth estimation."}
{"text": "This thesis presents Extended Isolation Forest (EIF), an enhancement to the model-free anomaly detection algorithm Isolation Forest, which addresses the issue of biased anomaly score assignment.  Heat maps of anomaly scores reveal artifacts stemming from the binary tree's branching criteria. These artifacts are explained and visually demonstrated. Two solutions are proposed: random data transformation before tree construction to average out bias, and the preferred method of allowing data slicing using hyperplanes with random slopes to directly eliminate the artifact. The robustness of EIF is demonstrated through reduced variance in anomaly scores along constant level sets. Performance evaluation on synthetic and real-world benchmark datasets using AUROC and AUPRC metrics shows no significant difference in convergence rate or computation time compared to the standard Isolation Forest."}
{"text": "This thesis introduces a new attention model capable of precisely focusing on target objects of diverse sizes and shapes within images. The model employs a progressive attentive process across multiple convolutional neural network layers, gradually suppressing irrelevant image regions during training. At each layer, the attentive process decides whether to transmit or block features at specific spatial locations for subsequent layer processing. This progressive attention mechanism proves particularly effective when integrated with hard attention. Furthermore, the model leverages local contexts to incorporate neighboring feature information for each location, resulting in a more accurate attention probability map. Experimental results on both synthetic and real datasets demonstrate that the proposed attention networks surpass conventional attention methods in visual attribute prediction tasks."}
{"text": "While Graph Neural Networks (GNNs) excel at graph representation learning, their reliance on loading entire attributed graphs into memory poses challenges for large graphs with limited resources. This paper introduces Binary Graph Convolutional Network (Bi-GCN), a novel approach that addresses this issue by binarizing both network parameters and input node features, replacing matrix multiplications with binary operations for acceleration. Theoretical analysis indicates that Bi-GCN reduces memory consumption by approximately 30x for both parameters and input data, while accelerating inference speed by approximately 47x on citation networks. A new gradient approximation-based back-propagation method is also proposed for effective training of Bi-GCN. Experiments demonstrate comparable performance to full-precision baselines, and the binarization approach's generalizability is verified through its successful application to other GNNs."}
{"text": "Single Image Super-Resolution (SISR) aims to generate high-resolution images from their low-resolution counterparts, a challenging ill-posed problem. While Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art performance in SISR, they often lack fine detail recovery. Generative Adversarial Networks (GANs), designed to address this issue, face training difficulties and can introduce artifacts. This paper proposes a novel method where CNNs align images in a space constructed using convex optimization, encompassing both low and high-frequency components. This approach enables the recovery of fine details while ensuring training stability."}
{"text": "Multi-label image and video classification pose significant challenges in computer vision due to the complexities of capturing spatial or temporal label dependencies and identifying discriminative features for each class. To address these challenges, this work introduces a novel approach utilizing cross-modality attention guided by semantic graph embedding for multi-label classification.  A novel adjacency-based similarity graph embedding method is proposed to learn semantic label embeddings from the constructed label graph, effectively leveraging label relationships. These learned embeddings then guide the generation of cross-modality attention maps. Experimental validation on two multi-label image classification datasets (MS-COCO and NUS-WIDE) demonstrates superior performance compared to existing state-of-the-art methods. Furthermore, the method's generalization capability is confirmed through evaluation on a large multi-label video classification dataset (YouTube-8M Segments)."}
{"text": "While multiple-solution inpainting methods offer the potential for diverse image completion, they often struggle to maintain high quality across all generated solutions. To address this, we introduce a two-stage model inspired by hierarchical vector quantized variational auto-encoders (VQ-VAEs). Our model first generates multiple coarse results with varying structures by leveraging the VQ-VAE's ability to sample diverse, high-quality structures from a discrete distribution. Subsequently, a texture generation network equipped with a structural attention module refines each coarse result, incorporating structural information to capture distant correlations and enhance texture realism.  The use of VQ-VAE also enables the calculation of feature losses that further improve structure coherence and texture quality. Experiments on CelebA-HQ, Places2, and ImageNet datasets demonstrate that our method not only increases the diversity of inpainting solutions but also elevates the visual quality of the generated images. Code and models are accessible at: https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting."}
{"text": "Chromosome karyotype analysis is crucial for diagnosing and treating diseases, particularly genetic disorders. While computer-assisted analysis has enhanced efficiency and accuracy compared to manual methods, overlapping chromosomes in images pose a significant challenge to analysis accuracy. Traditional segmentation methods relying on manually defined features are susceptible to image quality variations. This paper proposes an adversarial multiscale feature learning framework to enhance the accuracy and adaptability of overlapping chromosome segmentation. A nested U-shape network with dense skip connections serves as the generator, leveraging multiscale features to optimize chromosome image representation. A conditional generative adversarial network (cGAN) with a least-square GAN objective is employed to generate images resembling the originals, improving training stability. Lovasz-Softmax is incorporated to facilitate convergence in a continuous optimization setting. Evaluation on public datasets using eight criteria demonstrates the superior performance of our framework compared to existing algorithms, highlighting its potential for overlapping chromosome segmentation."}
{"text": "This thesis introduces rBDLR, a novel unsupervised representation learning model designed to uncover multi-subspace structures and extract salient features while preserving local information.  rBDLR, built upon a Frobenius-norm based latent low-rank representation model, jointly learns coding coefficients and salient features, enhancing robustness to outliers and errors through representation and weighting in a clean data space.  A block-diagonal structure is enforced on the coefficients via auto-weighting, minimizing reconstruction error while constrained by a block-diagonal regularizer. This process ensures a strict block-diagonal weight matrix, enabling adaptive locality preservation for salient features. By minimizing the difference between coefficient and weight matrices, rBDLR achieves a block-diagonal coefficient matrix, facilitating information exchange between salient features and coefficients. Experimental results highlight the superior performance of rBDLR compared to existing state-of-the-art methods."}
{"text": "To address the limitations of task-agnostic feature alignment in domain generalization (DG) and unsupervised domain adaptation (UDA), this paper introduces a unified framework called Feature Alignment and Restoration (FAR). FAR simultaneously promotes both generalization and discrimination capabilities by aligning feature distributions across domains while preserving task-relevant information.  Specifically, it employs moment alignment of attentively selected features to reduce inter-domain discrepancies. To maintain high discrimination power, a Feature Restoration (FR) operation distills task-relevant features from residual information and integrates them with the aligned features. A dual ranking entropy loss constraint is further imposed during FR to enhance the separation of task-relevant and task-irrelevant features. Extensive experiments on various classification benchmarks validate the superior performance and robust generalization of the FAR framework for both DG and UDA."}
{"text": "Generating point clouds, such as molecular structures, with arbitrary rotations, translations, and enumerations poses a significant challenge.  Leveraging the data-efficiency of neural networks with symmetry invariant layers, we propose an architecture that generates valid Euclidean distance matrices, inherently invariant to rotation and translation of the represented object. Aiming to generate molecular structures in Cartesian space, we employ this architecture within a Wasserstein GAN framework, incorporating a permutation invariant critic network. This enables one-shot generation of molecular structures by producing three-dimensionally embedded Euclidean distance matrices."}
{"text": "Effective data representation is crucial for successful machine learning, as most algorithms rely on features to build models.  Representations can enhance performance by separating classes in classification or revealing data manifolds in regression.  Common representation learning methods include statistical techniques like principal component analysis and manifold learning techniques such as Isomap or locally linear embedding. Autoencoders, a versatile tool among these methods, are the focus of this paper, where we demonstrate how to manipulate their learned representations to achieve desired learning outcomes. We present a series of tasks: data embedding for visualization, image denoising, semantic hashing, abnormal behavior detection, and instance generation, modeling them from a representation learning perspective using state-of-the-art methodologies. For each task, we propose a solution employing autoencoders as the sole learning method. We implement these solutions using diverse datasets, discuss the results for each case study, and briefly explore six additional learning applications. Furthermore, we investigate current challenges and approaches to explainability in the context of autoencoders. Our findings suggest that, through modifications to their structure and objective function, autoencoders hold promise as a core component for solving various problems that can be framed as feature space transformations."}
{"text": "Accurately modeling dependencies between observations in multiple time series is essential for various applications, including anomaly detection, financial risk management, causal analysis, and demand forecasting.  However, existing methods often struggle with the computational and numerical challenges of estimating time-varying and high-dimensional covariance matrices, limiting their applicability to a few hundred dimensions or requiring strong assumptions about the dependencies between series. To address these limitations, we propose a novel approach that combines an RNN-based time series model with a Gaussian copula process output model employing a low-rank covariance structure. This strategy significantly reduces the number of parameters, enabling the modeling of time-varying correlations among thousands of time series while accommodating non-Gaussian marginal distributions. We demonstrate the effectiveness of our method on several real-world datasets, showcasing significant accuracy improvements over state-of-the-art baselines. Furthermore, we conduct an ablation study to analyze the contributions of the different components of our model."}
{"text": "This thesis addresses the interpretability of data exploration workflows that involve dimensionality reduction, group identification, and comparative analysis. We propose a novel approach that leverages the dimensionality reduction model to elucidate the key distinctions between identified groups, framing this as an interpretable machine learning problem. Our contribution is twofold: we introduce Global Counterfactual Explanations (GCEs) as a new type of explanation and develop Transitive Global Translations (TGT), an algorithm for computing GCEs. TGT employs compressed sensing to identify pairwise group differences while enforcing consistency across all groups. Empirical results demonstrate that TGT generates accurate and sparse explanations that align with real data patterns."}
{"text": "As the use of AI grows, security and privacy vulnerabilities in machine learning systems are becoming apparent. One such vulnerability, known as a model inversion attack, allows adversaries to extract private information about the training data used for a specific machine learning model. This attack traditionally involves exploiting classification scores to generate high-confidence representations of different classes. However, in deep networks, this often results in unrecognizable representations. This paper proposes a more practical definition of model inversion, assuming the adversary knows the model's purpose (e.g., OCR or facial recognition) and aims to find realistic class representations within the corresponding lower-dimensional manifold (e.g., general symbols or faces). To achieve this, we utilize generative adversarial networks to construct a connected lower-dimensional manifold and demonstrate the effectiveness of our model inversion attack within this manifold."}
{"text": "This paper introduces a novel global analysis framework for a category of low-rank matrix recovery problems situated on a Riemannian manifold. The framework investigates the global behavior of Riemannian optimization with random initialization, employing the Riemannian gradient descent algorithm to minimize a least squares loss function. The analysis delves into the asymptotic behavior and precise convergence rate, uncovering a previously unidentified geometric characteristic of the low-rank matrix manifold: the presence of spurious critical points for the basic least squares function. Under specific assumptions, the study demonstrates that Riemannian gradient descent, initiated from a random initialization, circumvents these spurious critical points with high probability, converging solely to the ground truth at a near-linear rate (i.e., (\\text(\\frac)+ \\text(n)) iterations for an \\epsilon-accurate solution). Two applications exemplify this global analysis: rank-1 matrix recovery and a generalized Gaussian phase retrieval problem exhibiting weak isometry but similar behavior to the first application, except for an additional saddle set. The convergence guarantee proves nearly optimal and largely dimension-free, providing a comprehensive explanation for numerical observations. This global analysis holds potential for extension to other data problems characterized by random measurement structures and empirical least squares loss functions."}
{"text": "This thesis proposes a novel unsupervised, iterative algorithm for registering unlabeled N-dimensional Euclidean point clouds.  The algorithm, grounded in linear least squares, evaluates all potential point pairings and iteratively aligns the two point sets. This process continues until the count of point pairs falls below the predetermined maximum number of permissible one-to-one correspondences."}
{"text": "The multi-step process of preparing and scanning histopathology slides involves numerous parameters that can vary between laboratories and even within the same laboratory over time, leading to substantial variations in tissue appearance and hindering the generalization of automated image analysis techniques. While ad-hoc methods like staining normalization are often employed to mitigate this variability, this paper proposes a systematic solution utilizing domain-adversarial neural networks. We posit that eliminating domain information from the model representation enhances generalization capabilities. To validate this hypothesis, we focused on mitosis detection in breast cancer histopathology images, comparing our approach to two alternative methods. Our findings demonstrate that integrating color augmentation with domain-adversarial training outperforms conventional approaches in improving the generalization of deep learning methods."}
{"text": "This research explores the application of boosted regression trees for generating human-understandable solutions in reinforcement learning. By aggregating multiple regression trees, boosting enhances accuracy while preserving the inherent interpretability of individual trees. Although previous research has addressed reinforcement learning and interpretable machine learning separately, the intersection of these fields remains relatively unexplored. Our empirical findings demonstrate that boosted regression trees can produce solutions that are both interpretable and achieve performance comparable to state-of-the-art reinforcement learning techniques."}
{"text": "This work presents a novel image interpolation technique that aims to achieve smooth, minimally altered, and realistic transitions between input images. While the Wasserstein Barycenter Problem (WBP) offers a solution for minimizing changes, it may produce unnatural results. To address this, we propose a constrained WBP variant that incorporates an image prior, ensuring intermediate images adhere to desired characteristics. Our algorithm, demonstrated using sparse priors and generative adversarial networks, effectively combines minimal change with visual realism in image morphing."}
{"text": "This research proposes a novel approach to enhance clustering performance by focusing on the entanglement of latent code representations learned by an autoencoder, rather than employing the conventional deep clustering framework. Entanglement is defined as the proximity of data points belonging to the same class or structure relative to those from different classes or structures. The soft nearest neighbor loss, augmented with an annealing temperature factor, is utilized to quantify entanglement. This method achieved test clustering accuracies of 96.2% on MNIST, 85.6% on Fashion-MNIST, and 79.2% on EMNIST Balanced, surpassing the performance of baseline models."}
{"text": "This research introduces a novel approach to interpretable image synthesis by developing a hierarchical compositional AND-OR model with a sparse generator network. The model leverages a scene-objects-parts-subparts-primitives hierarchy, where scenes comprise various object types (OR) composed of multiple objects (AND), recursively extending to subparts and primitives. To implement this hierarchy, the generator network comprises two key components: (i) each hierarchical level is represented by an over-complete set of convolutional basis functions, realized using existing convolutional neural architectures, and (ii) sparsity-inducing constraints are integrated into end-to-end training, resulting in a sparsely activated and connected AND-OR model from the initial dense network. This sparsity is achieved by activating only the top-k basis functions at each layer. The learned basis functions also enable image reconstruction, providing interpretability by explaining input images. Experimental evaluation on four benchmark datasets demonstrates the effectiveness of the proposed method, achieving superior image synthesis and reconstruction quality compared to baseline approaches while learning meaningful and interpretable hierarchical representations."}
{"text": "Existing optical flow algorithm benchmarks assess performance through direct comparison of predicted flow fields with ground truth or indirectly by evaluating interpolated frames against actual frames using objective metrics like mean squared error. However, these measures may not fully capture the perceived image quality. To address this, a subjective quality assessment crowdsourcing study was conducted on interpolated frames from the Middlebury benchmark, employing forced-choice paired comparisons with ground truth and a novel artefact amplification technique to enhance sensitivity. Thurstone's model was used to reconstruct absolute quality scale values from the crowdsourced data, resulting in a re-ranking of the 155 participating algorithms based on visual quality. This highlights the importance of visual quality assessment as an additional evaluation metric for optical flow and frame interpolation benchmarks, providing ground truth for developing new image quality assessment (IQA) methods tailored to interpolated images. As an initial step, a new full-reference method called WAE-IQA was proposed, which outperformed the current best full-reference IQA approach by weighting local differences between interpolated and ground truth images."}
{"text": "Creating autonomous vehicles capable of navigating complex driving situations and interacting safely with other road users necessitates the ability to semantically comprehend the driving environment, often by analyzing vast amounts of naturalistic driving data. A crucial approach enabling autonomous vehicles to learn from human drivers and gain insights involves identifying the fundamental components of traffic flow, known as traffic primitives. However, the exponential growth of data presents a significant challenge in extracting primitives from high-dimensional time-series traffic data involving diverse road users. Consequently, automated primitive extraction has emerged as a cost-effective method to assist autonomous vehicles in understanding and predicting intricate traffic scenarios. Ideally, extracted primitives from raw data should be suitable for automated driving applications and readily usable for generating new traffic scenarios. Despite this need, existing literature lacks a method for automatically learning these primitives from large-scale traffic data. This paper makes two key contributions. Firstly, it proposes a novel framework for generating new traffic scenarios from limited data. Secondly, it introduces a nonparametric Bayesian learning method, a sticky hierarchical Dirichlet process hidden Markov model, to automatically extract primitives from multidimensional traffic data without prior knowledge of primitive configurations. The effectiveness of the developed method is validated using one day of naturalistic driving data. Experimental results demonstrate the ability of the nonparametric Bayesian learning method to extract primitives from traffic scenarios encompassing both binary and continuous events."}
{"text": "This work presents a collection of publicly available datasets from diverse areas of fundamental physics, including particle physics, astroparticle physics, and hadron- and nuclear physics, to facilitate supervised machine learning research. These datasets encompass hadronic top quarks, cosmic-ray induced air showers, phase transitions in hadronic matter, and generator-level histories, enabling cross-disciplinary and transfer learning applications. A novel, flexible graph-based neural network architecture is introduced, demonstrating performance comparable to state-of-the-art methods across all datasets. To promote wider adoption, clear guidelines for constructing graph-based representations of physics-relevant data structures are provided, along with code implementations for several examples and the proposed method, as well as reference algorithms."}
{"text": "This work tackles the challenge of generating accurate dense depth maps from single RGB images. Building upon a standard encoder-decoder convolutional neural network, we investigate the role of global information processing in enhancing depth estimation accuracy. To this end, we introduce AdaBins, a transformer-based architecture block that partitions the depth range into bins, with each bin's center value estimated adaptively for each input image. The final depth map is then reconstructed as a linear combination of these bin centers. Our experiments demonstrate significant performance gains over existing state-of-the-art methods across multiple benchmark datasets and evaluation metrics. We further validate the efficacy of AdaBins through ablation studies and make our code and pre-trained model weights publicly available."}
{"text": "This work investigates the capabilities and limitations of time-lagged autoencoders (TAEs) for slow mode discovery in dynamical systems through theoretical and numerical analyses. We derive theoretical bounds on TAE performance, demonstrating that they generally learn a combination of slow and maximum variance modes. Numerical experiments on a 2D \"Washington beltway\" potential and the alanine dipeptide molecule in explicit water illustrate cases where TAEs successfully and unsuccessfully identify the leading slowest mode.  We further compare TAE results with those from state-free reversible VAMPnets (SRVs), a variational neural network approach, revealing that SRVs can accurately discover slow modes in instances where TAEs fail."}
{"text": "This work introduces two novel metrics designed to assess the performance of generative models in class-conditional image generation. These metrics extend the widely used unconditional metrics, Inception Score (IS) and Fre'chet Inception Distance (FID), to the class-conditional setting. A theoretical framework is presented to justify the design of each proposed metric, establishing a connection between the new metrics and their unconditional counterparts through a product relationship for IS and an upper bound for FID.  An extensive empirical evaluation is conducted, comparing the proposed metrics to existing alternatives and leveraging them to analyze the performance of current generative models, offering insights into aspects such as unlearned classes and mode collapse."}
{"text": "The increasing accessibility of electronic health records (EHRs) has facilitated medical research, but cohort selection for rare diseases remains challenging due to limited record availability, hindering robust analysis.  To address this, we introduce ODVICE, a data augmentation framework that utilizes a medical concept ontology and a novel ontologically guided Monte-Carlo graph spanning algorithm to systematically augment EHR records. ODVICE empowers users with interactive controls to manage the augmentation process.  Evaluations on the MIMIC-III dataset for two learning tasks demonstrate ODVICE's efficacy, achieving a ~30% improvement in area under the curve (AUC) compared to non-augmented data and other augmentation strategies."}
{"text": "This paper presents two novel regularization techniques that directly modify weight matrices: Weight Reinitialization, which partially resets a sparse subset of parameters based on a simplified Bayesian assumption, and Weight Shuffling, which introduces entropy- and weight distribution-invariant non-white noise to the parameters, effectively acting as an ensemble method. The efficacy of these methods is evaluated on benchmark datasets, including MNIST, CIFAR-10, and the JSB Chorales database, as well as on time series modeling tasks. Results demonstrate improvements in both performance and network entropy. The code for these methods is publicly available on GitHub (https://github.com/rpatrik96/lod-wmm-2019)."}
{"text": "Irregularly sampled multivariate time series, frequently encountered in fields such as clinical research, climatology, and finance, pose unique challenges for data analysis. While recent research has primarily focused on classification, regression, and forecasting tasks with such data, forecasting in this context requires predicting both the value and the timing of events within the irregular time series. This work introduces a novel approach that addresses this challenge by forecasting both the values and their expected occurrence times."}
{"text": "Traditional image captioning models employ Convolutional Neural Networks (CNNs) to extract image features, which are then processed by recurrent models to generate captions. Recent research has explored the use of image scene graphs to enhance captioning models by incorporating structural semantic information, such as object entities, relationships, and attributes. However, directly utilizing scene graphs from black-box generators has been shown to negatively impact captioning performance, and existing scene graph-based models often require explicit use of image features for satisfactory results. To address these limitations, we introduce \\textbf, a novel framework that leverages only scene graph labels to achieve competitive image captioning performance. Our approach aims to bridge the semantic gap between the scene graph derived from the input image and the scene graph extracted from its caption by incorporating spatial object locations and Human-Object-Interaction (HOI) labels as an additional HOI graph. Experimental results demonstrate that SG2Caps significantly outperforms existing scene graph-only captioning models, highlighting the potential of scene graphs as a powerful representation for image captioning. By directly utilizing scene graph labels, our method eliminates the need for expensive graph convolutions on high-dimensional CNN features, resulting in a 49% reduction in trainable parameters. Our code is publicly available at: https://github.com/Kien085/SG2Caps."}
{"text": "Automatic semantic segmentation of fine-resolution remotely sensed imagery is crucial for applications like urban planning and environmental monitoring, but remains challenging due to the images' complexity. This paper presents A2-FPN, a novel framework for automated land segmentation based on the Feature Pyramid Network (FPN) enhanced with an Attention Aggregation Module (AAM). While FPN effectively builds a feature pyramid with high-level semantics, its feature extraction and fusion capabilities are limited. AAM addresses this by employing attention-guided feature aggregation to improve multi-scale feature learning. Experiments on three datasets demonstrate the superior segmentation accuracy of A2-FPN. Code is available at https://github.com/lironui/A2-FPN."}
{"text": "While CURL effectively utilizes contrastive learning for feature extraction from individual video frames in reinforcement learning, its treatment of consecutive frames as independent entities limits data efficiency. To address this, we propose masked contrastive representation learning for RL, which incorporates an auxiliary Transformer module to capture correlations between consecutive frames. This module, trained alongside the CNN encoder and policy network via contrastive learning on masked frame reconstruction, leverages contextual information to enhance feature representation. During inference, the Transformer module is discarded, and the CNN encoder and policy network guide action selection. Our method demonstrates consistent performance gains over CURL across a range of environments from DMControl and Atari 2600 Games. The code is publicly accessible at https://github.com/teslacool/m-curl."}
{"text": "The utilization of graphical information in semi-supervised classification has garnered significant attention, leading to the development of novel learning models that employ graph convolutions as a foundational step in data classification. To evaluate the efficacy of this approach, we investigate the classification of a Gaussian mixture, where node attributes correspond to a stochastic block model. Our findings demonstrate that graph convolution expands the range of linear separability by a factor of approximately 1/D, where D represents the average node degree, compared to the mixture model data alone. Moreover, we observe that the linear classifier, trained by minimizing cross-entropy loss after graph convolution, exhibits generalization capabilities to out-of-distribution data characterized by distinct intra- and inter-class edge probabilities from the training data."}
{"text": "This work presents a variational approach to multi-view shape-from-shading under natural lighting conditions. By formulating a coupled variational problem that links PDE-based shape-from-shading solutions across multiple images and color channels, we avoid the suboptimal results often obtained through iterative single-image solutions and cross-view consistency optimization. An efficient ADMM algorithm is employed to solve the coupled problem. Extensive experiments on both synthetic and real-world data demonstrate the effectiveness of our method, achieving highly accurate dense reconstructions without requiring dense correspondence estimation. This variational integration of multi-view information significantly enhances the applicability of shape-from-shading techniques to real-world reconstruction scenarios, enabling the recovery of fine-grained geometry even in regions with subtle brightness variations and limited texture."}
{"text": "Deep learning has enabled advancements in image classification using complex discriminative models. However, these models' complexity demands extensive labeled datasets for effective generalization. This is particularly challenging in medical image analysis, where limited training data is common, leading to overfitting. This paper introduces and examines a reinforced classifier designed to enhance generalization with limited training data. Drawing inspiration from reinforcement learning, the proposed classifier leverages generalization-feedback from a subset of the training data to refine its parameters, complementing the conventional cross-entropy loss. The classifier's efficacy is evaluated across three distinct classification tasks, comparing its performance against standard deep classifiers incorporating existing overfitting-prevention techniques. Results demonstrate not only an overall improvement in classification accuracy but also notable generalization capabilities, suggesting significant potential for medical classification applications."}
{"text": "While FPN-based detectors have demonstrated success in general object detection tasks like MS COCO and PASCAL VOC, their performance suffers in scenarios involving tiny object detection. This paper posits that the top-down connections within FPN exert a dual influence on tiny object detection, presenting both advantages and drawbacks. To address this, we introduce the concept of a \"fusion factor\" to regulate the information flow from deeper to shallower layers, thereby tailoring FPN for enhanced tiny object detection. Through experimentation and analysis, we develop a statistical method for estimating an optimal fusion factor value specific to a given dataset, based on the object distribution across layers.  Evaluations on tiny object detection datasets such as TinyPerson and Tiny CityPersons demonstrate that incorporating a properly configured fusion factor into FPN leads to substantial performance improvements over the baseline. Our code and models will be made publicly available."}
{"text": "This paper introduces face X-ray, a novel image representation for detecting face image forgery. Face X-ray generates a grayscale image that indicates the presence of image blending, revealing the blending boundary in forged images and the absence of blending in real images. Leveraging the common practice of blending altered faces with background images in most face manipulation techniques, face X-ray effectively detects forgery generated by these methods. Its generality stems from its reliance solely on the presence of a blending step, without requiring knowledge of specific artifact patterns associated with particular manipulation techniques. Notably, the face X-ray algorithm can be trained without using fake images from state-of-the-art face manipulation methods. Experiments demonstrate face X-ray's effectiveness in detecting forgery from unseen manipulation techniques, while existing face forgery or deepfake detection algorithms exhibit performance degradation."}
{"text": "While attention mechanisms have become prevalent, concerns remain regarding the interpretability of their distributions. Although attention offers insights into model operation, its use as a sole explanation for predictions is questionable.  The pursuit of more interpretable strategies for identifying key decision-making regions continues. To address this, we introduce Bilinear Representative Non-Parametric Attention (BR-NPA), a novel approach that captures task-relevant, human-interpretable information. BR-NPA first distills the target model to produce higher-resolution intermediate feature maps. Representative features are then grouped based on local pairwise similarity, resulting in finer-grained attention maps that highlight task-relevant input regions. These maps are ranked by the 'active level' of the compound feature, indicating the importance of the highlighted regions. BR-NPA's adaptability to various deep classification models, coupled with its accuracy, speed, and efficiency, makes it a valuable tool. Extensive experiments demonstrate its superior visual explanations compared to state-of-the-art models across tasks like few-shot classification, person re-identification, and fine-grained image classification. BR-NPA illuminates how neural networks focus their attention differently across various tasks."}
{"text": "This study experimentally evaluates various feature engineering pipelines, incorporating convolutional neural networks (CNNs), for classifying eyes-open and eyes-closed states from electroencephalogram (EEG) time-series data in the Bonn dataset. Leveraging Takens' embedding to generate simplicial complexes from EEG data, we compare two topological invariants derived from these complexes: \\epsilon-series of Betti-numbers and \\epsilon-series of graph spectra (a novel approach). These invariants, inspired by Topological Data Analysis, are used as features to capture the local geometry of the time-series and are compared against raw EEG time-series data, addressing a gap in the literature regarding benchmarking. Furthermore, we assess the robustness of these feature pipelines to downsampling and data reduction. This research aims to provide a clearer understanding of time-series classification using geometric features and how CNNs for time-series perform with data of reduced resolution."}
{"text": "Cervical spondylosis (CS), a prevalent chronic condition affecting a significant portion of the population, presents a substantial burden on both individuals and society. Early detection is crucial for enhancing treatment outcomes and minimizing costs. However, the complexity of the pathology and the subtlety of early symptoms hinder diagnosis, particularly in its initial stages. Furthermore, the time-consuming and expensive nature of hospital-based medical services often deters individuals from seeking timely CS identification. Consequently, there is an urgent need for a convenient and cost-effective intelligent CS identification method. This paper introduces an intelligent method leveraging deep learning to identify CS using surface electromyography (sEMG) signals. To address the challenges posed by the complexity, high dimensionality, and limited usability of sEMG signals, we propose and develop a multi-channel EasiCSDeep algorithm based on a convolutional neural network. This algorithm encompasses feature extraction, spatial relationship representation, and classification. To our knowledge, EasiCSDeep represents the first attempt to utilize deep learning and sEMG data for CS identification. Our algorithm demonstrates a significant improvement over existing state-of-the-art methods."}
{"text": "While the natural gradient's invariance to differentiable model reparameterizations is desirable, this property is compromised in practice due to the use of finite step sizes. This work investigates invariance from the perspectives of both Riemannian geometry and numerical differential equation solving, defining the order of invariance of a numerical method as its convergence rate to an invariant solution. To enhance invariance in optimization trajectories, we propose employing higher-order integrators and geodesic corrections. We establish the numerical convergence properties of geodesic-corrected updates, demonstrating their computational efficiency comparable to the standard natural gradient. Experimental results highlight the benefits of invariance, showcasing faster optimization and improved performance of our techniques over traditional natural gradient in deep neural network training and natural policy gradient for reinforcement learning."}
{"text": "Space-time video super-resolution (STVSR) seeks to enhance both the spatial and temporal resolutions of videos with low resolution and frame rates. While deformable convolution-based methods have shown promise in STVSR, they are limited to inferring pre-defined intermediate frames during training and often underutilize short-term motion information between adjacent frames. To address these limitations, this paper introduces the Temporal Modulation Network (TMNet) for accurately reconstructing arbitrary intermediate frames at high resolution. TMNet incorporates a Temporal Modulation Block (TMB) to modulate deformable convolution kernels, enabling controllable feature interpolation. Furthermore, a Locally-temporal Feature Comparison (LFC) module, combined with a Bi-directional Deformable ConvLSTM, is proposed to effectively capture both short-term and long-term motion cues within videos. Experimental results on three benchmark datasets demonstrate the superior performance of TMNet compared to existing STVSR methods. The code is accessible at https://github.com/CS-GangXu/TMNet."}
{"text": "While dropout is a common regularization technique used to prevent overfitting in deep neural networks by mitigating co-adaptation between nodes, its effectiveness may not be solely attributed to this mechanism. This paper proposes an alternative explanation for dropout's success, suggesting it functions as an optimization technique that encourages input values to reach the saturation region of nonlinear activation functions. By accelerating gradient flow in this region during backpropagation, dropout enhances the model's robustness. Based on this insight, a novel activation function design technique, GAAF, is introduced to further facilitate gradient flow in the saturation area. Experimental results validate the proposed dropout explanation and demonstrate that GAAF improves image classification performance as expected."}
{"text": "While deep learning excels at perceptual tasks and powers intelligent software, its vulnerability to adversarial examples poses a significant challenge. Existing defense mechanisms often prove attack-dependent, ineffective against novel attacks, or suffer from high computational overhead, hindering real-world deployment. To address these limitations, we introduce DAFAR, a feedback framework designed for effective and universal adversarial example detection and purification with minimal overhead. DAFAR's simple architecture comprises a victim model, a feedback network, and a detector. By leveraging high-level features from the victim model to reconstruct the input via a feedback autoencoder, DAFAR transforms subtle adversarial perturbations into easily detectable reconstruction errors, effectively mitigating both strong and weak attacks. Experiments on MNIST and CIFAR-10 datasets demonstrate DAFAR's efficacy against various advanced attacks without compromising performance on legitimate samples, highlighting its broad applicability and robustness."}
{"text": "This work presents an unsupervised video object segmentation method that leverages knowledge from image-based instance embedding networks. By generating an embedding vector for each pixel, these networks facilitate the identification of pixels belonging to the same object. Despite being trained on static images, the instance embeddings exhibit temporal stability across consecutive video frames, enabling object tracking over time.  Our approach adapts these pre-trained instance networks to video segmentation by integrating the embeddings with objectness and optical flow features, without requiring model retraining or online fine-tuning. Experimental results demonstrate the superiority of our method over existing state-of-the-art unsupervised segmentation techniques on the DAVIS and FBMS datasets."}
{"text": "While reward shaping effectively integrates domain knowledge into reinforcement learning (RL), relying entirely on potentially flawed human-derived shaping reward functions can hinder performance. This paper addresses the challenge of adaptively leveraging shaping rewards by framing it as a bi-level optimization problem. The lower level optimizes policy using shaping rewards, while the upper level optimizes a parameterized shaping weight function to maximize true reward. By deriving the gradient of expected true reward with respect to shaping weight function parameters, we propose three learning algorithms based on varying assumptions. Experiments in sparse-reward cartpole and MuJoCo environments demonstrate our algorithms' ability to capitalize on beneficial shaping rewards, disregard unbeneficial ones, and even transform detrimental rewards into advantageous ones."}
{"text": "Conditional normalizing flows (CNFs) offer a powerful approach for modeling conditional densities p(y|x) by leveraging the ability of normalizing flows (NFs) to capture complex, high-dimensional correlations. Unlike per-pixel loss-based methods, CNFs effectively model the intricate relationships between output dimensions, making them well-suited for multivariate structured prediction tasks. By conditioning the base density to output space mapping on an input x, CNFs enable efficient sampling and inference while allowing for training with a likelihood-based objective. As generative flows, CNFs avoid the pitfalls of mode collapse and training instabilities often encountered in other generative models. This study presents an effective method for training continuous CNFs for binary problems, demonstrating their competitive performance on super-resolution and vessel segmentation tasks using standard benchmark datasets, as evaluated by both likelihood and conventional metrics."}
{"text": "Scene flow, the understanding of 3D point motion in dynamic environments, is crucial for various robotics and human-computer interaction applications. Although existing methods primarily utilize stereo or RGB-D images, few explore direct scene flow estimation from point clouds. This work introduces FlowNet3D, a novel deep neural network designed to learn scene flow from point clouds in an end-to-end manner. FlowNet3D simultaneously learns hierarchical point cloud features and flow embeddings representing point motions, leveraging two newly proposed learning layers for point sets. The network's performance is evaluated on both synthetic data from FlyingThings3D and real Lidar scans from KITTI. Trained solely on synthetic data, FlowNet3D generalizes effectively to real scans, surpassing various baselines and achieving competitive results compared to previous methods.  Furthermore, we demonstrate the network's versatility through two applications: scan registration and motion segmentation, highlighting its potential for wide-ranging use cases."}
{"text": "This paper presents a novel framework for dense 3D reconstruction from monocular multispectral video sequences, leveraging a combined approach of semi-dense SLAM and multispectral photometric stereo. The framework first employs SALM to generate a semi-dense 3D shape and a sparse depth map from the multispectral video. This depth map serves as a prior for optimization-based multispectral photometric stereo, enabling accurate dense surface normal recovery. Subsequently, the recovered camera pose facilitates view conversion during fusion, where the sparse point cloud and dense surface normals are combined using a novel automated cross-scale fusion method. This process results in a dense point cloud enriched with subtle texture information. Experimental results demonstrate the effectiveness of the proposed method in achieving high-quality dense 3D reconstructions."}
{"text": "The rapid growth of generative adversarial network (GAN) research has led to significant advancements, particularly in computer vision, where GANs have enabled realistic image and video generation and manipulation.  However, GAN applications are expanding beyond computer vision to encompass diverse fields, including time series and sequence generation. This paper presents a review of GAN variants tailored for time series applications, proposing a taxonomy that categorizes them into discrete-variant GANs and continuous-variant GANs based on the type of time series data they handle. We highlight recent and prominent literature in this domain, examining their architectures, results, and applications. Furthermore, we provide an overview of commonly used evaluation metrics and their suitability across different applications.  The paper also discusses privacy considerations for GANs, exploring existing protection measures and outlining future directions for handling sensitive data. Our aim is to provide a clear and concise summary of the latest state-of-the-art research in this area and its implications for real-world technologies."}
{"text": "Monocular 3D object detection, crucial for cost-effective autonomous driving, presents a significant challenge due to the inherent lack of depth information. While advancements in 2D detection offer potential solutions, adapting them to 3D detection is non-trivial. This paper proposes FCOS3D, a general framework built upon a fully convolutional single-stage detector, to address this challenge. By transforming 7-DoF 3D targets to the image domain and decoupling them into 2D and 3D attributes, FCOS3D distributes objects across feature levels based on 2D scale and assigns them solely based on projected 3D-center during training.  Furthermore, center-ness is redefined using a 2D Gaussian distribution centered on the 3D-center to align with the 3D target formulation. This simple yet effective framework eliminates the need for 2D detection or 2D-3D correspondence priors. FCOS3D achieved first place among vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020. Code and models are available at https://github.com/open-mmlab/mmdetection3d."}
{"text": "Video feedback is essential in surgery, providing surgeons with primary sensory information.  Understanding surgical scenes is crucial for computer-assisted interventions (CAI) and post-operative analysis, with semantic segmentation of surgical instruments and anatomical structures being a fundamental component. While deep learning has significantly advanced semantic segmentation, it relies heavily on labeled datasets for training. This paper presents a new dataset for semantic segmentation of cataract surgery videos, utilizing annotated images from the publicly available CATARACTS challenge dataset.  Furthermore, the performance of several state-of-the-art deep learning models for semantic segmentation is evaluated on this dataset. The dataset is publicly accessible at https://cataracts.grand-challenge.org/CaDIS/ ."}
{"text": "Education is crucial for societal progress, as it disseminates knowledge and prepares future generations. Effective teachers tailor their materials, methodologies, and assessments to student learning styles. While artificial intelligence has focused primarily on machine learning, this paper advocates for increased attention to the role of teaching, proposing an optimization framework termed \"learning to teach.\" This approach involves two interacting intelligent agents: a student model representing the learner and a teacher model responsible for selecting data, loss function, and hypothesis space to optimize student training. Through reinforcement learning, the teacher model adapts its strategies based on student feedback, fostering co-evolution. The efficacy of this approach is demonstrated through deep neural network training, where \"learning to teach\" techniques enable comparable accuracy with significantly less data and fewer iterations across various DNN architectures and machine learning tasks."}
{"text": "Although reinforcement learning has achieved notable successes in domains such as games and robotics, its widespread practicality remains limited due to challenges related to sample efficiency and unreliable performance in infrequent but demanding situations. Motivated by the efficacy of deliberate practice in fostering expert-level human performance, we introduce a novel adversarial sampling method driven by a failure predictor termed \"CoachNet\". CoachNet is trained concurrently with the agent to estimate the likelihood of failure. This probability is then incorporated into a stochastic sampling process to direct the agent towards more challenging episodes. By concentrating training on the agent's areas of weakness rather than scenarios it has already mastered, this approach enhances sample efficiency. We detail the design and underlying principles of CoachNet and empirically validate its effectiveness in augmenting sample efficiency and test-time robustness across standard continuous control tasks."}
