{"text": "Aim: To create a robust and scalable method for predicting individual patient costs by automatically discerning concealed temporal patterns from multidimensional time series data in insurance claims, using a Convolutional Neural Network (CNN) design. Methodology: We employed three years' worth of medical and pharmacy claims data from 2013 to 2016, sourced from a healthcare insurer. The initial two years' data were utilized to construct the model, which was then applied to predict costs in the third year. The data was structured as patient health status matrices, with time windows on one axis and cost, visit, and medical features on the other. These patient health status matrices were processed through a customized CNN method, featuring a specific architecture. After optimizing hyperparameters, the architecture comprised three sequential blocks of convolution and pooling layers, each with an LReLU activation function and a tailored kernel size for healthcare data. The learned temporal patterns were fed into a fully connected layer.\n\nFindings: The proposed CNN configuration enhanced the precision of individual healthcare cost prediction by learning features. It outperformed methods that search for pre-defined pattern shapes, as it can extract a flexible number of patterns with diverse forms. The temporal patterns derived from medical, visit, and cost data substantially boosted the prediction performance. Hyperparameter tuning indicated that three-month data patterns yielded the highest prediction accuracy. Our findings suggest that patient images extracted from multidimensional time series data differ from standard images, necessitating unique CNN architecture designs."}
{"text": "The importance of nonlinearity lies in its ability to accurately predict future states of a dynamic system, its reaction to disturbances, and the identification of its underlying causal structure. However, conventional methods for causality detection and impulse response analysis, like Vector Autoregression (VAR), are linear and therefore lack the capacity to account for complexity. In this study, we propose a vector autoencoder nonlinear autoregression neural network (VANAR), which not only automates time series feature extraction for its inputs but also estimates the functional form. We assessed VANAR through three methods: first, by evaluating its forecasting accuracy, second, by determining the correct causality between variables, and third, by modeling trajectories following external shocks. These assessments were conducted on a simulated nonlinear chaotic system and an empirical system using Philippine macroeconomic data. The results indicate that VANAR significantly surpasses VAR in forecasting and causality tests. VANAR demonstrates superior accuracy even compared to advanced models such as SARIMA and TBATS. In the impulse response test, both models failed to predict the trajectories of the nonlinear chaotic system. VANAR proved to be robust in its ability to model a diverse range of dynamics, including chaotic, high-noise, low-data environments, and macroeconomic systems."}
{"text": "Suggested are novel generator architectures for Boundary Equilibrium Generative Adversarial Networks, inspired by Learning from Simulated and Unsupervised Images through Adversarial Training. These architectures aim to eliminate the reliance on a noise-based latent space, instead functioning primarily as refiner networks to achieve a photo-realistic representation of synthetic images. The proposed approach also seeks to address the poorly understood properties of the latent space by eliminating noise injection and replacing it with an image-based concept. This new, flexible, and simple generator architecture will also provide the ability to manage the balance between restrictive refinement and expressiveness. Unlike other existing methods, this architecture will not necessitate a paired or unpaired dataset of real and synthetic images for the training phase. Instead, a relatively small set of real images would be sufficient."}
{"text": "General-purpose robots can acquire diverse behavioral skills through autonomous learning, eliminating the need for extensive manual engineering. However, existing robotic skill learning methods often compromise in various ways to facilitate real-world application, such as necessitating manually designed policy or value function representations, reliance on human-supplied demonstrations, modification of the training environment, or prolonged training periods. This paper introduces a novel reinforcement learning algorithm for teaching manipulation skills, which minimizes human engineering while maintaining swift, efficient learning in unpredictable environments. Our method is based on the guided policy search (GPS) algorithm, converting the reinforcement learning problem into supervised learning from a computational instructor (without human demonstrations). Unlike previous GPS methods, our approach accommodates randomized initial states, making it applicable in environments where deterministic resets are unattainable. We assess our method against existing policy search techniques in simulations, demonstrating its ability to train complex neural network policies with the same sample efficiency as prior GPS methods, and provide real-world results using a PR2 robotic manipulator."}
{"text": "The proposed approach entails a hierarchical image segmentation technique, which constructs a dissimilarity graph over the image, subsequently over-segments it into watershed basins. A fresh graph is then established on these basins, followed by the merging of basins using a size-dependent, modified variant of single linkage clustering. The method's quasi-linear runtime renders it ideal for segmenting large-scale images. This technique is demonstrated through the complex task of segmenting 3D electron microscopic brain images."}
{"text": "In this research, we delve into the issue of identifying anomalous arms in multi-armed bandit scenarios, a problem that holds significant relevance across various high-impact sectors such as finance, healthcare, and online advertising. The objective is to discern arms whose anticipated rewards significantly deviate from the majority of other arms. Unlike previous research, our focus is on generic outlier arms or outlier arm groups, whose expected rewards may exceed, fall below, or align with those of typical arms. To address this, we first present a comprehensive definition of such generic outlier arms and outlier arm groups. Subsequently, we introduce a novel algorithm, GOLD, designed to detect these generic outlier arms. This algorithm constructs a real-time neighborhood graph using upper confidence bounds and identifies the behavioral patterns of outliers among normal arms. We further analyze the performance of GOLD from multiple perspectives. In experiments conducted on both simulated and real-world data sets, the proposed algorithm demonstrates an accuracy of 98% while reducing the average exploration cost by 83% compared to current state-of-the-art techniques."}
{"text": "This study introduces a new approach for concurrently developing a parametric 3D face model and 3D face reconstruction from a variety of sources. Traditionally, 3D face modeling methods learn from a single type of source, such as scanned data or in-the-wild images. While scanned data offer precise geometric details of facial shapes, the associated capture systems are costly, and these datasets often consist of a limited number of subjects. In contrast, in-the-wild images are abundant, yet they lack explicit geometric information. In this research, we put forth a method to create a unified face model from multiple sources. In addition to scanned face data and face images, we also employ a vast collection of RGB-D images captured using an iPhone X to bridge the gap between the two sources. Our experimental findings indicate that a more potent face model can be learned with training data from multiple sources."}
{"text": "The method of Differentiable Neural Architecture Search (DNAS) stands out for its efficiency and simplicity in Neural Architecture Search (NAS), achieved by concurrently optimizing model weights and architecture parameters within a weight-sharing supernet via gradient-based algorithms. During the search phase, operations with the largest architecture parameters are chosen to construct the final architecture, under the assumption that the magnitude of architecture parameters signifies the operation's strength. However, this assumption has been under-explored. This work offers both empirical and theoretical evidence to demonstrate that the size of architecture parameters does not always correlate with the operation's impact on the supernet's performance. Instead, we propose an alternative architecture selection method based on perturbations, which directly quantifies each operation's impact on the supernet. We apply this method to various differentiable NAS techniques and discover that it consistently extracts improved architectures from the supernets. Moreover, we identify that several issues in DARTS can be significantly mitigated with the proposed selection method, suggesting that a substantial portion of DARTS' poor generalization may be due to the inadequacy of magnitude-based architecture selection, rather than solely the optimization of its supernet."}
{"text": "In the realm of image processing, particularly for object detection, the significance of edge detection algorithms cannot be overstated. This is due to the fact that edges delineate the contours of objects, serving as the boundary between an object and its background, and demarcating the borders of overlapping objects. Accurate identification of edges in an image enables the location of all objects and the measurement of fundamental properties such as area, perimeter, and shape. Given the importance of computer vision, which encompasses the identification and categorization of objects within an image, edge detection emerges as an indispensable tool. In our study, we evaluated two edge detectors employing distinct methods for edge detection and compared their performance across various scenarios to ascertain the superiority of each detector under distinct conditions."}
{"text": "Novel causal discovery methods typically construct a new model for each sample derived from a distinct causal graph. Yet, these samples frequently contain common dynamics, such as those illustrating the impact of causal relationships, which are discarded using this methodology. This study introduces Amortized Causal Discovery, a novel framework that capitalizes on these shared dynamics to deduce causal relationships from time-series data. This innovation allows for the training of a solitary, amortized model that infers causal relationships across samples with varying underlying causal graphs, thereby utilizing the shared information. Experimental results indicate that this approach, realized as a variational model, yields substantial enhancements in causal discovery efficiency, and we illustrate its extension to excel under concealed confounding."}
{"text": "Neural attention (NA) has emerged as a crucial element in sequence-to-sequence models, delivering top-tier performance in complex tasks such as abstractive document summarization (ADS) and video captioning (VC). NA mechanisms operate by inferring context vectors, which are weighted sums of input sequence encodings, dynamically selected over extended time spans. Drawing inspiration from recent advancements in amortized variational inference (AVI), this study proposes treating the context vectors produced by soft-attention (SA) models as latent variables. We infer their approximate finite mixture model posteriors via AVI. We hypothesize that this approach may enhance generalization capacity, consistent with the results of previous AVI applications to deep networks. To demonstrate our approach, we develop and experimentally assess it on challenging ADS, VC, and MT benchmarks. This demonstrates its superior performance compared to state-of-the-art alternatives."}
{"text": "The optimal method for acquiring information about an unforeseen state is through the use of informative measurements. We present a first-principles derivation of a versatile dynamic programming algorithm, which generates a series of informative measurements by successively maximizing the entropy of potential measurement outcomes. This algorithm is adaptable for use by an autonomous agent or robot, aiding in the determination of the most advantageous location for the next measurement, thereby planning an optimal sequence of informative measurements. This algorithm is applicable to states and controls that are either continuous or discrete, and to agent dynamics that are either stochastic or deterministic, including Markov decision processes. Recent advancements in approximate dynamic programming and reinforcement learning, including real-time solutions like rollout and Monte Carlo tree search, enable an agent or robot to address the measurement task in a timely manner. The nearly optimal solutions produced by this method include non-myopic paths and measurement sequences that often surpass, sometimes significantly, the performance of conventional greedy heuristics such as maximizing the entropy of each individual measurement outcome. This is substantiated for a global search problem, where online planning with an extended local search is found to cut the number of measurements in the search by half."}
{"text": "The objective of this research is to create and execute an autonomous anomaly detection algorithm for meteorological time-series data. To attain this objective, we devise a method for assembling an ensemble of anomaly detectors, coupled with adaptive threshold selection, which is informed by artificially generated anomalies. The effectiveness of the proposed method is validated by incorporating its implementation into the \"Minimax-94\" road weather information system."}
{"text": "The focus of our research is on the issue of Salient Object Subitizing, which involves predicting the presence and quantity of prominent objects within an image using global cues. This task is based on the human ability to swiftly and accurately determine the number of items within the subitizing range (1-4). To address this, we have developed a dataset of approximately 14,000 everyday images, annotated through an online crowdsourcing platform. Our results indicate that an end-to-end trained Convolutional Neural Network (CNN) model can achieve prediction accuracy comparable to human performance in identifying images with zero or one salient object. Furthermore, our model exhibits significantly better than chance performance in images with multiple salient objects, without the need for a localization process. Additionally, we propose a method to enhance the training of the CNN subitizing model using synthetic images. Our experiments demonstrate the precision, applicability, and effectiveness of our CNN subitizing model in salient object detection and image retrieval."}
{"text": "In this research, we investigate a dynamic assortment planning problem during a fixed selling season of duration T. At each time interval, a customer is presented with a selection of interchangeable products, and the customer makes a purchase from the presented options based on a discrete choice model. The objective of the seller is to maximize the expected revenue, or alternatively, to minimize the maximum expected regret. A significant hurdle is that the product utilities are unknown to the seller and must be learned. While the dynamic assortment planning problem has gained significant attention in revenue management, most existing work relies on multinomial logit choice models (MNL). In this paper, we explore the problem of dynamic assortment planning under a more comprehensive choice model -- the nested logit model, which models hierarchical choice behavior and is considered the \"most widely used member of the GEV (generalized extreme value) family\". By utilizing the revenue-ordered structure of the optimal assortment within each nest, we develop a novel upper confidence bound (UCB) policy with an integrated estimation scheme. This policy concurrently learns customer choice behavior and makes dynamic decisions on assortments based on the current knowledge. It achieves an accumulated regret of the order of ~, where M is the number of nests and N is the number of products in each nest. We also provide a lower bound result of ~, which demonstrates the near optimality of the upper bound when T is significantly larger than M and N. When the number of items per nest N is substantial, we offer a discretization heuristic to enhance the performance of our algorithm. Empirical results are presented to illustrate the practical performance of our proposed algorithms."}
{"text": "In this study, we focus on the estimation of Conditional Average Treatment Effects (CATEs) in graph-structured scenarios, such as molecular graphs of pharmaceuticals. Under a mild assumption on the effect, we introduce a plug-in estimator that breaks down CATE estimation into manageable, simpler optimization tasks. This estimator (a) separates the causal estimands, thereby minimizing regularization bias, and (b) accommodates the use of any learning models. Experimental results with small-world and molecular graphs demonstrate the superiority of our method over previous approaches and its resilience to varying selection biases. Our method is available online for implementation."}
{"text": "Current research on action recognition tends to view activities as single, discrete events within videos. The potential of defining actions as a collection of fundamental action units has demonstrated significant improvements in action comprehension, thanks to the development of datasets annotated in this manner, enabling the learning of representations that capture this information. Nevertheless, there is a dearth of studies that expand upon action composition and utilize multiple perspectives and multiple types of data for representation learning. To stimulate research in this area, we present Home Action Genome (HOMAGE): a multi-view action dataset with multiple modalities and viewpoints, accompanied by hierarchical activity and atomic action labels, as well as dense scene composition labels. By capitalizing on the wealth of multi-modal and multi-view settings, we propose Cooperative Compositional Action Understanding (CCAU), a collaborative learning framework for hierarchical action recognition that is mindful of compositional action components. CCAU consistently delivers performance improvements across all modalities. Additionally, we demonstrate the value of co-learning compositions in few-shot action recognition, achieving a mAP of 28.6% with only a single sample."}
{"text": "The challenge of restoring seismic data with missing traces has been a persistent issue in seismic data processing. In recent times, rank reduction operations have become popular solutions, yet they necessitate a predetermined rank for seismic data, which is often unknown and time-consuming to estimate manually. Traditional methods based on deep learning demand extensive datasets for training, a challenge due to practical limitations in data acquisition. To address this, this study introduces a novel unsupervised learning method, leveraging the inherent properties of the U-net convolutional neural network, without the need for training datasets. This method requires only undersampled seismic data and can autonomously utilize the deep seismic prior of the input data. It is capable of handling both irregular and regular seismic data. To evaluate its performance, the DSPRecon algorithm was tested against the singular spectrum analysis (SSA) method for irregular data reconstruction and the de-aliased Cadzow method for regular data reconstruction. The experimental results demonstrated that the DSPRecon method outperformed both the SSA and Cadzow methods, with recovered signal-to-noise ratios (SNRs) of 32.68 dB and 19.11 dB, respectively, for the DSPRecon and SSA algorithms, and 35.91 dB and 15.32 dB, respectively, for the DSPRecon and Cadzow methods."}
{"text": "The Bokeh aesthetic is employed in photography to create images where nearby objects appear crisp, while distant ones remain blurred. Such images are typically captured using Single Lens Reflex cameras, utilizing a narrow depth-of-field. Modern smartphones can also generate Bokeh images, often through the use of dual rear cameras or advanced auto-focus hardware. However, for smartphones with a single rear camera and subpar auto-focus capabilities, software is utilized to produce Bokeh images. This software-based system can also be employed to add a Bokeh effect to previously captured images. In this research, a comprehensive deep learning framework is presented to generate high-quality Bokeh effects from images. This is achieved by blending the original image with various smoothed versions, aided by a monocular depth estimation network. The proposed method is compared to a saliency detection-based baseline, as well as several approaches from the AIM 2019 Challenge on Bokeh Effect Synthesis. Extensive experiments are conducted to analyze the various components of the proposed algorithm. The network is lightweight and can process an HD image in just 0.03 seconds. This approach secured the second place in the AIM 2019 Bokeh effect challenge-Perceptual Track."}
{"text": "In this study, we introduce PointDAN, a novel 3D Domain Adaptation Network designed specifically for point cloud data. PointDAN addresses the scarcity of methods for direct domain adaptation in 3D point cloud data by focusing on its unique spatial geometric information and the semantic contribution of regional geometric structures. General-purpose domain adaptation methods, which often struggle with global feature alignment and disregard local geometric information, are inadequate for 3D domain alignment. To overcome this, PointDAN concurrently aligns global and local features at multiple levels. For local alignment, we develop the Self-Adaptive (SA) node module, which adjusts its receptive field to model discriminative local structures for domain alignment. To represent hierarchically scaled features, a node-attention module is added to weight the relationships between SA nodes across objects and domains. For global alignment, an adversarial-training strategy is employed to learn and align global features across domains. Due to the lack of a common evaluation benchmark for 3D point cloud domain adaptation scenarios, we create a general benchmark, PointDA-10, by extracting data from three popular 3D object/scene datasets (ModelNet, ShapeNet, and ScanNet) for cross-domain 3D object classification. Experimental results on PointDA-10 demonstrate the superiority of our model over existing general-purpose domain adaptation methods."}
{"text": "Neural network models for computer vision rely heavily on visual attention mechanisms, which selectively focus on specific objects or image regions to highlight the most pertinent features. These mechanisms construct more potent representations by concentrating on the most relevant features. In recent times, continuous-domain alternatives to discrete attention models have emerged, capitalizing on the continuous nature of images. These models represent attention as straightforward unimodal densities, such as a Gaussian, but they struggle with images that have complexly shaped regions of interest or are composed of multiple non-contiguous patches. In this study, we introduce a novel continuous attention mechanism that generates multimodal densities, specifically mixtures of Gaussians. We employ the EM algorithm to cluster significant regions in the image and use a description length penalty to determine the number of components in the mixture. Our densities can be expressed as a linear combination of unimodal attention mechanisms, providing closed-form Jacobians for the backpropagation step. Our experiments on the VQA-v2 dataset demonstrate competitive accuracies and regions of interest that more closely resemble human attention in VQA-HAT. We provide several examples that indicate how multimodal attention maps are more naturally interpretable than their unimodal counterparts, demonstrating our model's ability to automatically separate objects from their background in complex scenes."}
{"text": "This study introduces a groundbreaking 3D face recognition algorithm, which combines a deep convolutional neural network (DCNN) and a 3D augmentation technique. The advancements in 2D face recognition performance are primarily attributed to the enhanced representational power of deep neural networks and the utilization of extensive labeled training data. In contrast, developing discriminative deep features for 3D face recognition is challenging due to the scarcity of large-scale 3D face datasets. In this research, we demonstrate that transfer learning from a CNN trained on 2D face images can be effectively applied to 3D face recognition by fine-tuning the CNN with a limited number of 3D facial scans. Furthermore, we introduce a 3D face augmentation technique that generates various facial expressions from a single 3D face scan. Our proposed method achieves impressive recognition rates on Bosphorus, BU-3DFE, and 3D-TEC datasets, without relying on hand-crafted features. Additionally, the 3D identification using our deep features exhibits scalability for large databases."}
{"text": "In the realm of deep learning-driven automatic colorization, there remains a limitation in terms of few-shot learning capabilities. Traditional models demand a substantial volume of training data. To address this challenge, we introduce a novel memory-enhanced colorization model, MemoPainter, capable of generating high-quality colorization with minimal data. Notably, our model effectively captures and colors rare instances. Furthermore, we put forth a novel threshold triplet loss, which facilitates unsupervised training of memory networks without the necessity of class labels. Our experiments demonstrate that MemoPainter outperforms other models in both few-shot and one-shot colorization tasks."}
{"text": "Although Generative Adversarial Networks (GANs) have achieved success in generating realistic images, their potential for tasks unrelated to synthesis remains under-investigated. Specifically, does the learning process of GANs involve the recognition of meaningful structural components of objects? In this study, we examine this hypothesis and introduce a straightforward and efficient method for semantic part segmentation using GANs. This method necessitates only a single labeled example in conjunction with an unlabeled dataset. Our approach capitalizes on a pre-trained GAN to extract pixel-wise representations from the input image, which are then utilized as feature vectors for a segmentation network. Our findings indicate that the GAN-derived representations are \"readily discriminative\" and yield results that are remarkably comparable to those produced by supervised baselines trained with a substantially larger number of labels. We posit that this innovative re-purposing of GANs opens up a new avenue for unsupervised representation learning, with potential applications in a wide range of tasks. Further results can be found at https://repurposegans.github.io/."}
{"text": "The presented work offers a straightforward, efficient algorithm for hyperparameter optimization, drawing upon methods from the study of Boolean functions. The primary focus lies in the high-dimensional realm, typified by training neural networks with a substantial number of hyperparameters. The proposed algorithm, an iterative application of compressed sensing techniques for orthogonal polynomials, necessitates only uniform sampling of hyperparameters, making it highly parallelizable.\n\nExperiments conducted on Cifar-10 for deep neural network training demonstrate that our algorithm produces significantly superior solutions compared to contemporary tools such as Hyperband and Spearmint, even surpassing what can be achieved through manual tuning in certain cases. In terms of total running time, which encompasses the time spent sampling various hyperparameter settings and additional computational time, our algorithm is at least an order of magnitude faster than Hyperband and Bayesian Optimization. Furthermore, it outperforms Random Search by a factor of 8.\n\nMoreover, our method is backed by theoretical guarantees and marks the first advancement in the sample complexity of learning decision trees in over two decades. Specifically, we have developed the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity."}
{"text": "We introduce a metric, Layer Saturation, which is the ratio of the number of eigenvalues required to account for 99% of the variance in the latent representations of neural network layers. This metric, grounded in spectral analysis, can be computed efficiently, enabling real-time analysis of representations during training. We offer potential avenues for future research by discussing the behavior of layer saturation in various neural architectures and problems. Additionally, we demonstrate a connection between saturation and the generalization and predictive capabilities of neural networks."}
{"text": "In the realm of connected and autonomous vehicles (CAV), the importance of situational awareness has garnered considerable attention in contemporary research. The safety of drivers hinges on the robustness, reliability, and scalability of these systems. Cooperative mechanisms, leveraging high-speed wireless vehicular networks, have emerged as a solution to enhance situational awareness, addressing issues like occlusion and sensor range limitations. Nevertheless, the network capacity plays a crucial role in determining the maximum amount of information shared among cooperative entities. Our previous work introduced the concept of feature sharing to tackle these challenges by striking a balance between computational and communicational loads. In the present study, we introduce a flexible mechanism to adapt to communication channel capacity and a novel, decentralized shared data alignment method to further boost cooperative object detection performance. The efficacy of our proposed framework is validated through experiments on the Volony dataset. The findings indicate that our proposed framework surpasses our previous cooperative object detection method (FS-COD) in terms of average precision."}
{"text": "The proposed deep learning approach, centered on structured output learning, exhibits potential in tasks such as semantic image segmentation. We introduce an efficient deep learning scheme for model learning, demonstrating the application of Convolutional Neural Networks (CNNs) to estimate messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). By employing CNN message estimators, we eliminate the requirement for learning or evaluating potential functions in message calculation. This enhancement significantly improves learning efficiency, as it avoids the need for costly inference during every stochastic gradient iteration when performing structured learning for a CRF with CNN potentials. Unlike the network output for general CNN potential functions in CRFs, which is exponentially dependent on the order of the potentials, the network output dimension for message estimation is equivalent to the number of classes. This results in fewer network parameters and increased scalability for cases involving a large number of classes. We apply this method to semantic image segmentation on the PASCAL VOC 2012 dataset, achieving an intersection-over-union score of 73.4 on its test set, the best reported result for methods using the VOC training images alone. This outstanding performance underscores the efficacy and utility of our CNN message learning method."}
{"text": "Our study introduces Earliness-Aware Deep Convolutional Networks (EA-ConvNets), a deep learning framework for the early classification of time series data. Unlike many existing methods, which rely on pre-defined features, our framework concurrently executes feature learning (by learning a deep hierarchy to capture salient characteristics in each time series) and a dynamic truncation model to focus on the early parts of each time series. This allows for highly reliable early predictions, surpassing numerous state-of-the-art methods for early time series classification, and remaining competitive with state-of-the-art time series classification algorithms. To our knowledge, this is the first framework to perform data-driven (deep) feature learning in the context of early classification of time series data. We conduct extensive experiments on various benchmark datasets, demonstrating that our method delivers significantly better predictions than contemporary methods for early time series classification. Furthermore, our experiments reveal that the learned deep shapelets-based features are highly interpretable, aiding in a better comprehension of the inherent characteristics of time series data."}
{"text": "In this study, we delve into the application of influence functions, a well-known instance-based interpretation technique, within the realm of unsupervised learning, particularly for deep generative models known as variational auto-encoders (VAE). We formulate the counter-factual question addressed by influence functions in this context and conduct a theoretical analysis to elucidate the influence of training samples on traditional unsupervised learning methods. Subsequently, we present VAE-TracIn, an efficient and theoretically robust solution for VAEs, inspired by Pruthi et al. [28]. Lastly, we assess the performance of VAE-TracIn using various real-world datasets, providing both quantitative and qualitative analysis."}
{"text": "Deep generative models have achieved significant success in modeling continuous data, yet the challenge persists in representing discrete structures with formal grammars and semantics, such as computer programs and molecular structures. The generation of data that is both syntactically and semantically correct remains largely unsolved. Drawing inspiration from compiler theory, where syntax and semantics checks are performed through syntax-directed translation (SDT), we introduce a novel syntax-directed variational autoencoder (SD-VAE) by incorporating stochastic lazy attributes. This method transforms the offline SDT check into real-time guidance for controlling the decoder. In contrast to existing methods, our approach imposes constraints on the output space, ensuring that the output is not only syntactically valid, but also semantically plausible. We test the proposed model in applications related to programming languages and molecules, including reconstruction and program/molecule optimization. The results indicate the model's effectiveness in incorporating syntactic and semantic constraints in discrete generative models, outperforming current state-of-the-art approaches."}
{"text": "This study revisits the continuous optimization framework known as NOTEARS, designed for learning Bayesian networks. Initially, we broaden the algebraic representations of acyclicity to a group of matrix polynomials. In a one-parameter-per-edge setting, we demonstrate that the Karush-Kuhn-Tucker (KKT) optimality conditions for the NOTEARS formulation can only be met in a trivial scenario, elucidating the behavior of the associated algorithm. Subsequently, we establish the KKT conditions for an equivalent reformulation, verify their necessity, and connect them to explicit conditions that certain edges must not be present in the graph. When the score function is convex, these KKT conditions are not only necessary but also sufficient for local minimality, despite the non-convexity of the constraints. Inspired by the KKT conditions, a local search post-processing algorithm is proposed, and it is shown to significantly and universally enhance the structural Hamming distance of all tested algorithms, often by a factor of 2 or more. Some combinations of this algorithm with local search are not only more accurate but also more efficient than the original NOTEARS."}
{"text": "The use of Braille has enabled the visually impaired community to engage in reading and writing, yet it has simultaneously introduced a divide due to the widespread inability of non-Braille users to comprehend Braille scripts. This divide has stimulated researchers to develop Optical Braille Recognition methods, aimed at converting Braille documents into natural language. The primary objective of this research is to bridge the communication gap in academic institutions by translating the personal documents of visually impaired students. This objective has been achieved by proposing an affordable and efficient technique that digitizes Braille documents using a smartphone camera. For any given Braille image, a dot detection mechanism based on Hough transform is proposed, which is resilient to skewness, noise, and other obstacles. The detected dots are subsequently grouped into Braille cells using a distance-based clustering algorithm. Subsequently, the standard physical parameters of each Braille cell are estimated for feature extraction and classification as natural language characters. The thorough evaluation of this technique on a dataset of 54 Braille scripts has resulted in an accuracy of 98.71%."}
{"text": "Our proposed method for relocalization in large-scale point clouds combines global place recognition and local 6DoF pose refinement in a unified approach. We develop a Siamese network that concurrently learns 3D local feature detection and description from raw 3D points. This network incorporates FlexConv and Squeeze-and-Excitation (SE) to ensure the learned local descriptor captures multi-level geometric information and channel-wise relationships. We predict 3D keypoints in an unsupervised manner based on the discriminativeness of local descriptors. The global descriptor is generated by aggregating the learned local descriptors using an effective attention mechanism. This process infers both local and global 3D descriptors in a single forward pass. Experiments on various benchmarks show that our method competes effectively with state-of-the-art approaches for both global point cloud retrieval and local point cloud registration. To verify the generalizability and robustness of our 3D keypoints, we demonstrate that our method also performs well on point clouds generated by a visual SLAM system without fine-tuning. Code and related materials can be found at https://vision.in.tum.de/research/vslam/dh3d."}
{"text": "This paper introduces Georeference Contrastive Learning of visual Representation (GeoCLR), a technique for streamlined training of Convolutional Neural Networks (CNNs). GeoCLR utilizes geographical data by creating image pairs from photos taken in proximity, contrasting them with pairs from distant locations. The rationale is that images captured in close proximity tend to share visual similarities, a notion that holds true in seafloor robotic imaging, where image areas are typically a few meters in size and are taken to overlap along the vehicle's path. In contrast, seafloor substrates and habitats have much larger patch sizes. A significant advantage of GeoCLR is its self-supervised nature, requiring no human intervention for CNN training. It is computationally efficient, allowing for results to be produced during multi-day AUV missions using resources typically available in oceanic field trials. We employ GeoCLR for habitat classification on a dataset comprising approximately 86k images collected by an Autonomous Underwater Vehicle (AUV). We showcase the utility of GeoCLR's latent representations in guiding human annotation efforts, as the semi-supervised framework enhances classification accuracy by an average of 11.8% compared to state-of-the-art transfer learning using the same CNN and an equivalent number of human annotations for training."}
{"text": "A video comprehension model should learn the intricate relationship between a scene's static elements and its dynamic changes: When presented with an image, the model should be capable of forecasting the subsequent evolution of the depicted scene, and conversely, a video should be interpreted based on its static image content and all other attributes not initially visible. This implies a bijective mapping between the video domain and both the static content and residual information. Unlike typical stochastic image-to-video generation, this model does not produce random videos that merely progress from the initial image. Instead, it offers a one-to-one mapping between residual vectors and the resulting video, with stochastic outcomes when sampling. This approach is effectively implemented using a conditional invertible neural network (cINN), which can independently model static and other video characteristics, thereby establishing a foundation for controlled video synthesis. Our method's efficacy, in terms of both quality and diversity of the synthesized results, is demonstrated through experiments on four diverse video datasets. Our project details can be found at https://bit.ly/3t66bnU."}
{"text": "The burgeoning focus on transformers has prompted speculation about their capacity to serve as robust \"universal\" models for a variety of computer vision tasks, including classification, detection, and segmentation. Most research in this area has primarily focused on discriminative models. However, our study delves into the application of transformers in more challenging vision tasks, such as generative adversarial networks (GANs). Our objective is to initiate a preliminary investigation into constructing a GAN devoid of convolutions, utilizing solely transformer-based architectures. Our basic GAN architecture, named TransGAN, comprises a memory-efficient transformer-based generator that incrementally enhances feature resolution, and a multi-scale discriminator designed to simultaneously capture semantic contexts and low-level textures. To address the memory bottleneck, we incorporate a novel grid self-attention module. Furthermore, we devise a distinct training methodology incorporating various techniques to address the training instability issues of TransGAN, including data augmentation, modified normalization, and relative position encoding. Our best architecture demonstrates competitive performance compared to current state-of-the-art GANs with convolutional backbones. Notably, TransGAN sets new records for inception score (10.43) and FID (18.28) on STL-10, surpassing StyleGAN-V2. For higher-resolution generation tasks (e.g., 256 x 256), such as CelebA-HQ and LSUN-Church, TransGAN continues to generate visually diverse examples with high fidelity and intricate texture details. Additionally, we delve into the behavioral differences between transformer-based and convolutional generation models by visualizing training dynamics. The code for TransGAN is accessible at https://github.com/VITA-Group/TransGAN."}
{"text": "In the presented research, we propose SalsaNext, a real-time, uncertainty-aware semantic segmentation method for complete 3D LiDAR point clouds. SalsaNext is an advancement of SalsaNet [1], featuring an encoder-decoder structure with a ResNet block-equipped encoder and a decoder that integrates upsampled features from residual blocks. Unlike SalsaNet, we incorporate a novel context module, replace the ResNet encoder blocks with a new residual dilated convolution stack expanding gradually in receptive fields, and insert a pixel-shuffle layer in the decoder. We also replace stride convolution with average pooling and apply central dropout. To directly optimize the Jaccard index, we merge the weighted cross-entropy loss with Lovasz-Softmax loss [2]. Lastly, we implement a Bayesian treatment to determine the epistemic and aleatoric uncertainties for each point in the cloud. Our comprehensive quantitative assessment on the Semantic-KITTI dataset [3] indicates that SalsaNext surpasses other leading semantic segmentation networks and tops the Semantic-KITTI leaderboard. We have made our source code publicly available at https://github.com/TiagoCortinhal/SalsaNext."}
{"text": "Critical care necessitates effective glycemic management, yet the lack of research on individualized optimal strategies complicates this task. This study endeavors to develop personalized optimal glycemic trajectories for severely ill septic patients by employing data-driven policies to identify optimal target blood glucose levels for clinicians. We represented patient conditions using a sparse autoencoder and utilized a reinforcement learning approach with policy iteration to derive the optimal policy from data. We further calculated the anticipated return based on the policy derived from the recorded glycemic trajectories, resulting in a function illustrating the correlation between actual blood glucose values and 90-day mortality rates. This implies that the derived optimal policy could potentially decrease the estimated 90-day mortality rate by 6.3%, from 31% to 24.7%. The findings indicate that reinforcement learning, combined with suitable patient state encoding, may offer optimal glycemic trajectories, enabling clinicians to create personalized glycemic management strategies for septic patients."}
{"text": "In the realm of reinforcement learning, the real-world state is typically represented by feature vectors. Yet, not all these features are essential for addressing the current task. We introduce Feature Selection Explore and Exploit (FS-EE), an algorithm that autonomously chooses the essential features during the learning process of a Factored Markov Decision Process. Under reasonable assumptions, we demonstrate that the algorithm's sample complexity is proportional to the in-degree of the dynamics of only the necessary features, rather than the in-degree of all features. This can lead to significantly improved sample complexity when the in-degree of the necessary features is smaller than the in-degree of all features."}
{"text": "In autonomous systems, vision-based depth estimation is a crucial component, frequently utilizing a single camera or multiple independent ones. In a monocular setup, obtaining dense depth is typically accomplished through additional input from expensive LiDARs with numerous beams, such as 64, or camera-only methods, which are plagued by scale-ambiguity and infinite-depth issues. This paper introduces a novel approach to densely estimate metric depth by integrating a lightweight LiDAR, typically featuring 4 beams, similar to contemporary automotive-grade mass-produced laser scanners, with a monocular camera. Drawing inspiration from recent self-supervised methods, we present a new framework, LiDARTouch, which generates dense depth maps from monocular images using LiDAR \"touches,\" without requiring dense ground-truth depth. In our setup, the minimal LiDAR input contributes on three distinct levels: as an additional input for the model, in a self-supervised LiDAR reconstruction objective function, and to estimate changes in pose, a vital component of self-supervised depth estimation architectures. The LiDARTouch framework sets a new benchmark in self-supervised depth estimation on the KITTI dataset, validating our decision to incorporate the very sparse LiDAR signal with other visual features. Furthermore, we demonstrate that the use of a few-beam LiDAR helps resolve scale ambiguity and infinite-depth problems that camera-only methods encounter. Additionally, we show that methods from the fully-supervised depth-completion literature can be adapted to a self-supervised regime with a minimal LiDAR signal."}
{"text": "In this study, we present a reinforcement learning approach to address the challenge of a soccer agent navigating a region while maintaining ball possession, despite an adversary's attempts to seize the ball. The adversary employs a fixed policy, while the dribbler learns optimal actions at each decision point. By establishing significant variables to represent the state space and implementing high-level macro-actions to incorporate domain knowledge, we apply the reinforcement learning algorithm with CMAC for function approximation. Our experimental results indicate that, following the training phase, the dribbler successfully completes its task against a formidable adversary approximately 58% of the time."}
{"text": "The non-contact method of remote photoplethysmography (rPPG) holds significant promise in various fields, particularly remote healthcare. However, current end-to-end rPPG and heart rate (HR) measurement techniques derived from facial videos are susceptible to less-structured scenarios, such as those involving head movement and poor lighting. In this communication, we delve into the reasons behind the subpar performance of existing end-to-end networks in challenging conditions and develop a robust end-to-end baseline (AutoHR) for remote HR measurement, utilizing neural architecture search (NAS). Our method is divided into three components: 1) a potent searched backbone incorporating Temporal Difference Convolution (TDC), designed to identify inherent rPPG-related cues between frames; 2) a hybrid loss function that takes into account constraints from both temporal and frequency domains; and 3) spatio-temporal data augmentation strategies to enhance learning of better representations. Extensive experiments on three standard datasets demonstrate our superiority in both intra- and cross-dataset testing."}
{"text": "The field of reinforcement learning (RL), particularly deep RL (DRL), is currently experiencing a high level of activity, marked by numerous new contributions. However, several scientific and technical hurdles persist, such as the capacity to abstract actions and the challenge of exploring the environment, which can be mitigated by intrinsic motivation (IM). This article offers a review of the role of intrinsic motivation in DRL, classifying various types of intrinsic motivations and discussing their advantages and drawbacks in relation to the aforementioned challenges. Furthermore, we delve into significant ongoing research questions within the DRL domain, either currently being studied or overlooked. Our survey approach focuses on understanding how to accomplish tasks, and we propose that addressing current challenges could foster a more comprehensive developmental architecture capable of handling a wide range of tasks. We describe this developmental architecture using a series of building blocks, each consisting of a RL algorithm and an IM module for information compression."}
{"text": "Video Super-Resolution (VSR) endeavors to generate a photo-realistic high-resolution (HR) video frame from its low-resolution (LR) counterpart (reference frame) and multiple adjacent frames (supporting frames). The challenge arises due to the varying motion of cameras or objects, causing misalignment between the reference frame and each support frame. Consequently, achieving temporal alignment is a significant yet complex issue in VSR. Traditional VSR approaches typically employ optical flow estimation between the reference frame and each support frame for temporal alignment, thereby making the performance of these image-level wrapping-based models heavily reliant on the precision of optical flow prediction. Inaccurate optical flow can result in artifacts in the wrapped support frames, which subsequently propagate into the final HR video frame. To circumvent this limitation, this paper introduces a Temporal Deformable Alignment Network (TDAN) that adaptively aligns the reference frame and each support frame at the feature level without relying on optical flow computation. The TDAN employs features from both the reference frame and each support frame to dynamically predict sampling convolution kernel offsets. By employing these kernels, TDAN transforms support frames to match the reference frame. A reconstruction network, which takes the aligned frames and the reference frame, is utilized to produce the HR video frame. The experimental results validate the efficacy of the proposed TDAN-based VSR model."}
{"text": "Deep learning methods for graph representation learning primarily employ a node-aggregation process. This work examines significant characteristics of these models and proposes a method to address certain issues. Notably, the scope of nodes influencing a node's representation is influenced by the graph structure, similar to the progression of a random walk. To accommodate varying local neighborhood properties and tasks, we investigate an architecture, jumping knowledge (JK) networks, which adaptively utilizes diverse neighborhood ranges for each node, thereby enhancing structure-aware representation. In a series of experiments on social, bioinformatics, and citation networks, our model demonstrates superior performance compared to existing standards. Additionally, integrating the JK framework with models such as Graph Convolutional Networks, GraphSAGE, and Graph Attention Networks consistently enhances their performance."}
{"text": "This work serves as a concise account of our entry in the Recognizing Families In the Wild Data Challenge (4th Edition), in association with FG 2020 Forum. The field of automatic kinship recognition has garnered significant research interest due to its potential applications, yet remains a complex task due to the scarcity of data points useful for distinguishing blood relatives from non-relatives. In this study, we delved into existing methodologies and developed our own approach. We experimented with various techniques, such as deep metric learning, to generate deep embedding features for each image, and subsequently employed Euclidean distance or class-based methods to ascertain blood relations. Our findings revealed several strategies, including increasing the number of negative samples and employing high-resolution images, that contributed to improved performance. Furthermore, we introduced a symmetric network with a binary classification method, which yielded our highest scores across all tasks."}
{"text": "We present a novel approach to video comprehension by framing the video recognition challenge as an image recognition task. We illustrate that a standalone image classifier is sufficient for video understanding, without the need for temporal modeling. Our method is straightforward and applicable across various domains. It involves assembling input frames into a composite image for training an image classifier to perform action recognition, analogous to classifying a single image. We substantiate the feasibility of this concept by exhibiting robust and promising results on four publicly accessible datasets, including Kinetics400, Something-to-something (V2), MiT, and Jester, utilizing a contemporary vision transformer. Additionally, we conduct experiments with prevalent ResNet image classifiers in computer vision to reinforce our idea. The performance on Kinetics400 is comparable to some of the top-performing CNN approaches based on spatio-temporal modeling. Our code and models will be accessible at https://github.com/IBM/sifar-pytorch."}
{"text": "The experimental findings indicate that the effectiveness of distributed training using stochastic gradient (SGD) is significantly influenced by the batch size and, in asynchronous implementations, by the gradient staleness. Notably, it has been noted that the speedup plateaus beyond a specific batch size and/or when delays become excessively large. We have pinpointed a data-dependent parameter that elucidates the speedup plateau in both these scenarios. Our extensive theoretical examination, encompassing strongly convex, convex, and non-convex settings, consolidates and broadens previous research directions that typically focused on only one of these two factors. Specifically, our methodology enables us to derive enhanced speedup results under commonly considered sparsity assumptions. Our theoretical findings provide practical recommendations on how learning rates can be optimally adjusted. We demonstrate that our results are optimal and substantiate key findings through numerical experiments."}
{"text": "Over the past decade, the proliferation of camera networks has sparked a growing research focus on analytics for multi-camera networks. This encompasses tasks like object detection, attribute identification, and vehicle/person tracking across multiple cameras without overlap. Existing management systems are primarily designed for multi-camera networks in controlled dataset environments with minimal camera variability and well-defined surveillance environment characteristics. Moreover, these systems are optimized for offline analytics, relying on human operators for guidance in forensic applications. This paper introduces a teamed classifier framework for video analytics in diverse, many-camera networks under challenging conditions such as multi-scale, multi-resolution cameras capturing the environment with varying occlusion, blur, and orientations. We detail an implementation for vehicle tracking and vehicle re-identification (re-id), incorporating a zero-shot learning (ZSL) system that provides continuous vehicle tracking. Our experiments on VeRi-776 and Cars196 demonstrate that the teamed classifier framework is resilient to adverse conditions, adaptable to changing video characteristics like new vehicle types/brands and new cameras, and delivers real-time performance compared to existing offline video analytics methods."}
{"text": "Siamese network-based trackers typically execute tracking without model updates and lack the ability to adaptively learn target-specific variation. Furthermore, these trackers determine the new state of tracked objects by producing axis-aligned bounding boxes, which incorporate unnecessary background noise and struggle to accurately estimate the rotation and scale transformation of moving objects, potentially impacting tracking performance. In this study, we introduce a novel Rotation-Scale Invariant Network (RSINet) to tackle this issue. Our RSINet tracker encompasses a target-distractor discrimination branch and a rotation-scale estimation branch, allowing for the explicit learning of rotation and scale knowledge via a multi-task learning approach in an end-to-end fashion. Additionally, the tracking model is adaptively optimized and updated under spatio-temporal energy control, ensuring model stability, reliability, and high tracking efficiency. Extensive experiments on OTB-100, VOT2018, and LaSOT benchmarks indicate that our proposed RSINet tracker surpasses recent trackers in terms of performance, while maintaining a real-time speed of approximately 45 FPS."}
{"text": "The Slope Difference Distribution (SDD) is calculated for one-dimensional curves, demonstrating its robustness in determining the logical partitioning point and the clustering center of each partition. Notably, SDD has shown superior performance in image segmentation compared to existing methods. To validate this, we have made available on Matlab Central the Matlab codes for comparing SDD with other image segmentation methods. The contour of an object bears a resemblance to the image's histogram, making feature detection via SDD from the object's contour feasible. In this study, SDD features are defined and serve as a sparse representation of the object contour. A reference model for each object is constructed using these SDD features, and model matching is employed for online object recognition. The experimental results are promising, with SDD achieving 100% accuracy for two public datasets, namely the NUS dataset and the near-infrared dataset, in gesture recognition. Similarly, SDD achieved 100% accuracy for the Kimia 99 dataset in object recognition."}
{"text": "Developing machine learning models using EEG data acquired outside of a controlled laboratory environment necessitates techniques resilient to noisy data and channels that are intermittently unavailable. This need is particularly acute when dealing with sparse EEG montages (1-6 channels), commonly found in consumer-grade or mobile EEG devices. Traditional machine learning models and deep neural networks, when applied to EEG data, are not usually designed or tested for resilience to data corruption, especially in the context of randomly missing channels. Although some studies have suggested strategies for utilizing data with missing channels, these methods are impractical for use with sparse montages and in devices with limited computational resources, such as wearables and smartphones. To address this issue, we propose dynamic spatial filtering (DSF), a multi-head attention module that can be integrated into a neural network's initial layer to manage missing EEG channels by learning to prioritize good channels and disregard poor ones. We evaluated DSF on publicly available EEG data consisting of approximately 4,000 recordings with simulated channel corruption and a private dataset of around 100 at-home mobile EEG recordings with natural corruption. Our proposed method matches the performance of baseline models in the absence of noise, but surpasses them by up to 29.4% in accuracy when substantial channel corruption is present. Furthermore, DSF's outputs are interpretable, allowing for real-time monitoring of channel importance. This approach could facilitate EEG analysis in challenging scenarios where channel corruption impedes the interpretation of brain signals."}
{"text": "The primary objective of this study is to expedite image classification in imaging AI, a task that is often laborious due to the time-consuming nature of labeling images. Previously, we have shown that reinforcement learning (RL) can accurately classify 2D slices of MRI brain images. In this work, we aim to advance this by: firstly, automatically extracting class labels from clinical reports using the SBERT natural language processing approach; secondly, extending our 2D classification work to fully 3D image volumes.\n\nIn the first part of our approach, we trained SBERT with 90 radiology report impressions and used it to predict class labels for Part 2. In the second part, we employed multi-step image classification, combining Deep-Q learning using 3D convolutions and TD(0) Q learning. We trained on a set of 90 images and tested on a separate set of 61 images, utilizing the classes predicted from patient reports by the trained SBERT in Part 1. For comparison, we also trained and tested a supervised deep learning classification network on the same set of training and testing images using the same labels.\n\nThe results showed that the SBERT model achieved 100% accuracy for both normal and metastasis-containing scans upon training with the corpus of radiology reports in Part 1. In Part 2, while the supervised approach overfitted the training data and performed poorly on the testing set (66% accuracy), the reinforcement learning approach achieved an accuracy of 92%. These results were found to be statistically significant, with a p-value of 3.1 x 10^-5."}
{"text": "The present study introduces Impressions2Font (Imp2Font), a novel approach for creating font images that convey specific impressions. Imp2Font is an expansion of conditional generative adversarial networks (GANs), capable of generating font images based on an array of impression words provided as conditions. These impression words are transformed into a soft-constraint vector by an impression embedding module, which employs a word embedding technique. Evaluations, both qualitative and quantitative, demonstrate that Imp2Font produces font images of superior quality compared to existing methods, as it can handle multiple impression words or even unlearned words."}
{"text": "In the realm of scene comprehension, the objectives of monocular depth estimation and semantic segmentation are pivotal. Numerous studies have delved into the joint learning of these tasks through task interaction algorithms. However, a significant drawback of existing methods is their inability to fully exploit semantic labels. These methods disregard the provided context structures, using semantic labels solely for supervising segmentation split prediction. This limitation negatively impacts the performance of both tasks. In this research, we present a network, Contextual Information Network (CI-Net), to address this issue. Our approach involves incorporating a self-attention block in the encoder to produce an attention map. By employing supervision from an ideal attention map derived from semantic labels, the network is infused with contextual information, enabling it to perceive the scene more accurately and utilize correlated features for precise prediction. Furthermore, a feature sharing module is designed to facilitate deep fusion of task-specific features, and a consistency loss is devised to guide the features mutually. The efficacy of CI-Net is assessed on the NYU-Depth-v2 and SUN-RGBD datasets. The experimental outcomes corroborate that our proposed CI-Net significantly enhances the precision of semantic segmentation and depth estimation."}
{"text": "In this study, we present a novel approach for discerning between natural images (NIs) and colorized images (CIs) using a convolutional neural network (CNN). As deep learning techniques advance and offer greater computational power, image colorization results become increasingly lifelike, making it harder for human eyes to distinguish fakes. Our method excels in classification accuracy and is capable of handling the complex scenario of blind detection, where no training samples are available from unidentified colorization algorithms during testing. This blind detection performance can be interpreted as a measure of the model's generalization ability. Initially, we develop and implement a base network that typically outperforms existing methods in terms of classification accuracy and generalization. Subsequently, we incorporate a new branch that focuses on analyzing smaller regions of extracted features, enhancing both the classification accuracy and the generalization of the network in most cases. To further boost the blind detection performance, we propose the automatic generation of negative samples through linear interpolation of paired natural and colorized images. These negative samples are then incrementally integrated into the original training dataset for continued network training. Our experimental results indicate that our method consistently delivers robust and high generalization performance when tested against various state-of-the-art colorization algorithms."}
{"text": "The World Health Organization (WHO) offers guidelines for managing Particulate Matter (PM) levels due to the potential risks to human health when PM levels are elevated. To effectively manage PM levels, it is essential to first establish a method for measuring PM values. In this study, we employ Tapered Element Oscillating Microbalance (TEOM)-based PM sensors, as they offer greater cost-effectiveness compared to Beta Attenuation Monitor (BAM)-based sensors, despite having a higher likelihood of malfunctioning. We refer to any overall malfunction as an anomaly, and our objective is to identify these anomalies for the maintenance of PM measuring sensors. We introduce a novel architecture, the Hypothesis Pruning Generative Adversarial Network (HP-GAN), to address this aim. We conduct experiments comparing various anomaly detection architectures to validate the superior performance of our proposed model."}
{"text": "We put forth strategies for training convolutional neural networks (CNNs) with binarized weights and activations, resulting in quantized models that are well-suited for mobile devices with restricted power and computational resources. Previous research on quantizing CNNs typically attempts to represent floating-point information using a collection of discrete values, a method we refer to as value approximation, and usually assumes the same architecture as full-precision networks. In contrast, we introduce a novel \"structure approximation\" perspective on quantization, suggesting that different architectures designed for low-bit networks may be more effective for achieving optimal performance. Specifically, we propose a \"network decomposition\" strategy, named Group-Net, where we partition the network into groups. Each full-precision group can be efficiently replicated by aggregating a set of homogeneous binary branches. Additionally, we develop effective connections among groups to enhance the representation capacity. Notably, the proposed Group-Net exhibits strong generalization to various tasks. For example, we adapt Group-Net for precise semantic segmentation by incorporating rich context into the binary structure. Furthermore, for the first time, we utilize binary neural networks for object detection. Experiments on classification, semantic segmentation, and object detection tasks show that our methods surpass numerous quantized networks in the literature in terms of accuracy and computational efficiency, outperforming the best binary neural networks previously reported."}
{"text": "Dense video captioning involves identifying and describing significant events in unedited videos, with most existing methods focusing solely on visual features, disregarding the audio track entirely. A limited number of previous studies have employed both modalities, but they have produced subpar results or emphasized their importance in datasets with specific domains. In this study, we present the Bi-modal Transformer, an architecture that adapts the Transformer for bi-modal input. Our model demonstrates effectiveness in the dense video captioning task when utilizing both audio and visual modalities. The bi-modal transformer's module is versatile, capable of processing any two modalities in sequence-to-sequence tasks. Furthermore, we show that the pre-trained bi-modal encoder within the bi-modal transformer can function as a feature extractor for a straightforward proposal generation module. The model's performance is validated on the challenging ActivityNet Captions dataset, where it achieves exceptional results. The code for our model is accessible at: v-iashin.github.io/bmt."}
{"text": "In the context of deep learning advancements, performance in computer vision tasks like object detection and segmentation has significantly improved. Nevertheless, in practical applications like autonomous driving, the consequences of inaccurate object predictions are substantial. Traditional deep learning models for object detection, such as YOLO models, tend to be overconfident in their predictions and disregard the uncertainties in predictions on out-of-distribution data. This work introduces an efficient and effective method to incorporate model uncertainty in object detection and segmentation tasks using Monte-Carlo DropBlock (MC-DropBlock) inference. During both training and testing, MC-DropBlock is applied to the convolutional layer of deep learning models, including YOLO. We reveal that this approach results in a Bayesian convolutional neural network capable of capturing the epistemic uncertainty in the model. Furthermore, we account for aleatoric uncertainty using a Gaussian likelihood. Our experiments using out-of-distribution scenarios demonstrate the effectiveness of the proposed approach in modeling uncertainty in object detection and segmentation tasks. The results indicate that MC-DropBlock enhances the generalization, calibration, and uncertainty modeling capabilities of YOLO models in object detection and segmentation."}
{"text": "In recent times, deep neural networks (DNNs) have demonstrated exceptional performance in numerous low-level vision tasks. However, the state-of-the-art results are often attained by extremely deep networks, comprising up to tens of layers and tens of millions of parameters. To render DNNs compatible with resource-limited platforms, it is essential to mitigate the tradeoff between performance and efficiency. In this study, we introduce a novel activation unit, specifically designed for image restoration tasks. Unlike the commonly used per-pixel activation units, such as ReLUs and sigmoids, our unit incorporates a learnable nonlinear function with spatial connections. This allows the network to capture intricate features, thereby necessitating a substantially smaller number of layers to achieve the same performance level. We validate the efficacy of our units through experiments with leading-edge nets for denoising, de-raining, and super resolution, which are already recognized as compact. With our method, we manage to reduce these models by approximately 50% without any performance degradation."}
{"text": "In self-supervised learning, a model is trained to solve a pretext task on an unannotated dataset, with the ultimate goal of applying this model to a specific target domain and task. The primary strategy for transferring the model is fine-tuning, which limits the use of the same model or its parts for both pretext and target tasks. This paper introduces a new self-supervised learning framework that addresses challenges in designing and comparing various tasks, models, and data domains. Notably, this framework separates the structure of the self-supervised model from the fine-tuned model for the specific task. This separation enables us to: 1) quantitatively evaluate previously incompatible models, including handcrafted features; 2) demonstrate that deeper neural network models can generate superior representations from the same pretext task; 3) transfer knowledge from a deep model to a shallower one, thereby enhancing its learning capabilities. Utilizing this framework, we develop a novel self-supervised task that outperforms existing benchmarks on PASCAL VOC 2007, ILSVRC12, and Places by a substantial margin. Our learned features reduce the mAP gap between self-supervised and supervised learning models in object detection on PASCAL VOC 2007 from 5.9% to 2.6%."}
{"text": "Deep network architectures have historically been developed and refined through a laborious process of human expertise and trial-and-error. This process is resource-intensive, demanding significant time, expertise, and resources. To alleviate this issue, we introduce a novel algorithm to efficiently determine the hyperparameters of a deep network architecture autonomously. Our primary focus is on designing neural architectures for medical image segmentation tasks. Our proposed method employs policy gradient reinforcement learning, with the reward function assigned a segmentation evaluation metric (dice index). We demonstrate the effectiveness of our method, characterized by its low computational cost, in comparison to leading medical image segmentation networks. Additionally, we present a new architecture design, a densely connected encoder-decoder CNN, as a robust baseline for applying our hyperparameter search algorithm. We apply the proposed algorithm to each layer of the baseline architectures. As a case study, we train our system on cine cardiac MR images from the Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. Beginning with a baseline segmentation architecture, the resulting network architecture achieves state-of-the-art results in accuracy, without the need for trial-and-error-based architecture design methods or close supervision of hyperparameter adjustments."}
{"text": "In the realm of graph neural networks (GNNs), their potential utility has been explored across various disciplines that rely on graph data. However, a lack of uniform training conditions hinders equitable comparisons among novel methods, encompassing diverse model architectures and data augmentation strategies. To address this, we propose a standard, replicable benchmark for node classification tasks, featuring 9 datasets (comprising small- and medium-scale datasets from diverse domains) and 7 distinct models. We implement a k-fold evaluation methodology for smaller datasets and a consistent set of model training protocols for all datasets, fostering a standardized experimental framework for GNNs to promote fair architecture comparisons. We employ node2vec and Laplacian eigenvectors for data augmentation to examine the impact of input features on model performance. Our findings underscore the significance of topological information in node classification tasks. Enhancing the number of model layers does not generally improve performance, except on the PATTERN and CLUSTER datasets, where the graphs are disconnected. Data augmentation proves beneficial, particularly when using node2vec in the baseline, leading to a significant enhancement in baseline performance."}
{"text": "Advancements in smart home technology, autonomous vehicles, healthcare systems, and robotic assistance, coupled with strengthened legal regulations, have significantly shaped the direction of academic research in the field of explainable machine learning. Numerous researchers have devised strategies to elucidate the workings of any opaque model for classification tasks. However, the universal nature of the neighbourhood generation process in agnostic explanators does not ensure the true proximity between the generated neighbours and the original instance. This study proposes a methodology for generating explanations for a neural network's decisions within a local context, by taking into account the neural network's architecture in the creation of an instance's neighbourhood, thereby ensuring the proximity among the generated neighbours and the instance."}
{"text": "Modern robotics systems frequently utilize LiDAR as their primary sensing method due to its geometric abundance. Rotating base LiDARs, specifically rolling shutter types, are prevalent, where an array of lasers scans the environment. Data is emitted as a continuous sequence of packets, each corresponding to a sector of the 360-degree coverage. Current perception algorithms delay processing until the entire sweep is constructed, leading to an extra delay. For instance, a typical 10Hz LiDAR would have a delay of 100ms. Consequently, the output no longer accurately portrays the current state of the environment. This presents a problem, as robotics applications necessitate swift response times, allowing for prompt maneuver planning in emergency situations. In this paper, we introduce StrObe, a novel strategy that minimizes latency by processing LiDAR packets and generating a stream of detections without waiting for the full sweep to be constructed. StrObe recycles computations from previous packets and iteratively updates a latent spatial representation of the scene, serving as a memory, as new data is received, resulting in precise, low-latency perception. We validate the efficacy of our approach on a substantial real-world dataset, demonstrating that StrObe significantly outperforms existing methods when latency is considered, and achieves parity with traditional performance."}
{"text": "In our proposed framework, top-down saliency models generate a probability map that concentrates at locations determined by a task, such as object detection. Unlike traditional methods that rely on fully supervised training with pixel-level object annotations, our approach utilizes only binary labels to denote the presence or absence of an object in an image. Initially, the contribution of each image region to the confidence of a CNN-based image classifier is determined through a backtracking strategy, resulting in top-down saliency. Subsequently, from a collection of saliency maps generated by rapid bottom-up saliency techniques for the same image, we select the most suitable map for the top-down task. This chosen bottom-up saliency map is then merged with the top-down saliency map. Highly salient features are employed to train a linear SVM classifier to assess feature saliency. This integrated saliency is further enhanced through a multi-scale superpixel-averaging of the saliency map. Our weakly supervised top-down saliency model demonstrates comparable performance to fully supervised methods. Experiments were conducted on seven challenging datasets, and the results were compared quantitatively with 40 related approaches across four distinct applications."}
{"text": "The investigation of 2D object detection in pristine images has been extensively explored, yet concerns persist regarding its susceptibility to adversarial attacks. Previous research has attempted to enhance the resilience of object detectors through adversarial training, albeit at the cost of a substantial decline in average precision (AP) on unaltered images. In this study, we put forth the idea that aligning features from intermediate layers can enhance clean AP and robustness in object detection. Building upon adversarial training, we introduce two feature alignment modules: the Knowledge-Distilled Feature Alignment (KDFA) module and the Self-Supervised Feature Alignment (SSFA) module, which aid the network in producing more efficient features. To validate the efficacy of our proposed method, we perform comprehensive experiments on the PASCAL VOC and MS-COCO datasets. The code for our experiments can be found at https://github.com/grispeut/Feature-Alignment.git."}
{"text": "The word embedding technique known as word2vec, introduced by Mikolov (2013), is extensively employed in natural language processing, yet its theoretical foundation remains inadequate. This study aims to bridge this gap by providing a comprehensive analysis of word2vec's highly nonlinear functional. Our findings indicate that word2vec might predominantly rely on an underlying spectral method. This revelation could pave the way for deriving provable guarantees for word2vec. We substantiate our conclusions through numerical simulations. An intriguing question that arises is whether the nonlinear aspects of word2vec not accounted for by the spectral method are advantageous and, if so, through what mechanism."}
{"text": "The advancements in reinforcement learning, a prominent machine learning approach, have undeniably made substantial strides. Compared to reinforcement learning methods that rely on a known system model, those based on an unknown model typically display a significantly wider range of universality and applicability. In this study, a novel reinforcement learning architecture, the \"neural network iterative linear quadratic regulator (NNiLQR)\", is proposed and presented. This architecture operates without any prior knowledge of the system model, and it establishes a new non-parametric method for deriving the optimal policy solely from measurement data through iterative refinements of the neural network system. Notably, the NNiLQR method significantly surpasses the traditional iLQR method in terms of the objective function due to its innovative approach to further exploration. The superiority of the NNiLQR method is evident in the results of two demonstrative examples."}
{"text": "The present study expands the scope of composed image retrieval, a task that involves an input query comprising an image and a brief textual description of desired image modifications. Previous methods have been confined to processing simplistic images within specific domains, such as fashion products, thereby restricting the exploration of complex visual reasoning in diverse image and language contexts. To overcome this limitation, we introduce the Compose Image Retrieval on Real-life images (CIRR) dataset, containing over 36,000 pairs of open-domain images and human-generated modifying text. To extend the applicability of existing methods to open-domain images, we propose CIRPLANT, a transformer-based model that utilizes extensive pre-trained vision-and-language (V&L) knowledge to modify visual features based on natural language. Retrieval is performed by nearest neighbor lookup on the modified features. We show that with a relatively straightforward architecture, CIRPLANT surpasses existing methods on open-domain images, while achieving state-of-the-art accuracy on narrow datasets like fashion. In conjunction with the release of CIRR, we anticipate that this work will stimulate future research in the field of composed image retrieval."}
{"text": "Malicious websites, often referred to as malicious URLs, pose a significant and prevalent danger to cybersecurity. These URLs distribute unwanted content such as spam, phishing attempts, drive-by exploits, and more, enticing unwitting users to fall victim to scams resulting in monetary loss, data theft, and malware installation. Annually, these threats lead to financial losses in the billions. The urgency to identify and respond to such threats cannot be overstated. Historically, this identification has relied heavily on blacklists. However, blacklists are not exhaustive and are unable to detect newly created malicious URLs. To enhance the universality of malicious URL detectors, machine learning approaches have garnered growing interest in recent years. This article aims to offer a comprehensive review and a structural understanding of Malicious URL Detection techniques using machine learning. We define the Malicious URL Detection problem as a machine learning task and examine the literature that addresses various aspects of this issue (feature representation, algorithm design, etc.). This article serves as a timely and extensive resource for a diverse audience, including machine learning researchers and engineers in academia, as well as cybersecurity professionals and practitioners, to aid in their comprehension of the current state of the art and to support their own research and practical applications. We also discuss design challenges, open research questions, and suggest key directions for future research in this field."}
{"text": "In this study, we introduce a comprehensive end-to-end feature fusion attention network (FFA-Net) designed to directly generate haze-free images. The FFA-Net structure encompasses three primary elements: 1) A novel Feature Attention (FA) module, which merges Channel Attention with Pixel Attention, recognizing that distinct channel-wise features carry varying weighted information, and haze distribution is inconsistent across different image pixels. The FA module unequally treats features and pixels, thereby enhancing the flexibility of CNNs in handling diverse types of information. 2) A basic block structure, comprising Local Residual Learning and Feature Attention, enables the omission of less significant information such as thin haze regions or low-frequency details through multiple local residual connections, thereby enabling the main network architecture to concentrate on more pertinent information. 3) An Attention-based Feature Fusion (FFA) structure, where feature weights are dynamically learned from the Feature Attention (FA) module, prioritizing essential features. This structure also preserves shallow layer information and transfers it to deeper layers.\n\nThe experimental findings indicate that our proposed FFA-Net significantly outperforms existing single image dehazing methods, achieving a substantial improvement both quantitatively and qualitatively. The best published PSNR metric has been elevated from 30.23db to 36.39db on the SOTS indoor test dataset.\n\nThe code for this research is accessible on GitHub."}
{"text": "This work outlines the development of a robotic picking system by our team, Team Applied Robotics, for the Amazon Picking Challenge 2016. The challenge required teams to create a robotic system capable of picking a diverse range of products from shelves or totes. This paper delves into the design decisions and our strategic approach, including the implementation of a high-resolution 3D vision system, the integration of texture and shape-based object detection algorithms, the robot's path planning, and the design of object manipulators."}
{"text": "This study explores the challenge of deep face recognition under an open-set protocol, where the optimal face features exhibit a smaller maximum intra-class distance compared to the minimum inter-class distance within a selected metric space. The burgeoning field of hyperspherical face recognition, recognized for its potential, has garnered significant interest and is now a primary focus in face recognition research. One of the pioneering works in hyperspherical face recognition, SphereFace, proposed the concept of learning face embeddings with a large inter-class angular margin. However, SphereFace encounters significant training instability, which hinders its practical application. To alleviate this issue, we present a unified framework to comprehend large angular margins in hyperspherical face recognition. Within this framework, we expand upon SphereFace and develop an enhanced variant, SphereFace-R, with significantly improved training stability. We introduce two innovative methods to implement the multiplicative margin and examine SphereFace-R under three distinct feature normalization schemes (no normalization, hard normalization, and soft normalization). Additionally, we propose a training stabilization strategy, \"characteristic gradient detachment,\" and demonstrate the effectiveness of SphereFace-R through extensive experiments, which consistently outperform or match the state-of-the-art methods."}
{"text": "The recognition of human actions from skeleton data, facilitated by Graph Convolutional Networks (GCNs), has garnered significant interest due to its ability to effectively model non-Euclidean structure data. However, numerous existing GCN methods pre-define a graph and maintain it throughout the network, potentially losing implicit joint correlations. Furthermore, the prevalent spectral GCN is typically approximated by a first-order hop, thereby neglecting higher-order connections. To overcome these issues, we opt for Neural Architecture Search (NAS) and propose the first GCN designed automatically for skeleton-based action recognition. Our approach expands the search space by incorporating multiple dynamic graph modules, following a comprehensive exploration of spatial-temporal correlations between nodes. Additionally, we incorporate multiple-hop modules to surmount the representational capacity limitation resulting from first-order approximation. Furthermore, an efficient evolution strategy that considers both sampling and memory is proposed to discover an optimal architecture for this task. The resulting architecture demonstrates the efficacy of higher-order approximation and the dynamic graph modeling mechanism with temporal interactions, a topic scarcely addressed before. To assess the performance of the discovered model, we perform extensive experiments on two large-scale datasets, and the results indicate that our model achieves state-of-the-art results."}
{"text": "Scene text recognizers utilizing attention-based mechanisms have achieved significant success, as they employ a more condensed intermediate representation learned through 1D or 2D attention via a Recurrent Neural Network (RNN)-based encoder-decoder architecture. However, these methods encounter the attention-drift issue due to the high similarity among encoded features causing confusion under the RNN-based local attention mechanism. Furthermore, RNN-based methods exhibit low efficiency due to poor parallelization. To address these issues, we introduce MASTER, a self-attention-based scene text recognizer that (1) encodes input-output attention and learns self-attention to encode feature-feature and target-target relationships within the encoder and decoder, (2) develops a more potent and resilient intermediate representation to spatial distortion, and (3) boasts high training efficiency due to superior parallelization and rapid inference speed thanks to an efficient memory-cache mechanism. Extensive testing on various benchmarks confirms the superiority of our MASTER in handling both regular and irregular scene text. The Pytorch code can be found at https://github.com/wenwenyu/MASTER-pytorch, and the Tensorflow code can be found at https://github.com/jiangxiluning/MASTER-TF."}
{"text": "Video super-resolution techniques have demonstrated significant achievements in enhancing images from low-resolution inputs. The objective of video super-resolution is to leverage information from multiple images, often related through optical flow and consecutive image warping. In this research, we present an end-to-end video super-resolution network that integrates optical flow estimation into its architecture, unlike previous methods. Our analysis reveals that conventional image warping methods do not significantly enhance video super-resolution due to optical flow. Instead, we propose a motion compensation operation that directly warps from low to high resolution. Our findings indicate that this network configuration allows video super-resolution to benefit from optical flow, resulting in state-of-the-art performance on standard test sets. Furthermore, we demonstrate that processing entire images rather than individual patches contributes significantly to improved accuracy."}
{"text": "Histopathological image analysis is a laborious and time-consuming process, involving experienced pathologists meticulously examining extensive whole-slide images, from cells to tissues. The burgeoning field of transfer learning techniques has been extensively explored for image comprehension tasks with minimal annotations. However, when applied to histology image analytics, many of these techniques struggle to mitigate the performance decline due to the domain disparity between the source training dataset and the target dataset, such as different tissues, staining variations, and imaging devices. To address this issue, we propose a novel unsupervised domain adaptation method for histopathological image analysis. This method utilizes a backbone for embedding input images into a feature space and a graph neural layer for propagating supervision signals of labeled images. The graph model is constructed by linking every image with its closest neighbors in the embedded feature space. Subsequently, a graph neural network is employed to generate new feature representations for each image. During the training phase, target samples with high confidence inferences are dynamically assigned pseudo labels. The cross-entropy loss function is employed to constrain the predictions of source samples with manually marked labels and target samples with pseudo labels. Additionally, maximum mean diversity is used to promote the extraction of domain-invariant feature representations, and contrastive learning is leveraged to improve the category discrimination of the learned features. In experiments of unsupervised domain adaptation for histopathological image classification, our method demonstrates state-of-the-art performance on four public datasets."}
{"text": "Geometric learning in representation has demonstrated significant potential across various machine learning scenarios, encompassing areas such as relational learning, natural language processing, and generative models. In this study, we focus on the challenge of executing manifold-valued regression within a hyperbolic space, a crucial step for numerous pertinent machine learning applications. Specifically, we reframe the task of predicting tree nodes as a manifold regression problem in a hyperbolic space, offering a novel perspective on two complex tasks: a) hierarchical classification via label embeddings and b) the extension of hyperbolic representation taxonomies. To tackle the regression problem, we examine existing methods and propose two innovative approaches that offer computational benefits: a deep learning model with parameters informed by the geodesics of the target space and a non-parametric kernel-method for which we also establish excess risk bounds. Our experimental results suggest that utilizing hyperbolic geometry is a promising strategy. Notably, in the taxonomy expansion setting, hyperbolic-based estimators significantly surpass methods that perform regression in the Euclidean space."}
{"text": "A crucial foundation for secure machine learning (ML) is the proactive elimination of flaws in neural networks to enable their use in critical applications. A common category of safety hazards are learned shortcuts, or spurious correlations that a network utilizes for its decisions, which have no meaningful connection to the actual task. Networks that rely on such shortcuts risk poor generalization to unseen inputs. Explanatory methods aid in exposing such network weaknesses. However, many of these techniques are inapplicable when direct network access is restricted, in what are known as black-box scenarios. These scenarios are common when using third-party ML components. To overcome this limitation, we propose a method to identify learned shortcuts using a network designed for interpretability as a stand-in for the black-box model of interest. By leveraging the proxy's guarantees on introspection, we automatically identify potential learned shortcuts. Their transferability to the black box is systematically verified. Specifically, as a proxy model, we select a BagNet, which makes decisions based solely on local image patches. We show on the autonomous driving dataset A2D2 that patch shortcuts extracted from the proxy significantly impact the black box model. By efficiently identifying such patch-based vulnerabilities, we contribute to the development of safer ML models."}
{"text": "Research in human-computer interaction has emphasized object detection, with skin area detection playing a pivotal role in various recognitions such as face recognition, human motion detection, and predicting pornographic and nude images. The majority of research in skin detection has been conducted on images of individuals from African, Mongolian, and Anglo-Saxon ethnic origins. However, there is a lack of focus on the skin color of individuals from the Indian sub-continent. This study aims to compare three image segmentation approaches using images of individuals from the Indian sub-continent, with the goal of optimizing detection criteria and identifying efficient parameters for skin area detection. The results indicate that the HSV color model approach is more effective for skin detection in individuals from the Indian sub-continent, achieving a significant success rate of 91.1% true positives and 88.1% true negatives."}
{"text": "We present DeepV2D, a deep learning architecture designed for predicting depth from video. This architecture leverages the representation capabilities of neural networks and the geometric principles underlying image formation. By converting classical geometric algorithms into trainable modules and integrating them into a single, end-to-end differentiable architecture, DeepV2D is created. The model consists of two interleaved stages: motion estimation and depth estimation. During the inference process, these stages are iteratively alternated, leading to accurate depth predictions. The code for DeepV2D can be found at https://github.com/princeton-vl/DeepV2D."}
{"text": "Generating highly authentic counterfeit images using Deepfake or Generative Adversarial Networks (GANs) poses a significant social disruption. While various techniques have been suggested to identify such false images, they are susceptible to adversarial manipulations - deliberately crafted distortions that can lead to incorrect classifications. Traditional methods of assaulting fake image detectors typically introduce adversarial manipulations across the entire image, which is redundant and enhances the visibility of these manipulations. In this study, we introduce a novel approach to disrupt fake image detection by identifying crucial pixels for the detector and attacking only these key pixels. This approach significantly reduces the L_0 and L_2 norms of adversarial manipulations compared to existing methods. Our experiments on two public datasets using three fake image detectors demonstrate that our proposed method outperforms current state-of-the-art in both white-box and black-box attacks."}
{"text": "In the realm of autonomous driving, there is a stringent requirement for 3D perception using sensors that adhere to the automotive industry's standards. The surge in popularity of MEMS LiDAR is significant due to its cost-effectiveness, robustness, and compatibility with mass-production. However, its limited field of view (FoV) hinders its widespread adoption. This paper introduces LEAD, or LiDAR Extender for Autonomous Driving, a system designed to expand the capabilities of MEMS LiDAR by incorporating images in terms of both FoV and range. We propose a multi-stage propagation strategy, grounded in depth distributions and uncertainty maps, demonstrating impressive propagation capabilities. Our depth outpainting/propagation network employs a teacher-student training approach, transferring depth estimation skills to a depth completion network without any scale errors. To assess the quality of LiDAR extension, we leverage a high-precision laser scanner to create a ground-truth dataset. Both quantitative and qualitative evaluations indicate that our method surpasses state-of-the-arts by a substantial margin. We anticipate that the LEAD system and the accompanying dataset will contribute significantly to the depth research community."}
{"text": "The current focus on neural network compression stems from the computational demands of contemporary deep learning models. In this study, our primary goal is to transfer knowledge from a precise and complex model to a more compact one. Our contributions encompass three aspects: (i) we put forth an adversarial network compression method to train the compact student network to emulate the complex teacher, without the necessity of labels during training; (ii) we present a regularization method to inhibit a trivialy-powerful discriminator without compromising the network's capacity; (iii) our approach is versatile and applicable across various teacher-student model configurations. In a comprehensive analysis on five standard datasets, we demonstrate that our student network experiences minimal accuracy loss, outperforms other knowledge transfer methods, and surpasses the performance of the same network trained with labels. Furthermore, we showcase leading results in comparison to other compression techniques."}
{"text": "The focus of our research is on the task of multi-turn conversational fashion image retrieval, utilizing multiple rounds of natural language feedback. Previous research predominantly operates in single-turn settings, and existing models for multi-turn conversational fashion image retrieval have limitations, including the use of traditional models and suboptimal performance. We introduce a novel framework designed to efficiently manage multi-turn conversational fashion image retrieval. This framework utilizes the conversation history, encoded reference image, and feedback text information to search for relevant images. Additionally, it employs a mutual attention strategy to leverage fashion attribute information. Due to the lack of a suitable existing dataset for our multi-turn task, we generate a large-scale multiturn fashion dataset through additional manual annotation on an existing single-turn dataset. Our experimental results demonstrate that our proposed model significantly surpasses existing state-of-the-art methods."}
{"text": "Real-time human pose tracking is a significant challenge that involves recognizing distinct human pose instances and linking them across various frames of a video over time. Current pose tracking techniques struggle to accurately model temporal dependencies and typically demand substantial computational resources, often performing computations offline. This work introduces an efficient Multi-person Pose Tracking method, KeyTrack, which solely utilizes keypoint data without relying on RGB or optical flow information. The keypoints are tracked using our Pose Entailment method, where a pair of pose estimates is drawn from different video frames, tokenized, and then a Transformer-based network classifies whether one pose follows another temporally. Additionally, we enhance our top-down pose estimation method with a novel, parameter-free, keypoint refinement technique that enhances the keypoint estimates used during the Pose Entailment step. KeyTrack delivers state-of-the-art results on the PoseTrack'17 and PoseTrack'18 benchmarks while consuming only a small fraction of the computational resources required by most other methods for generating tracking information."}
{"text": "The study of early classification in time series has garnered significant attention due to its potential for reducing class prediction latency in time-critical domains like healthcare and finance. The primary objective of early classification methods is to classify incomplete time series as promptly as possible while maintaining a desired level of accuracy. Over the past few years, numerous early classification techniques for time series have emerged. Given the diverse solutions proposed for this problem, a comprehensive review of existing approaches is crucial to understand the current state of the field. These methods have shown promising results across various applications, such as human activity recognition, health diagnostics based on gene expression, industrial monitoring, and more. In this paper, we offer a systematic review of the current literature on early classification techniques for both univariate and multivariate time series. We categorize the existing approaches into four distinct groups based on their solution strategies: prefix-based, shapelet-based, model-based, and miscellaneous approaches. Furthermore, we discuss the applications of early classification in various sectors, including industrial monitoring, intelligent transportation, and medicine. Lastly, we provide a concise summary of the current literature, along with suggestions for future research directions."}
{"text": "The arrangement of text on an image significantly contributes to the creation of visually appealing designs. Automating this process involves identifying suitable position, angle, and style for textual components, which necessitates comprehending the content of the underlying image. We term this process as \"copyspace detection,\" distinguishing it from foreground-background separation. Our research has led to the development of solutions employing single and multi-stage object detection techniques, trained on data meticulously labeled by experts. This workshop aims to explore these copyspace detection algorithms and showcase their implementation in generative design tools and workflows like Einstein Designer."}
{"text": "In the era of widespread social media usage, the fashion industry, influenced by celebrities, designers, and influencers, has experienced a shortened lifecycle in design and production. Amidst the influx of fashion-related content and an abundance of user-generated fashion images, it poses a challenging task for designers to discern trending fashion by sifting through social media photos. To address this, deep analysis of fashion photos on social media is required to identify and categorize various fashion items within a single image. Despite the availability of extensive labeled datasets for common objects in competitions like MSCOCO, finding large labeled datasets for fast fashion items is challenging. Moreover, current state-of-the-art object detectors lack the ability to absorb vast amounts of unlabeled data from social media to fine-tune object detectors using labeled datasets. This study demonstrates the application of a versatile object detector, capable of unsupervised pre-training, on 24 categories from the recently released Open Images V4 dataset. Initially, the base architecture of the object detector is trained using unsupervised learning on 60K unlabeled photos from 24 categories sourced from social media. Subsequently, it is fine-tuned on 8.2K labeled photos from Open Images V4 dataset. On 300 x 300 image inputs, we achieve a mAP of 72.7% on a test dataset of 2.4K images, outperforming state-of-the-art object detectors by 11% to 17%. Our improvement is attributed to the chosen architecture's ability to support unsupervised learning and its superior performance in identifying small objects."}
{"text": "The central aspect of face anti-spoofing, as demonstrated by previous research, is the intricate image pattern, known as \"spoof traces\", such as color aberrations, 3D mask edges, Moire patterns, and more. Developing a versatile face anti-spoofing model to assess these spoof traces can enhance not only the model's spoof detection generalization but also its interpretability. However, this task is complex due to the variety of spoof types and the scarcity of authentic spoof trace data. In this study, we propose a novel adversarial learning framework to separate spoof faces into spoof traces and live counterparts. Leveraging physical properties, spoof generation is modeled as a combination of an additive process and an inpainting process. The additive process signifies spoofing as the introduction of additional patterns (e.g., moire pattern), with the live counterpart recoverable by eliminating these patterns. The inpainting process represents spoofing as complete coverage of certain regions by the spoof material, necessitating an estimation of the live counterpart for those regions. We employ 3 additive components and 1 inpainting component to represent traces across various frequency bands. The disentangled spoof traces can be utilized to generate realistic new spoof faces after appropriate geometric correction, and these synthesized spoof faces can be used for training to improve the generalization of spoof detection. Our method exhibits superior spoof detection performance across 3 testing scenarios: known attacks, unknown attacks, and open-set attacks. Simultaneously, it offers a visually-persuasive estimation of the spoof traces. Upon publication, the source code and pre-trained models will be made publicly accessible."}
{"text": "This study introduces a novel access control approach for semantic segmentation models, which employs a spatially invariant permutation of feature maps combined with a secret key. The training and testing of these models involve the permutation of selected feature maps using the secret key. This method enables authorized users with the correct key to access the model fully and maintain its original performance, while unauthorized users without the key experience a performance degradation. Traditional access control methods primarily focus on image classification tasks and have not been implemented in semantic segmentation tasks. In an experiment, it was shown that the protected models allow authorized users to achieve performance similar to that of unprotected models, while also providing robustness against unauthorized access. Furthermore, a conventional method utilizing block-wise transformations was found to perform poorly under semantic segmentation models."}
{"text": "In a conventional convolutional layer, the filters are permanently set post-training. In contrast, we propose a novel architecture, the Dynamic Filter Network, where filters are generated dynamically based on the input. We demonstrate that this architecture offers increased versatility due to its adaptive nature, without a significant increase in the number of model parameters. A broad range of filtering operations can be learned through this method, including local spatial transformations, as well as others such as selective (de)blurring or adaptive feature extraction. Furthermore, multiple such layers can be integrated, for instance, in a recurrent architecture. We validate the efficacy of the dynamic filter network on tasks like video and stereo prediction, achieving state-of-the-art performance on the moving MNIST dataset with a smaller model. Visualizing the learned filters reveals that the network has acquired flow information solely from unlabeled training data, implying its potential for pretraining networks for various supervised tasks in an unsupervised manner, such as optical flow and depth estimation."}
{"text": "We offer an enhancement to the model-free anomaly detection algorithm, Isolation Forest, named Extended Isolation Forest (EIF). EIF addresses the challenge of assigning anomaly scores to data points, a problem we illustrate using heat maps for anomaly scores. These heat maps exhibit artifacts due to the criteria for the branching operation of the binary tree. We delve into this issue in depth and visually demonstrate its occurrence. To address this, we propose two solutions. Initially, we suggest randomly altering the data prior to the creation of each tree, which averages out the bias. Alternatively, we suggest using hyperplanes with random slopes for data slicing, a method that effectively eliminates the artifact in the anomaly score heat maps. We demonstrate the enhanced algorithm's robustness by examining the variance of scores for data points distributed along constant level sets. We provide AUROC and AUPRC values for our synthetic datasets, as well as real-world benchmark datasets. Our findings indicate no significant difference in the rate of convergence or computation time between the standard Isolation Forest and EIF."}
{"text": "The presented study introduces a new attention model, capable of precisely focusing on target objects of diverse scales and forms within images. This model is trained to successively filter out non-essential regions in an input image through a sequential, attentive process across multiple layers of a convolutional neural network. In each layer, the attentive process decides whether to transmit or obstruct features at specific spatial positions for utilization in subsequent layers. The proposed progressive attention mechanism demonstrates effectiveness, particularly when integrated with hard attention. Additionally, local contexts are utilized to incorporate surrounding features of each location, resulting in a more accurate attention probability map estimation. Results from experiments on both synthetic and real datasets indicate that the proposed attention networks surpass traditional attention methods in visual attribute prediction tasks."}
{"text": "In this study, we introduce the Binary Graph Convolutional Network (Bi-GCN), a novel approach that significantly reduces memory consumption and accelerates inference speed in graph representation learning. Traditional Graph Neural Networks (GNNs) often require loading the entire attributed graph for processing, which can be problematic when memory resources are limited, particularly with large graphs. To address this issue, our Bi-GCN employs binarization for both network parameters and input node features, and replaces matrix multiplications with binary operations for efficiency. Theoretical analysis indicates that our Bi-GCN reduces memory consumption for both network parameters and input data by an average of ~30x, and speeds up inference by an average of ~47x on citation networks. Additionally, we develop a new gradient approximation-based back-propagation method to effectively train our Bi-GCN. Experimental results demonstrate that our Bi-GCN performs comparably to full-precision baselines, and our binarization approach is easily applicable to other GNNs, as demonstrated in our experiments."}
{"text": "The Single Image Super-Resolution (SISR) challenge involves learning a transformation from low-resolution images to their high-resolution counterparts. This task is notoriously challenging due to its ill-posed nature. In recent years, Convolutional Neural Networks (CNNs) have demonstrated exceptional performance in SISR. However, the images generated by CNNs lack fine details. To address this issue, Generative Adversarial Networks (GANs) have been developed to recover sharp details. However, GANs are notoriously difficult to train and tend to introduce artifacts in high-resolution images. In this study, we propose a method that employs CNNs to align images across different spaces, a space designed using convex optimization techniques. This approach encourages CNNs to learn both high-frequency and low-frequency components of images. Our results demonstrate that this method can effectively recover fine details in images and exhibits stability during the training process."}
{"text": "Multi-label image and video classification represent significant yet complex tasks within the realm of computer vision. The primary difficulties stem from the need to identify spatial or temporal dependencies between labels and locate distinguishing features for each class. To address these challenges, we propose the utilization of cross-modality attention in conjunction with semantic graph embedding for multi-label classification. By constructing a label graph, we introduce an adjacency-based similarity graph embedding method to derive semantic label embeddings, which effectively leverage label relationships. Subsequently, our innovative cross-modality attention maps are generated under the guidance of the learned label embeddings. Our method surpasses existing state-of-the-art approaches on two multi-label image classification datasets (MS-COCO and NUS-WIDE). Furthermore, our method's effectiveness is confirmed on a large multi-label video classification dataset (YouTube-8M Segments), demonstrating its generalization capabilities."}
{"text": "In the absence of additional constraints, image inpainting inherently offers multiple viable solutions, provided they appear credible. Recent advancements have led to the development of multiple-solution inpainting methods, capable of generating a variety of results. However, these techniques struggle to guarantee the quality of each solution, often resulting in distorted structures and blurry textures. To address this issue, we introduce a two-phase model for diverse inpainting. The initial phase generates multiple rough results, each with a distinct structure, while the second phase refines each rough result individually by enhancing texture. This model is inspired by the hierarchical vector quantized variational auto-encoder (VQ-VAE), which separates structural and textural information through its hierarchical architecture. Furthermore, the vector quantization in VQVAE allows for autoregressive modeling of the discrete distribution over the structural information. Sampling from this distribution can effortlessly generate diverse and high-quality structures, constituting the first phase of our model. In the second phase, we incorporate a structural attention module within the texture generation network. This module leverages structural information to capture long-range correlations. Additionally, we employ the VQ-VAE to compute two feature losses, which aid in improving structure coherence and texture realism, respectively. Our method, as demonstrated by experiments on CelebA-HQ, Places2, and ImageNet datasets, not only increases the diversity of inpainting solutions but also improves the visual quality of the generated multiple images. The code and models for our method can be found at: https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting."}
{"text": "Automated chromosome karyotype analysis, particularly in the context of genetic diseases, holds significant clinical significance for diagnosis and treatment. Manual analysis is laborious and time-consuming, leading to the widespread use of computer-assisted methods. However, due to the strip-like shape of chromosomes, they often overlap in images, negatively impacting analysis accuracy. Traditional segmentation methods for overlapping chromosomes are typically reliant on manually tagged features, making them susceptible to image quality issues such as resolution and brightness. To rectify this, this paper introduces an adversarial multiscale feature learning framework to enhance the precision and adaptability of overlapping chromosome segmentation. The framework utilizes a nested U-shape network with dense skip connections as the generator to extract optimal chromosome image representations through multiscale features. Subsequently, a conditional generative adversarial network (cGAN) is employed to produce images resembling the originals, with training stability improved via the least-square GAN objective. Lastly, Lovasz-Softmax is implemented to facilitate model convergence in a continuous optimization setting. Compared to existing algorithms, our framework demonstrates superior performance using public datasets across eight evaluation criteria, indicating its promising potential for overlapping chromosome segmentation."}
{"text": "We present a new, unsupervised model for representation learning, named Robust Block-Diagonal Adaptive Locality-constrained Latent Representation (rBDLR). This model is capable of recovering multi-subspace structures and simultaneously extracting adaptive locality-preserving salient features. Utilizing a Frobenius-norm based latent low-rank representation model, rBDLR concurrently learns coding coefficients and salient features, enhancing results by boosting robustness to outliers and errors in the provided data. It preserves local information of salient features adaptively and maintains the block-diagonal structures of the coefficients. To boost robustness, we perform latent representation and adaptive weighting in a recovered clean data space. To ensure the coefficients are block-diagonal, we employ auto-weighting through minimizing the reconstruction error based on salient features, subject to a block-diagonal regularizer. This guarantees a strict block-diagonal weight matrix and salient features with adaptive locality-preserving properties. By minimizing the gap between the coefficient and weights matrices, we can acquire a block-diagonal coefficients matrix, which also facilitates the exchange and propagation of useful information between salient features and coefficients. Extensive results showcase the superiority of rBDLR compared to other contemporary methods."}
{"text": "In the realm of domain generalization (DG) and unsupervised domain adaptation (UDA), a common approach involves cross-domain feature alignment to minimize the distribution gap between different domains, thereby enabling the learning of domain-agnostic representations. However, this alignment, being task-agnostic, may lead to a decrease in the discriminative power of the feature representation, thereby impeding optimal performance. In this study, we introduce a unified framework, Feature Alignment and Restoration (FAR), designed to enhance both the generalization and discriminative power of the networks for effective DG and UDA. Our approach involves feature alignment (FA) across domains by matching the moments of the distributions of selectively attended features to minimize the discrepancy. To maintain high discrimination, we propose a Feature Restoration (FR) operation that distills task-relevant features from residual information and employs them to supplement the aligned features. For improved disentanglement, we impose a dual ranking entropy loss constraint during the FR step to promote the segregation of task-relevant and task-irrelevant features. Extensive experiments on various classification benchmarks confirm the superior performance and robust generalization of our FAR framework for both DG and UDA."}
{"text": "Producing point clouds, such as molecular structures, in various rotations, displacements, and arrangements continues to pose a significant challenge. However, neural networks equipped with symmetry-invariant layers have demonstrated the ability to optimize their training objective in a data-efficient manner. In line with this, we introduce an architecture capable of generating valid Euclidean distance matrices, inherently invariant to the rotation and translation of the described object. Inspired by the objective of generating molecular structures in Cartesian space, we employ this architecture to establish a Wasserstein GAN with a permutation-invariant critic network. This setup enables the generation of molecular structures in a single instance by producing Euclidean distance matrices with a three-dimensional embedding."}
{"text": "In numerous machine learning applications, crafting an effective data representation often serves as the foundation for constructing effective solutions. This is due to the fact that most learning algorithms rely on features to discover models for the data. For example, classification accuracy can be enhanced when data is mapped to a space where classes are distinct, and regression can be simplified by identifying a data manifold in the feature space. Typically, features are transformed using statistical techniques like principal component analysis, or manifold learning methods such as Isomap or locally linear embedding. Among the various representation learning methods, the autoencoder stands out as a highly versatile tool. In this study, we aim to show how to manipulate the autoencoder's learned representations to elicit the desired learning behavior. To achieve this, we present a series of learning tasks: data visualization through embedding, image denoising, semantic hashing, anomaly behavior detection, and instance generation. Each task is modeled from a representation learning perspective, adhering to the latest methodologies in each field. For each task, a solution is proposed using autoencoders as the sole learning method. The theoretical concepts are applied using a variety of datasets for different problems, and the results are analyzed in each case study. Additionally, we discuss six other learning applications and examine the current challenges and strategies for explainability in the context of autoencoders. This research suggests that, through structural modifications and adjustments to the objective function, autoencoders could form the basis of a solution for numerous problems that can be modeled as a transformation of the feature space."}
{"text": "The ability to forecast interdependencies among multiple time series is essential for tasks like anomaly detection, financial risk management, causal analysis, and demand forecasting. However, the challenges in calculating time-varying and high-dimensional covariance matrices often restrict existing techniques to managing no more than a few hundred dimensions or imposing stringent assumptions on the relationship between series. We suggest integrating an RNN-based time series model with a Gaussian copula process output model featuring a low-rank covariance structure to decrease computational complexity and accommodate non-Gaussian marginal distributions. This facilitates a substantial reduction in the number of parameters, enabling the modeling of time-varying correlations for thousands of time series. Our method demonstrates superior accuracy compared to current state-of-the-art benchmarks on various real-world datasets, and we conduct an ablation study to assess the impact of each model component."}
{"text": "In the process of data exploration, a frequent approach involves deriving a low-dimensional depiction of the data, distinguishing clusters of points within this representation, and scrutinizing the disparities between these clusters to discern their underlying meaning. We approach this process as an interpretable machine learning challenge by utilizing the model that generated the low-dimensional representation to aid in identifying the significant differences between the clusters. To address this challenge, we propose a novel form of explanation, the Global Counterfactual Explanation (GCE), and an algorithm, Transitive Global Translations (TGT), for calculating GCEs. TGT distinguishes the differences between each pair of groups using compressed sensing, yet it ensures that these pairwise differences are consistent across all groups. Experimentally, we show that TGT can identify explanations that accurately elucidate the model while maintaining a relatively sparse nature, and these explanations align with genuine patterns in the data."}
{"text": "The escalating integration of AI technology has led to the exposure of security and privacy vulnerabilities within machine learning systems. One such vulnerability allows an attacker to access confidential information about the training instances of the targeted machine learning model, a phenomenon known as model inversion attack. This attack method exploits the sequential utilization of classification scores to generate high-confidence representations for various classes. However, for deep networks, these procedures often result in unidentifiable representations that offer no benefit to the attacker. In this study, we propose a more practical definition of model inversion, where the attacker is cognizant of the intended purpose of the attacked model (for example, whether it is an OCR system or a facial recognition system), and the objective is to discover authentic class representations within the corresponding lower-dimensional manifold (of, respectively, general symbols or general faces). To achieve this, we utilize the properties of generative adversarial networks to construct a connected lower-dimensional manifold, and we demonstrate the effectiveness of our model inversion attack, which is executed within that manifold."}
{"text": "In this study, we introduce a novel global analysis framework for a specific category of low-rank matrix recovery problems on the Riemannian manifold. We delve into the global characteristics of Riemannian optimization with random initialization. We employ the Riemannian gradient descent algorithm to minimize a least squares loss function, examining both the asymptotic behavior and the exact convergence rate. We uncover an undiscovered geometric feature of the low-rank matrix manifold, namely, the presence of spurious critical points for the basic least squares function on the manifold. We demonstrate that under certain conditions, the Riemannian gradient descent, initialized randomly, evades these spurious critical points with high probability, converging only to the true solution in a nearly linear convergence rate, approximately (\\text(\\frac)+ \\text(n)) iterations to attain an \\epsilon-accurate solution. We present two applications as illustrative examples for our global analysis. The first application pertains to a rank-1 matrix recovery problem. The second application is an extension of the Gaussian phase retrieval problem, which only adheres to the weak isometry property but exhibits behavior similar to the first application, except for an additional saddle set. Our convergence guarantee is nearly optimal and almost dimension-free, providing a comprehensive explanation for the observed numerical results. The global analysis may potentially be extended to other data problems with random measurement structures and empirical least squares loss functions."}
{"text": "A novel, unsupervised, and iterative point-set registration method is presented for an unlabeled N-dimensional Euclidean point-cloud. This approach leverages linear least squares and evaluates all potential point pairings, continuously adjusting the alignment of the two sets until the number of one-to-one point correspondences no longer surpasses the maximum permissible limit."}
{"text": "The process of preparing and examining histopathology slides involves numerous steps, each with a wide range of adjustable parameters. These parameters can differ between pathology labs and even within the same lab over time, leading to substantial variations in tissue appearance that complicate the application of automated image analysis techniques. To mitigate this issue, common strategies include methods like staining normalization, which aim to diminish the variability in appearance. In this study, we present a systematic solution employing domain-adversarial neural networks. Our hypothesis is that eliminating domain information from the model's representation enhances generalization. We validated this hypothesis in the context of mitosis detection in breast cancer histopathology images, comparing our approach with two other methods. Our results indicate that integrating color augmentation with domain-adversarial training offers a superior approach for enhancing the generalization of deep learning methods compared to conventional techniques."}
{"text": "We suggest employing boosted regression trees as a method for deriving human-comprehensible solutions to reinforcement learning issues. By aggregating multiple regression trees, boosting enhances their accuracy without compromising their inherent interpretability. Previous research has primarily concentrated on reinforcement learning and interpretable machine learning separately, with limited advancements in interpretable reinforcement learning. Our experimental findings indicate that boosted regression trees generate solutions that are both understandable and comparable in quality to top-tier reinforcement learning techniques."}
{"text": "Image transformation, often referred to as image morphing, entails a visually seamless transition between two (or more) initial images. To ensure aesthetic appeal, this transition should exhibit (i) a smooth progression, (ii) minimal alterations to the images, and (iii) a natural appearance, avoiding artificial artifacts in the transition's images. The Wasserstein Barycenter Problem (WBP) is a well-known method for achieving a smooth transition, as it guarantees minimal changes under the Wasserstein metric. However, the resulting images may appear unnatural. In this study, we introduce a novel approach for image morphing that encompasses all three desirable properties. We formulate a constrained version of the WBP that mandates the intermediate images to adhere to an image prior. We present an algorithm that addresses this problem and illustrate its application using the sparse prior and generative adversarial networks."}
{"text": "Deep learning-based clustering techniques concurrently develop representations and groupings by concurrently optimizing a clustering loss and a non-clustering loss. In these techniques, a deep neural network is employed for representation learning in conjunction with a clustering network. In contrast to enhancing clustering performance by adhering to this framework, we put forth a more straightforward method of optimizing the intertwining of the encoded latent representation learned by an autoencoder. We characterize intertwining as the proximity of pairs of points belonging to the same class or structure, in comparison to pairs of points from distinct classes or structures. To assess the intertwining of data points, we employ the soft nearest neighbor loss, and augment it by incorporating an annealing temperature factor. Employing our proposed method, the test clustering accuracy was 96.2% on the MNIST dataset, 85.6% on the Fashion-MNIST dataset, and 79.2% on the EMNIST Balanced dataset, surpassing our baseline models."}
{"text": "This study introduces a method for teaching a hierarchical compositional AND-OR model for interpretable image synthesis through the sparsification of the generator network. The suggested approach employs the scene-objects-parts-subparts-primitives hierarchy in image representation, where a scene comprises various types (OR), each containing multiple objects (AND). This hierarchy can be recursively formulated and terminates at the primitive level, such as wavelets-like basis.\n\nTo implement this AND-OR hierarchy in image synthesis, we develop a generator network with two primary components: (i) Each layer of the hierarchy is represented by an over-complete set of convolutional basis functions, with off-the-shelf convolutional neural architectures used to implement the hierarchy. (ii) Sparsity-inducing constraints are incorporated during end-to-end training, resulting in a sparsely activated and sparsely connected AND-OR model from the initially densely connected generator network. A simple sparsity-inducing constraint is employed, allowing only the top-k basis functions to be activated at each layer (where k is a hyper-parameter).\n\nThe learned basis functions are also capable of image reconstruction to provide insights into the input images. The proposed method is evaluated on four benchmark datasets, and the results indicate that meaningful and interpretable hierarchical representations are learned, with superior qualities of image synthesis and reconstruction compared to baselines."}
{"text": "Optical flow algorithm evaluations primarily assess estimation accuracy through direct comparison of predicted flow fields with ground truth or indirectly via frame interpolation and comparison of the interpolated frames with actual frames, often using objective measures like mean squared error. However, these measures do not fully capture the subjective quality perceived by users. To address this, we undertook a subjective quality assessment study for interpolated frames from the Middlebury benchmark. We collected forced-choice paired comparisons between interpolated images and their ground truth. To enhance observer sensitivity in paired comparisons, we introduced a novel method called artefact amplification in the field of full-reference quality assessment. From the crowdsourced data, we derived absolute quality scale values using Thurstone's model, which led to a re-ranking of 155 participating algorithms based on the visual quality of interpolated frames. This re-ranking underscores the importance of visual quality assessment as an additional metric for optical flow and frame interpolation benchmarks. Moreover, it serves as a basis for developing new image quality assessment (IQA) methods focused on the perceptual quality of interpolated images. As an initial step, we proposed a new full-reference method, WAE-IQA, which outperformed the current best FR-IQA approach from literature by weighing local differences between interpolated images and their ground truth."}
{"text": "To create an autonomous vehicle capable of navigating complex driving situations and communicating effectively with other road users necessitates the capacity to semantically comprehend and interpret the driving environment, often achieved through the analysis of extensive naturalistic driving data. A crucial concept enabling autonomous vehicles to learn from human drivers and derive insights is the recognition of fundamental traffic components, referred to as traffic primitives. However, the exponential growth of data poses a significant challenge in identifying primitives from high-dimensional, time-series traffic data involving various types of road users. Consequently, the automatic extraction of primitives has emerged as a cost-effective approach to aid autonomous vehicles in understanding and forecasting intricate traffic scenarios. Furthermore, the primitives derived from raw data should 1) be suitable for automated driving applications and 2) be easily applicable for generating new traffic scenarios. Regrettably, existing literature lacks a method for automatically learning these primitives from extensive traffic data. This paper offers two contributions. Firstly, we propose a novel framework for generating new traffic scenarios using minimal traffic data. Secondly, we introduce a nonparametric Bayesian learning method -- a sticky hierarchical Dirichlet process hidden Markov model -- to automatically extract primitives from multidimensional traffic data without prior knowledge of the primitive settings. The efficacy of the developed method is demonstrated using one day of naturalistic driving data. The experimental results indicate that the nonparametric Bayesian learning method is capable of extracting primitives from traffic scenarios where both binary and continuous events coexist."}
{"text": "We present a compilation of datasets derived from fundamental physics research, encompassing particle physics, astroparticle physics, hadron- and nuclear physics, for the purpose of supervised machine learning investigations. These datasets, comprising hadronic top quarks, cosmic-ray induced air showers, phase transitions in hadronic matter, and generator-level histories, are now accessible to facilitate future research on cross-disciplinary machine learning and transfer learning in fundamental physics. This work introduces a graph-based neural network architecture, which offers a straightforward and adaptable solution for a multitude of supervised learning tasks within these domains. Our approach demonstrates performance comparable to leading dedicated methods across all datasets. To facilitate application to diverse problems, we offer clear guidelines on constructing graph-based representations of data structures relevant to fundamental physics, and provide code implementations for several of these. Implementations are also provided for our proposed method and all reference algorithms."}
{"text": "We explore the challenge of generating a high-quality dense depth map from a single RGB input image. Initially, we employ a standard encoder-decoder convolutional neural network architecture and investigate the potential benefits of global information processing for enhanced depth estimation. To achieve this, we introduce a transformer-based architecture module, which partitions the depth range into adaptive image-specific bins, with the center value of each bin estimated. The final depth values are calculated as linear combinations of these bin centers. We label this new module AdaBins. Our findings indicate significant advancements over existing state-of-the-art on various popular depth datasets, across all evaluation metrics. Furthermore, we substantiate the efficacy of the proposed module through an ablation study and offer the code and pre-trained weights of the new state-of-the-art model for replication."}
{"text": "Nonlinear Time-lagged Autoencoders (TAEs) have emerged as a deep learning regression method for identifying slow modes in complex dynamical systems. Yet, a comprehensive analysis of these nonlinear TAEs is still absent. This study delves into the potential and constraints of TAEs, employing both theoretical and numerical investigations. Theoretically, we establish performance bounds for TAEs in slow mode discovery, revealing that they typically learn a combination of slow and maximum variance modes. Numerically, we demonstrate scenarios where TAEs successfully and unsuccessfully identify the primary slowest mode in two model systems: a 2D \"Washington beltway\" potential and the alanine dipeptide molecule in explicit water. We further contrast the TAE findings with those derived from state-free reversible VAMPnets (SRVs), a variational-based neural network approach for slow mode discovery. Our results indicate that SRVs can accurately identify slow modes where TAEs falter."}
{"text": "In this study, we introduce two novel evaluation metrics for assessing generative models in the class-conditional image generation scenario. These metrics are derived from the two most widely used unconditional metrics: the Inception Score (IS) and the Frechet Inception Distance (FID). A theoretical examination justifies the rationale behind each proposed metric and demonstrates the connection between the new metrics and their unconditional counterparts. This connection is expressed as a product for the IS and an upper bound for the FID. We conduct an extensive experimental evaluation, comparing the metrics with their unconditional versions and other metrics, and employ them to scrutinize existing generative models, thereby offering additional insights into their performance, encompassing unlearned classes and mode collapse."}
{"text": "The proliferation of electronic health records (EHR) has provided researchers with a wealth of data to explore various medical queries. The selection of cohorts for the hypothesis being investigated is a crucial factor in EHR analysis, particularly for rare diseases, where the number of records is limited, thereby compromising the analysis's robustness. To address this issue, data augmentation techniques, primarily utilizing simulated records, have been effectively employed in other domains. In this study, we introduce ODVICE, a data augmentation framework that employs a medical concept ontology to systematically expand records through a novel ontologically guided Monte-Carlo graph spanning algorithm. This tool offers users a set of interactive controls to manage the augmentation process. We assess the significance of ODVICE by conducting experiments on the MIMIC-III dataset for two learning tasks. Our findings indicate that ODVICE-augmented cohorts exhibit improved predictive performance, with approximately a 30% increase in the area under the curve (AUC) compared to the non-augmented dataset and other data augmentation strategies."}
{"text": "This study presents two adaptable regularization techniques that manipulate weight matrices directly. The initial method, Weight Reinitialization, employs a streamlined Bayesian hypothesis with the partial resetting of a sparse selection of parameters. The subsequent method, Weight Shuffling, incorporates an entropy- and weight distribution-neutral non-white noise into the parameters. This method can also be perceived as an ensemble strategy. The presented techniques are assessed on standard datasets, including MNIST, CIFAR-10, and the JSB Chorales database, as well as time series modeling tasks. Our findings indicate improvements in both performance and network entropy. The associated code for this research is accessible via a GitHub repository (https://github.com/rpatrik96/lod-wmm-2019)."}
{"text": "In various fields such as healthcare, meteorology, finance, and numerous others, sparse and non-uniformly sampled multivariate time series are prevalent. The current state-of-the-art methods primarily concentrate on classification, regression, or prediction tasks for such data. In the context of prediction, it is crucial not only to predict the accurate value but also to determine the time at which this value will occur in the non-uniform time series. This research proposes a method for predicting not only the values but also the expected time of their occurrence."}
{"text": "Mainstream image captioning models predominantly employ Convolutional Neural Network (CNN) image features, generating captions through recurrent models. To enhance these models, image scene graphs have been introduced, leveraging their structural semantics, such as object entities, relationships, and attributes. However, several studies have highlighted that using scene graphs from a black-box generator negatively impacts image captioning performance. To mitigate this, we propose a framework, \\textbf, that utilizes only scene graph labels for competitive image captioning performance. The strategy is to bridge the semantic gap between the scene graph derived from the image and the one derived from the caption. This is achieved by incorporating the spatial location of objects and Human-Object-Interaction (HOI) labels as an additional HOI graph. SG2Caps surpasses existing scene graph-only captioning models significantly, suggesting scene graphs as a promising representation for image captioning. Direct utilization of scene graph labels eliminates the need for costly graph convolutions over high-dimensional CNN features, reducing trainable parameters by 49%. Our code is accessible at: https://github.com/Kien085/SG2Caps."}
{"text": "The significance of semantic segmentation using high-resolution remote sensing images lies in its applications, including urban planning, environmental conservation, and landscape monitoring. However, automating semantic segmentation, specifically for high-resolution images with substantial spatial and spectral complexity, remains a formidable challenge. This area of research, which facilitates scene-level landscape pattern analysis and decision-making, is both intriguing and promising. In this study, we introduce an approach for automatic land segmentation utilizing the Feature Pyramid Network (FPN). Despite FPN's ability to construct a feature pyramid with high-level semantics, its inherent issues in feature extraction and fusion limit its potential to incorporate more discriminative features. To overcome this, we develop the Attention Aggregation Module (AAM) to enhance multi-scale feature learning through attention-guided feature aggregation. By integrating FPN and AAM, we create the Attention Aggregation Feature Pyramid Network (A2-FPN) for the semantic segmentation of high-resolution remote sensing images. Our A2-FPN demonstrates superior segmentation accuracy through extensive experiments on three datasets. The code for our proposed method can be found at https://github.com/lironui/A2-FPN."}
{"text": "Enhancing sample efficiency is a significant challenge in reinforcement learning (RL), and the CURL algorithm, which employs contrastive learning to extract high-level features from individual video frames, offers an efficient solution~\\citep. However, CURL treats consecutive video frames in a game as independent entities, despite their high correlation. To address this issue and boost data efficiency, we propose a novel algorithm, masked contrastive representation learning for RL, which considers the correlation among consecutive inputs. This method incorporates an additional Transformer module alongside the CNN encoder and policy network found in CURL. During training, we randomly conceal the features of certain frames, and the CNN encoder and Transformer are utilized to reconstruct these features based on the context frames. Contrastive learning is used to jointly train the CNN encoder and Transformer, with the reconstructed features being similar to the original ones and dissimilar to others. During inference, the CNN encoder and policy network are employed to make decisions, while the Transformer module is discarded. Our method demonstrates consistent improvements over CURL in 14 out of 16 environments from the DMControl suite and 21 out of 26 environments from the Atari 2600 Games. The code for this method can be found at https://github.com/teslacool/m-curl."}
{"text": "There has been a growing focus on semi-supervised classification utilizing graphical data. A novel category of learning models has arisen, primarily characterized by classifying data following a graph convolution operation. To evaluate the effectiveness of this method, we investigate the classification of a Gaussian mixture, with the data representing node attributes in a stochastic block model. We demonstrate that graph convolution expands the linear separability range of the data by approximately 1/D, where D is the average node degree, compared to the mixture model data alone. Additionally, we discover that the linear classifier, derived by minimizing cross-entropy loss post-graph convolution, generalizes well to out-of-distribution data, even when the unseen data has distinct intra- and inter-class edge probabilities compared to the training data."}
{"text": "We present a variational approach for multi-view shape-from-shading under natural lighting conditions. The central concept involves linking PDE-based solutions for single-image based shape-from-shading issues across various images and color channels through a unifying variational formulation. Instead of iteratively addressing the individual SFS problems and optimizing consistency across images and channels, which often results in suboptimal outcomes, we put forth an efficient solution to the combined problem using an ADMM algorithm. In a multitude of experiments on both simulated and real-world imagery, we showcase that the fusion of multiple-view reconstruction and shape-from-shading yields highly precise dense reconstructions without the necessity of computing dense correspondences. Our proposed variational integration of multi-view shape-from-shading techniques expands their applicability to complex real-world reconstruction problems, resulting in intricate geometry even in regions exhibiting smooth brightness variation and lacking texture."}
{"text": "The advent of deep learning has enabled the creation of more sophisticated image classification models with enhanced discriminative capabilities. However, these complex models necessitate a large number of labeled samples for generalization during training. This is particularly challenging in the field of medical image analysis due to the scarcity of training data, often leading to overfitting. This paper introduces and examines a reinforced classifier designed to enhance generalization with limited training data. Leveraging the concept of reinforcement learning, this classifier employs generalization-feedback from a subset of the training data to update its parameters, rather than relying solely on conventional cross-entropy loss. The effectiveness of the proposed classifier is assessed by applying it to three distinct classification problems, compared to standard deep classifiers equipped with existing overfitting-prevention techniques. The results demonstrate not only an overall improvement in classification performance, but also the classifier's notable ability to learn generalized patterns, which could hold significant promise for medical classification tasks."}
{"text": "In this study, we contend that the top-down connections within FPN architectures have a dual impact on tiny object detection, not just positively. We introduce a novel concept, the fusion factor, to regulate the data flow from deeper layers to shallower layers in FPN, tailoring it for tiny object detection. Through a series of experiments and analysis, we develop a statistical method to determine an optimal fusion factor value for a specific dataset, which is influenced by the distribution of objects in each layer. Our research is validated on tiny object detection datasets such as TinyPerson and Tiny CityPersons. Our findings indicate that when FPN is adjusted with an appropriate fusion factor, the network demonstrates substantial improvements in performance compared to the baseline on tiny object detection datasets. The associated code and models will be made publicly available."}
{"text": "In the present study, we introduce a new image representation termed face X-ray, designed to identify manipulation in facial images. The face X-ray of an input image is a grayscale image that demonstrates whether the input can be broken down into a combination of images from distinct sources. This is achieved by visualizing the blending boundary for a forged image and the absence of blending for an authentic image. Our findings indicate that the majority of existing face alteration methods incorporate a common stage: the blending of the modified face into an existing background image. Consequently, face X-ray serves as an efficient tool for detecting forgery produced by most existing face manipulation algorithms. The versatility of face X-ray lies in its assumption of the presence of a blending step, without relying on any specific artifacts related to a particular face manipulation technique. Notably, the algorithm for calculating face X-ray can be trained without utilizing fake images generated by any of the current state-of-the-art face manipulation methods. Extensive testing reveals that face X-ray maintains its effectiveness when applied to forgery generated by unseen face manipulation techniques, while most existing face forgery detection or deepfake detection algorithms exhibit a substantial drop in performance."}
{"text": "The increasing use of attention mechanisms in models has raised questions about the interpretability of attention distributions. While these mechanisms offer insights into the model's operations, their use as explanations for model predictions remains questionable. The research community is actively searching for more interpretable strategies to better identify the local active regions that significantly contribute to the final decision. To enhance the interpretability of existing attention models, we introduce a novel Bilinear Representative Non-Parametric Attention (BR-NPA) strategy. This strategy captures task-relevant human-interpretable information by first distilling the target model to generate higher-resolution intermediate feature maps. Representative features are then grouped based on local pairwise feature similarity, resulting in finer-grained, more precise attention maps that highlight task-relevant parts of the input. The attention maps are ranked based on the 'active level' of the compound feature, providing information about the importance of the highlighted regions. This model can be easily integrated into a wide range of modern deep models that involve classification. It is also more accurate, faster, and requires less memory than typical neural attention modules. Extensive experiments demonstrate more comprehensive visual explanations compared to the current state-of-the-art visualization model across various tasks such as few-shot classification, person re-identification, and fine-grained image classification. The proposed visualization model offers crucial insights into how neural networks focus their attention differently in different tasks."}
{"text": "The study explores a variety of feature engineering pipelines, specifically designed for a CNN, to classify eyes-open and eyes-closed states from EEG time-series data sourced from the Bonn dataset. By employing Takens' embedding, we construct simplicial complexes from the EEG data, which are then utilized to compute \\epsilon-series of Betti-numbers and \\epsilon-series of graph spectra, two topological invariants derived from the latent geometry of these complexes. These computations serve to bridge a gap in the literature by providing a benchmark for comparison with raw EEG time-series data. This research leverages Topological Data Analysis principles to capture the local geometry of time-series data. Furthermore, the robustness of these feature pipelines is assessed under conditions of downsampling and data reduction. The aim of this paper is to set more definitive expectations for time-series classification using geometric features and for the performance of CNNs on time-series data of reduced resolution."}
{"text": "A prevalent chronic condition, cervical spondylosis (CS), impacts approximately two-thirds of the population, imposing substantial personal and societal burdens. Early detection is crucial for enhancing cure rates and minimizing costs, yet the intricate pathology and subtle symptoms in the early stages complicate diagnosis. Moreover, the high cost and time commitment of hospital medical services often divert attention from CS identification. Therefore, there is an urgent need for a user-friendly, cost-effective method for intelligent CS identification. In this study, we introduce an intelligent CS identification method grounded in deep learning, utilizing surface electromyography (sEMG) signals. Given the complexity, high dimensionality, and limited utility of the sEMG signal, we designed and implemented a multi-channel EasiCSDeep algorithm, which incorporates a feature extraction, spatial relationship representation, and classification algorithm. To our knowledge, EasiCSDeep represents the first application of deep learning and sEMG data for CS identification. In comparison to existing state-of-the-art algorithms, our method demonstrates a substantial improvement."}
{"text": "The inherent characteristic of natural gradients lies in their resistance to modifications of the model through differentiable reparameterizations, which is a desirable property. Nevertheless, this property becomes invalid in practical applications that employ finite step sizes instead of infinitesimal ones. In this research, we delve into the invariance properties from a dual standpoint of Riemannian geometry and numerical solutions of differential equations. We define the order of invariance of a numerical method as the rate at which it converges to an invariant solution. We suggest employing higher-order integrators and geodesic corrections to generate more invariant optimization paths. We establish the numerical convergence properties of geodesic-corrected updates and demonstrate that they can match the computational efficiency of standard natural gradients. Empirically, we show that invariance accelerates optimization, and our methods surpass traditional natural gradient techniques in the training of deep neural networks and natural policy gradient for reinforcement learning."}
{"text": "The objective of Space-time Video Super-Resolution (STVSR) is to enhance both spatial and temporal resolutions in low-resolution and low-frame-rate videos. While deformable convolution-based methods have shown promising results in STVSR, they are limited to inferring pre-defined intermediate frames from the training stage. Moreover, these methods tend to disregard short-term motion cues among adjacent frames. In this study, we introduce the Temporal Modulation Network (TMNet) to interpolate arbitrary intermediate frames with precise high-resolution reconstructions. Our approach includes a Temporal Modulation Block (TMB) to control feature interpolation via modulating deformable convolution kernels. To effectively utilize temporal information, we propose a Locally-temporal Feature Comparison (LFC) module, coupled with Bi-directional Deformable ConvLSTM, to extract both short-term and long-term motion cues in videos. Our TMNet surpasses previous STVSR methods, as demonstrated by experiments on three benchmark datasets. The code for our TMNet can be found at https://github.com/CS-GangXu/TMNet."}
{"text": "In the realm of training deep neural networks, dropout serves as a common method to prevent overfitting by regularizing large models. Its performance enhancement is often attributed to the avoidance of co-adaptation between nodes. However, this raises the question of whether co-adaptation avoidance fully explains the dropout effect. In this study, we offer an additional rationale for dropout's efficacy and introduce a novel approach to develop improved activation functions. Our initial findings suggest that dropout can be understood as an optimization technique, facilitating the input's movement towards the saturation region of the nonlinear activation function by expediting gradient information flow, even in the saturation area during backpropagation. Leveraging this explanation, we propose a new activation function, , that enhances gradient flow in the saturation region. This allows the input to reach the saturation area, resulting in a more robust model as it converges on a flatter region. Our experimental results corroborate our interpretation of dropout and validate the proposed GAAF technique's effectiveness in enhancing image classification performance, demonstrating the expected properties."}
{"text": "Deep learning models, particularly in software applications, have demonstrated remarkable success in tackling complex perceptual tasks and delivering intelligent services. Yet, these models have been exposed to be susceptible to adversarial examples. Despite numerous methods proposed to counter adversarial inputs, many are either attack-specific or ineffective against novel attacks. Moreover, existing techniques often possess intricate structures or mechanisms that lead to excessive computational overhead or delay, making them impractical for real-world software. We introduce DAFAR, a feedback framework designed to detect and purify adversarial examples efficiently and universally, with minimal area and time overhead. DAFAR consists of a victim model, a supplementary feedback network, and a detector. The core concept involves transferring high-level features from the victim model's feature extraction layers into the feedback network to recreate the input, forming a feedback autoencoder. For robust attacks, this transformation converts the subtle attack on the victim model into a conspicuous reconstruction-error attack on the feedback autoencoder, making it simpler to identify; for weaker attacks, the reformulation process disrupts the structure of the adversarial examples. Experiments on MNIST and CIFAR-10 datasets demonstrate that DAFAR is effective against prominent and presumably the most advanced attacks, maintaining performance on genuine samples, and exhibiting high effectiveness and universality across attack methods and parameters."}
{"text": "We put forth a strategy for unsupervised video object segmentation, leveraging the knowledge embedded in image-based instance embedding networks. The instance embedding network generates an embedding vector for each pixel, facilitating the identification of all pixels belonging to the same object. Despite being trained on static images, the instance embeddings exhibit stability across consecutive video frames, enabling us to connect objects over time. Consequently, we adapt instance networks trained on static images for video object segmentation, integrating the embeddings with objectness and optical flow features, without the need for model retraining or online fine-tuning. Our proposed method surpasses the performance of current state-of-the-art unsupervised segmentation methods in the DAVIS dataset and the FBMS dataset."}
{"text": "In this study, we focus on the application of reward shaping as a method for integrating domain expertise into reinforcement learning (RL). Traditional reward shaping techniques, like potential-based approaches, heavily rely on a predefined shaping reward function. However, the conversion of human knowledge into numerical reward values can be flawed due to factors such as human cognitive bias, potentially leading to the underperformance of RL algorithms when fully utilizing the shaping reward function. To address this issue, we approach the problem of adaptively employing a given shaping reward function. We frame the utilization of shaping rewards as a bi-level optimization problem, with the lower level optimizing policy using the shaping rewards and the upper level optimizing a parameterized shaping weight function for optimal true reward maximization. We mathematically derive the gradient of the expected true reward with respect to the shaping weight function parameters and subsequently develop three learning algorithms based on distinct assumptions. Our experiments in sparse-reward cartpole and MuJoCo environments demonstrate that our algorithms effectively leverage advantageous shaping rewards while disregarding disadvantageous ones, and in some cases, even transforming them into beneficial rewards."}
{"text": "Normalizing Flows (NFs) exhibit the capability to model intricate distributions p(y) with substantial inter-dimensional correlations and high multimodality by converting a basic density p(z) via an invertible neural network under the change of variables formula. This characteristic is advantageous in multivariate structured prediction tasks, where conventional per-pixel loss-based methods fail to adequately represent strong correlations among output dimensions. This work focuses on conditional normalizing flows (CNFs), a type of NFs where the base density to output space mapping is contingent on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, can be trained with a likelihood-based objective, and, being generative flows, do not encounter mode collapse or training instabilities. We offer an effective method to train continuous CNFs for binary problems and specifically apply these CNFs to super-resolution and vessel segmentation tasks, demonstrating competitive performance on standard benchmark datasets in terms of likelihood and traditional metrics."}
{"text": "The exploration of numerous robotics and human-computer interaction applications can be significantly enhanced by the ability to comprehend the 3D motion of points within a changing environment, often referred to as scene flow. The majority of existing methods concentrate on stereo and RGB-D images as input, with only a few attempting to directly estimate scene flow from point clouds. In this study, we introduce a novel deep neural network, FlowNet3D, which learns scene flow from point clouds in an end-to-end manner. Our network concurrently learns deep, hierarchical features of point clouds and flow embeddings that represent point movements, facilitated by two newly developed learning layers for point sets. We assess the network's performance on both difficult synthetic data from FlyingThings3D and real Lidar scans from KITTI. Remarkably, our network, trained solely on synthetic data, generalizes effectively to real scans, surpassing various baselines and delivering competitive results compared to existing work. Furthermore, we illustrate two potential applications of our scene flow output (scan registration and motion segmentation) to highlight its broad applicability."}
{"text": "Achieving cost-effective, high-resolution 3D reconstructions is a significant objective in the SLAM domain. This paper introduces a framework for dense 3D reconstruction from monocular multispectral video sequences, combining semi-dense SLAM and Multispectral Photometric Stereo techniques. Initially, SALM (a) generates a semi-dense 3D shape, which is subsequently refined (b); it also recovers a sparse depth map, which serves as input for optimization-based multispectral photometric stereo to enhance the accuracy of dense surface normal recovery. Furthermore, SALM obtains camera pose, which is utilized in the view conversion process during fusion. Here, we combine the relative sparse point cloud with the dense surface normal using the cross-scale fusion method proposed in this paper, resulting in a dense point cloud with intricate texture details. The experiments demonstrate the effectiveness of our method in producing denser 3D reconstructions."}
{"text": "The proliferation of research on Generative Adversarial Networks (GANs) has experienced a steep rise in recent years, with the majority of its impact being observed in the computer vision domain, particularly in the realm of realistic image and video manipulation, particularly generation, leading to significant progress. However, the scope of GAN applications has expanded beyond computer vision, extending into disciplines such as time series and sequence generation. As a burgeoning area for GANs, there is ongoing research to generate high-quality, diverse, and private time series data. In this paper, we delve into various GAN variants designed for time series-related applications. We present a taxonomy of GANs, categorizing them into discrete-variant GANs and continuous-variant GANs, which handle discrete and continuous time series data, respectively. This paper highlights the latest and most prominent literature in this field, detailing their architectures, results, and applications. Additionally, we provide a list of common evaluation metrics and their applicability across various applications. Furthermore, we discuss privacy measures for these GANs and offer suggestions for handling sensitive data. Our objective is to provide a comprehensive and succinct overview of the latest and cutting-edge research in this area and its applications to real-world technologies."}
{"text": "Monocular 3D object detection plays a crucial role in autonomous driving due to its cost-effective nature. Compared to traditional 2D cases, it presents a more complex challenge due to its ill-posed property, primarily characterized by the absence of depth information. Advancements in 2D detection provide potential solutions to this issue. However, adapting a general 2D detector for this 3D task is not straightforward. In this research, we address this problem using a methodology based on a fully convolutional single-stage detector, introducing a versatile framework named FCOS3D. We convert the standard 7-DoF 3D targets into the image domain and separate them into 2D and 3D attributes. Objects are then distributed across various feature levels, taking into account their 2D scales, and are assigned during training based solely on the projected 3D-center. Additionally, we redefine the center-ness using a 2D Gaussian distribution based on the 3D-center to align with the 3D target formulation. This design ensures the framework is both straightforward and effective, eliminating the need for any 2D detection or 2D-3D correspondence priors. Our approach secured the first position among all vision-only methods in the nuScenes 3D detection challenge at NeurIPS 2020. The code and models can be found at https://github.com/open-mmlab/mmdetection3d."}
{"text": "Semantic segmentation of surgical procedures, particularly cataract surgery, is pivotal for both computer-assisted interventions and post-operative analysis. Understanding the scene is crucial for these purposes, and the localization of surgical instruments and anatomical structures is a fundamental aspect of such capabilities. Recent advancements in deep learning have significantly improved semantic segmentation techniques, but these methods are dependent on labeled datasets for model training. This paper presents a new dataset for semantic segmentation of cataract surgery videos, which is derived from the publicly available CATARACTS challenge dataset. Furthermore, the performance of several leading deep learning models for semantic segmentation is evaluated on this dataset. The dataset can be accessed at https://cataracts.grand-challenge.org/CaDIS/ ."}
{"text": "The significance of teaching in society lies in its capacity to disseminate human knowledge and nurture future generations. An effective teacher tailors teaching materials, employs effective methodologies, and designs targeted assessments, all in accordance with the learning patterns of students. In the realm of artificial intelligence, however, the role of teaching has not been comprehensively examined, with a primary focus on machines. This paper advocates for equal, if not increased, emphasis on teaching. Moreover, we propose an optimization framework, rather than heuristics, to generate effective teaching strategies, which we term `learning to teach'. This approach involves two intelligent agents: a student model, analogous to the learner in traditional machine learning algorithms, and a teacher model, responsible for selecting appropriate data, loss function, and hypothesis space to train the student model. The teacher model utilizes feedback from the student model to optimize its teaching strategies via reinforcement learning, fostering a co-evolutionary relationship between teacher and student. To validate the practical utility of our approach, we apply it to the training of deep neural networks (DNN), demonstrating that the `learning to teach' techniques enable the use of significantly less training data and fewer iterations, while maintaining near-identical accuracy across various DNN models (such as multi-layer perceptron, convolutional neural networks, and recurrent neural networks) and diverse machine learning tasks (like image classification and text understanding)."}
{"text": "Reinforcement learning has achieved significant advancements in the domains of games and robotics, but its practical applicability remains limited. Two primary hurdles are the sample inefficiency and inconsistent performance in difficult, infrequent scenarios. To address these issues, we propose a novel adversarial sampling method, which we call \"CoachNet\". CoachNet, a failure predictor, is trained concurrently with the agent to estimate the likelihood of failure. This probability is subsequently employed in a probabilistic sampling strategy to steer the agent towards more demanding training episodes. By doing so, the focus of training shifts from repetitive mastery of easy scenarios to addressing the agent's areas of weakness. This paper outlines the design of CoachNet, elucidates its fundamental principles, and provides empirical evidence of its efficacy in enhancing sample efficiency and robustness in common continuous control tasks."}
