{"text": "Objective: The aim is to devise a robust and scalable methodology for predicting individual patient costs by autonomously identifying concealed temporal patterns within multivariate time series data from patient insurance claims, utilizing a convolutional neural network (CNN) framework.\n\nMethods: Employing medical and pharmacy claims data from a healthcare insurer spanning 2013 to 2016, the initial two years were utilized for model development to forecast costs in the subsequent year. The dataset comprised multivariate time series of cost, visit, and medical attributes, structured as patient health status images (matrices with one axis representing time windows and the other displaying medical, visit, and cost features). These patient time series images were input into a CNN with a novel architecture. Post hyper-parameter optimization, the architecture featured three convolution and pooling layer blocks, each equipped with an LReLU activation function and a tailored kernel size for healthcare data. The temporal patterns learned by the CNN served as inputs to a fully connected layer.\n\nConclusions: The feature learning facilitated by the proposed CNN configuration markedly enhanced the accuracy of individual-level healthcare cost prediction. The CNN outperformed methods that seek a predefined set of pattern shapes, as it can extract a diverse array of patterns. The temporal patterns derived from medical, visit, and cost data significantly bolstered prediction performance. Hyper-parameter tuning revealed that three-month data patterns yielded the highest prediction accuracy. The study demonstrated that images derived from patient multivariate time series data are distinct from conventional images, necessitating specialized CNN architecture designs."}
{"text": "Integrating nonlinearity is crucial for forecasting the future states of dynamical systems, their reactions to disturbances, and the causal relationships that govern them. Yet, conventional methods for identifying causality and impulse responses, including Vector Autoregression (VAR), often fall short due to their reliance on linear assumptions, thereby overlooking intricate dynamics. This study introduces a novel approach, the Vector Autoencoder Nonlinear Autoregression Neural Network (VANAR), designed to autonomously extract features from time series data and estimate functional forms. The performance of VANAR is assessed through three dimensions: forecast accuracy, causality detection among variables, and impulse response modeling under external shocks. These evaluations were conducted on both a simulated nonlinear chaotic system and an empirical dataset comprising Philippine macroeconomic indicators. The findings reveal that VANAR outperforms VAR significantly in terms of forecasting and causality identification. Notably, VANAR demonstrates superior accuracy compared to advanced models like SARIMA and TBATS. However, in the impulse response test, neither model could accurately predict the trajectories of the nonlinear chaotic system post-shock. VANAR's robustness is highlighted by its effectiveness across diverse scenarios, encompassing chaotic, noisy, data-scarce conditions, and macroeconomic contexts."}
{"text": "This work proposes innovative generator designs for Boundary Equilibrium Generative Adversarial Networks, inspired by the principles of Learning from Simulated and Unsupervised Images through Adversarial Training. The proposed approach eliminates the reliance on a noise-driven latent space, instead, the generator functions predominantly as a refiner, enhancing synthetic images to achieve a photo-realistic quality. By removing the need for noise injection and adopting an image-centric approach, the method addresses the complexities associated with the latent space. The novel generator architecture is characterized by its simplicity and flexibility, enabling a controlled balance between the refinement of input images and the expressiveness of the output. Unlike conventional techniques, this architecture does not necessitate matched or unmatched pairs of real and synthetic images for training. Instead, a modest collection of real images is sufficient for the training process."}
{"text": "The autonomous acquisition of skills by robots holds the promise of enabling them to develop extensive behavioral capabilities without the need for extensive human engineering. However, current robotic skill learning techniques often involve compromises to facilitate practical learning in real-world settings, such as the use of handcrafted policy or value function representations, reliance on human demonstrations, the need for a controlled training environment, or the acceptance of prolonged training durations. This paper introduces a novel reinforcement learning algorithm designed for the acquisition of manipulation skills, which can train versatile neural network policies with minimal human intervention, while also ensuring rapid and efficient learning in stochastic environments. Our method extends the guided policy search (GPS) algorithm, which reformulates the reinforcement learning challenge as supervised learning from a computational instructor, without the need for human demonstrations. Unlike previous GPS approaches that necessitate a fixed set of initial states for system resets after each episode, our algorithm can accommodate randomized initial states, making it suitable for environments where deterministic resets are unfeasible. We evaluate our technique against established policy search methods in simulations, demonstrating its ability to train high-dimensional neural network policies with comparable sample efficiency to prior GPS methods, and we also provide empirical results from experiments conducted on a PR2 robotic manipulator."}
{"text": "A technique for hierarchical image segmentation is introduced, which involves constructing a disaffinity graph on the image, followed by its over-segmentation into watershed basins. Subsequently, a novel graph is defined on these basins, and a modified, size-dependent single linkage clustering approach is employed to merge the basins. The quasi-linear computational time of this method renders it particularly apt for the segmentation of extensive images. The efficacy of the method is demonstrated on the intricate task of segmenting three-dimensional electron microscopic images of the brain."}
{"text": "This article delves into the outlier arm detection challenge within the context of multi-armed bandit scenarios, a critical issue with significant implications in sectors such as finance, healthcare, and online advertising. The objective here is to pinpoint arms that exhibit expected rewards markedly different from the majority. Unlike previous research, our focus is on generic outliers, which may have higher, lower, or intermediate expected rewards relative to normal arms. We begin by defining these generic outliers rigorously. Subsequently, we introduce GOLD, a novel algorithm designed to detect such outliers. GOLD constructs a dynamic neighborhood graph using upper confidence bounds to discern outlier behavior amidst normal arms. A comprehensive analysis of its performance is also provided. Empirical evaluations on a mix of synthetic and real-world datasets demonstrate that GOLD achieves an accuracy of 98% while reducing exploration costs by an average of 83% compared to leading methods."}
{"text": "A new approach is introduced for the simultaneous acquisition of a 3D face parametric model and 3D face reconstruction, utilizing data from various origins. Conventional techniques typically focus on learning 3D face modeling from a single type of source, be it scanned data or images captured in natural settings. While 3D scanned data offer precise details of facial geometry, the acquisition process is costly and results in datasets with a limited number of individuals. Conversely, images of faces in-the-wild are abundant and easily accessible, but they lack explicit geometric information. This paper outlines a methodology for developing a comprehensive face model by integrating data from multiple sources. In addition to scanned face data and in-the-wild images, a substantial collection of RGB-D images, obtained using an iPhone X, is employed to reconcile the differences between these two types of sources. The empirical findings confirm that leveraging a broader range of training data leads to the creation of a more robust face model."}
{"text": "Differentiable Neural Architecture Search (NAS) stands out among NAS methodologies due to its efficient and straightforward approach, achieved by concurrently refining model weights and architecture parameters within a shared-weight supernet through gradient-based techniques. Upon concluding the search phase, the operations with the highest architecture parameter values are conventionally chosen to constitute the final architecture, under the presupposition that these parameter values are indicative of operation efficacy. Despite the extensive discourse on the supernet's optimization, the architecture selection mechanism has been relatively overlooked. Our study offers both empirical and theoretical insights, demonstrating that the size of architecture parameters does not unequivocally denote the operational contribution to the supernet's overall performance. To address this, we introduce a perturbation-based architecture selection method that quantifies each operation's impact on the supernet directly. Through reevaluation of various differentiable NAS techniques using our proposed selection strategy, we consistently derive architectures of notably enhanced quality from the underlying supernets. Moreover, our findings reveal that the proposed method can substantially mitigate several shortcomings of DARTS, suggesting that the suboptimal generalization observed in DARTS is largely due to the inadequacies of magnitude-based architecture selection, rather than solely the supernet's optimization process."}
{"text": "Given the pivotal role of edge detection in image processing for object identification, a thorough comprehension of edge detection algorithms is indispensable. This is because edges delineate the contours of objects, serving as the demarcation between an object and its background, and also highlighting the boundaries between intersecting objects. Consequently, precise identification of edges in an image facilitates the localization of all objects, enabling the measurement of fundamental attributes such as area, perimeter, and form. Given that computer vision tasks encompass the recognition and categorization of objects within images, edge detection emerges as a critical instrument. To evaluate their efficacy, we subjected two distinct edge detection techniques to a series of tests under diverse conditions, aiming to ascertain which method proved superior under varying circumstances."}
{"text": "Traditional causal discovery techniques necessitate the construction of a distinct model for each new causal graph encountered, disregarding the shared insights among different datasets, such as the underlying dynamics that govern causal interactions. To address this limitation, we introduce the Amortized Causal Discovery framework, which capitalizes on these shared dynamics to enhance the inference of causal relationships from time-series data. This framework enables the development of a single, generalized model capable of deducing causal links across various causal graphs, thereby harnessing the common information effectively. Through empirical validation using a variational model, we substantiate that this methodology significantly boosts the accuracy of causal discovery. Additionally, we illustrate its adaptability in handling scenarios with hidden confounders, showcasing its robustness and versatility."}
{"text": "Neural attention (NA) has emerged as a critical element in sequence-to-sequence models, demonstrating superior performance in complex tasks such as abstractive document summarization (ADS) and video captioning (VC). NA mechanisms effectively compute context vectors through weighted aggregations of the input sequence encodings, dynamically selected over extended temporal dimensions. Drawing from advancements in amortized variational inference (AVI), this study explores the treatment of context vectors produced by soft-attention (SA) models as latent variables, with their posterior distributions approximated by finite mixture models using AVI. It is hypothesized that this novel approach could enhance the generalization ability of models, mirroring the successes of AVI in deep learning contexts. To validate our methodology, we implement and empirically test it on demanding ADS, VC, and machine translation (MT) datasets, showcasing its superior efficacy compared to leading alternatives."}
{"text": "An optimal measurement strategy is the most effective approach to acquiring knowledge about an uncharted state. We present a foundational derivation of a versatile dynamic programming algorithm designed to generate a series of insightful measurements by iteratively maximizing the entropy of potential measurement results. This algorithm empowers autonomous entities, such as agents or robots, to determine the most advantageous location for the next measurement, thereby charting a course that aligns with an optimal series of informative observations. Its applicability spans continuous and discrete states and controls, accommodating both stochastic and deterministic agent dynamics, including scenarios modeled by Markov decision processes. Leveraging contemporary advancements in approximate dynamic programming and reinforcement learning, such as online techniques like rollout and Monte Carlo tree search, enables real-time resolution of the measurement challenge by the agent or robot. The resultant near-optimal strategies encompass foresighted paths and measurement sequences that often surpass, sometimes significantly, conventional greedy tactics, such as the maximization of entropy for individual measurement outcomes. This superiority is exemplified in a global search scenario, where online planning augmented with extended local search is shown to halve the measurement count required for the search."}
{"text": "The primary objective of this study is to design and apply an automated anomaly detection algorithm tailored for meteorological time-series data. In pursuit of this aim, we propose a method for creating an ensemble of anomaly detectors, complemented by an adaptive threshold selection technique that utilizes artificially generated anomalies. The effectiveness of our suggested approach is showcased through its successful integration into the \"Minimax-94\" road weather information system."}
{"text": "The focus of our investigation is the challenge of Salient Object Subitizing, which involves estimating the presence and quantity of prominent objects in a visual scene through the use of comprehensive visual cues. This endeavor is motivated by humans' innate capability to swiftly and precisely discern the count of objects within the subitizing range, typically encompassing one to four items. In pursuit of this objective, we introduce a comprehensive dataset of approximately 14,000 everyday images, meticulously annotated through an online crowdsourcing platform, designed to facilitate the study of salient object subitizing. Our findings reveal that an end-to-end trained Convolutional Neural Network (CNN) can attain prediction accuracies that are on par with human performance when identifying images that contain either zero or a single salient object. Furthermore, our model exhibits substantially higher-than-chance performance in scenarios involving multiple salient objects, a feat accomplished without the necessity for any explicit localization procedures. To enhance the training of the CNN for subitizing tasks, we propose an innovative approach that incorporates synthetic images. Through empirical evaluations, we showcase the precision and versatility of our CNN model in subitizing tasks, as well as its potential applications in the fields of salient object detection and image retrieval."}
{"text": "In this work, we investigate a simplified dynamic assortment planning issue that unfolds over a finite selling season of duration T. At every time interval, a seller presents an incoming customer with a selection of substitutable goods, from which the customer selects a purchase based on a discrete choice model. The seller's objective is to optimize expected revenue, which is equivalent to minimizing the worst-case expected regret. A significant hurdle in this scenario is the seller's lack of knowledge about the products' utilities, which must be learned over time. Despite growing interest in dynamic assortment planning within revenue management, the majority of prior research has focused on the multinomial logit (MNL) choice models. This paper extends the analysis to the nested logit model, a more comprehensive choice model that captures hierarchical decision-making and is recognized as \"the most widely used member of the GEV (generalized extreme value) family.\" By capitalizing on the revenue-ordered structure of the optimal assortment within each nest, we introduce a novel upper confidence bound (UCB) policy that employs an aggregated estimation method. This policy concurrently learns about customer preferences and dynamically decides on assortments based on the current understanding. It achieves an accumulated regret of \\tilde(), where M represents the number of nests and N is the number of products per nest. We also offer a lower bound of \\Omega(), which underscores the near optimality of our upper bound when T significantly exceeds M and N. For scenarios with a large number of items per nest (N), we propose a discretization heuristic to enhance the algorithm's performance. Empirical results are provided to showcase the effectiveness of our proposed methodologies."}
{"text": "The focus of our work is on estimating conditional average treatment effects (CATEs) in scenarios where treatments exhibit a graph-structured nature, such as in the case of drugs represented by molecular graphs. Under a mild assumption regarding the effect, we introduce a plug-in estimator that simplifies the CATE estimation process by breaking it down into distinct, more manageable optimization tasks. This novel estimator boasts two key advantages: first, it effectively identifies the causal estimands, thereby mitigating regularization bias; and second, it offers flexibility by enabling the integration of any chosen models for learning purposes. Through empirical evaluations involving small-world and molecular graphs, we demonstrate the superiority of our method over existing approaches, showcasing its resilience against diverse selection biases. The implementation of our method is available online."}
{"text": "In the realm of action recognition, current studies often regard activities as indivisible events within video content. The recent trend of decomposing actions into their constituent atomic-actions has proven advantageous for enhancing the comprehension of actions, thanks to the availability of datasets annotated with these details, which facilitate the learning of representations that encapsulate this granularity. Despite this progress, there is a notable gap in the literature regarding the exploration of action composition across multiple perspectives and data modalities for the purpose of representation learning. To address this deficiency and stimulate further research, we present Home Action Genome (HOMAGE), a comprehensive multi-view action dataset that encompasses various data modalities and viewpoints, enriched with hierarchical activity and atomic action annotations, as well as detailed scene composition labels. To capitalize on the multi-modal and multi-view richness of HOMAGE, we introduce Cooperative Compositional Action Understanding (CCAU), a novel cooperative learning framework designed for hierarchical action recognition that is sensitive to the compositional nature of actions. CCAU exhibits superior performance across all data modalities. Moreover, we showcase the effectiveness of co-learning compositional elements in few-shot action recognition, achieving a remarkable 28.6% mAP using merely a single example."}
{"text": "The challenge of reconstructing seismic data with absent traces has persisted in the field of seismic data processing. Recently, rank reduction techniques have gained popularity as a solution, but they necessitate the prior knowledge of the seismic data's rank, which is typically unknown for real-world data. Determining this rank often involves a time-consuming manual adjustment process, leading to only an approximate value. Deep learning-based methods, while promising, demand extensive datasets for training, which are often unattainable due to practical limitations such as physical or financial constraints. In response to these challenges, we have devised a new approach that leverages unsupervised learning, specifically the inherent capabilities of the U-net architecture, a type of convolutional neural network, without the need for training datasets. This method requires only a single undersampled seismic dataset, enabling the network to autonomously exploit the deep seismic prior of the input data, thereby simplifying the reconstruction process. Notably, this technique is applicable to both irregular and regular seismic data. To evaluate the efficacy of our proposed algorithm, termed the DSPRecon algorithm, we conducted tests on synthetic and field data, comparing its performance against the singular spectrum analysis (SSA) method for irregular data and the de-aliased Cadzow method for regular data. The results demonstrated superior reconstruction capabilities of our method over SSA and Cadzow, as evidenced by higher recovered signal-to-noise ratios (SNRs). Specifically, the DSPRecon algorithm achieved SNRs of 32.68 dB and 35.91 dB, whereas the SSA and Cadzow methods reached 19.11 dB and 15.32 dB, respectively."}
{"text": "The Bokeh effect, characterized by sharp foregrounds and blurred backgrounds, is a popular technique in photography often achieved using Single Lens Reflex cameras with a shallow depth-of-field. Modern smartphones, equipped with dual rear cameras or advanced auto-focus systems, can also produce Bokeh images. However, for devices with a single rear camera lacking sophisticated auto-focus capabilities, software solutions are necessary to simulate this effect, which can also be applied retrospectively to existing photographs. This paper introduces an end-to-end deep learning framework designed to synthesize high-quality Bokeh effects from images. By combining the original image with various smoothed versions, a monocular depth estimation network facilitates the creation of the desired Bokeh effect. The proposed method is evaluated against a baseline approach based on saliency detection and several other techniques from the AIM 2019 Challenge on Bokeh Effect Synthesis. Comprehensive experiments are conducted to dissect the various components of the algorithm. Notably, the network is lightweight, processing an HD image in just 0.03 seconds, and it secured the second position in the AIM 2019 Bokeh effect challenge-Perceptual Track."}
{"text": "Domain Adaptation (DA) methodologies have witnessed substantial advancements in enhancing performance across various machine learning and computer vision applications, encompassing classification, detection, and segmentation. Nevertheless, the application of DA techniques to 3D point cloud data remains relatively unexplored. The intricacy of point cloud data, characterized by its rich spatial geometric attributes and the significance of regional geometric structures in defining object semantics, poses unique challenges. Traditional DA approaches, which primarily focus on global feature alignment while neglecting local geometric details, are ill-suited for the alignment of 3D domains. In this work, we introduce PointDAN, a pioneering 3D Domain Adaptation Network tailored for point cloud data. PointDAN innovatively aligns both global and local features at multiple levels. To address local alignment, we devise a Self-Adaptive (SA) node module, equipped with a modulated receptive field, to capture distinctive local structures for domain alignment. Complementing this, a node-attention module is integrated to quantify the interrelations among SA nodes across different objects and domains, facilitating the representation of features at varying scales. For global alignment, an adversarial-training mechanism is employed to learn and harmonize global features across domains. Given the absence of a standardized evaluation framework for 3D point cloud DA, we construct a comprehensive benchmark, PointDA-10, derived from three prominent 3D object/scene datasets: ModelNet, ShapeNet, and ScanNet. This benchmark is designed for cross-domain 3D object classification. Extensive experimental evaluations on PointDA-10 substantiate the superior performance of our proposed model in comparison to leading general-purpose DA techniques."}
{"text": "Visual attention mechanisms are pivotal in neural network models for computer vision tasks, as they concentrate on specific objects or regions within images to extract pertinent features and construct robust representations. Recent advancements have introduced continuous-domain attention models as an alternative to discrete ones, capitalizing on the inherent continuity of images. However, these models often represent attention using simple unimodal densities, such as Gaussians, which may not adequately address images with complexly shaped or disjointed regions of interest. This paper proposes a novel continuous attention mechanism that generates multimodal densities, realized through mixtures of Gaussians. We employ the Expectation-Maximization (EM) algorithm to cluster significant image areas and utilize a description length penalty to determine the optimal number of mixture components. Our approach decomposes the attention density into a linear combination of unimodal mechanisms, facilitating the computation of closed-form Jacobians for the backpropagation process. Experimental results on the VQA-v2 dataset for visual question answering demonstrate competitive performance and a more human-like selection of attention regions, as evidenced by the VQA-HAT evaluation. Illustrative examples highlight the enhanced interpretability of multimodal attention maps compared to unimodal ones, showcasing the model's capability to autonomously distinguish objects from backgrounds in intricate scenes."}
{"text": "A new approach for 3D face recognition is introduced, combining a deep convolutional neural network (DCNN) with a 3D augmentation method. The effectiveness of 2D face recognition has been notably enhanced through the exploitation of deep neural networks' modeling capabilities and the availability of extensive labeled datasets. However, the development of discriminative deep features for 3D face recognition remains challenging due to the scarcity of large-scale 3D face databases. This study demonstrates that transfer learning, by fine-tuning a CNN pre-trained on 2D face images with a limited set of 3D facial scans, can be successfully applied to 3D face recognition. Additionally, we present a 3D face augmentation strategy that generates various facial expressions from a single 3D face scan. Our method achieves superior recognition performance on the Bosphorus, BU-3DFE, and 3D-TEC datasets, without relying on manually engineered features. Furthermore, our deep features for 3D identification exhibit good scalability for extensive databases."}
{"text": "In the realm of deep learning-driven automatic colorization, current models exhibit limitations, particularly in few-shot learning scenarios, due to their reliance on extensive training datasets. To address this challenge, we introduce MemoPainter, a pioneering memory-enhanced colorization framework designed to generate high-quality colorizations using minimal data. Specifically, our model excels in recognizing and colorizing infrequent patterns effectively. Additionally, we devise a new threshold triplet loss function, facilitating the unsupervised training of memory networks without the dependency on class labels. Our experimental results demonstrate that MemoPainter outperforms existing methods in terms of colorization quality, particularly in few-shot and one-shot learning tasks."}
{"text": "Although Generative Adversarial Networks (GANs) have proven their efficacy in generating realistic images, their potential for applications beyond image synthesis remains largely uncharted territory. A fundamental question arises: do GANs, in their endeavor to replicate objects, inadvertently learn the intrinsic structural components of these objects? This study aims to explore this hypothesis by introducing a straightforward yet potent methodology that harnesses GANs for semantic part segmentation, requiring merely a single labeled instance in conjunction with an unlabeled dataset. The crux of our approach lies in utilizing a pre-trained GAN to derive pixel-level representations from the input image, which are subsequently employed as feature vectors for a segmentation network. Our empirical findings reveal that the representations extracted from GANs are inherently discriminative, yielding impressive outcomes that rival those obtained from supervised baselines trained on a substantially larger labeled dataset. We posit that this innovative reapplication of GANs heralds a novel paradigm in unsupervised representation learning, with potential implications for a myriad of other tasks. Additional findings can be accessed at https://repurposegans.github.io/."}
{"text": "A straightforward and expedient algorithm for hyperparameter optimization is presented, drawing inspiration from methodologies utilized in Boolean function analysis. This algorithm is particularly effective in high-dimensional scenarios, such as the training of neural networks with a vast array of hyperparameters. Employing an iterative approach that harnesses compressed sensing techniques for orthogonal polynomials, the algorithm necessitates merely uniform sampling of hyperparameters, facilitating its parallel implementation. Experimental evaluations on the Cifar-10 dataset for deep neural network training demonstrate that our algorithm surpasses state-of-the-art tools, including Hyperband and Spearmint, in identifying superior solutions, occasionally outperforming manual tuning. Regarding total execution time, encompassing the time for sampling various hyperparameter configurations and supplementary computational overhead, our method is at least ten times more efficient than Hyperband and Bayesian Optimization, and it outperforms Random Search by a factor of eight. Moreover, our technique is accompanied by theoretical assurances and introduces the first advancements in the sample complexity of learning decision trees in more than twenty years. Specifically, we achieve the first quasi-polynomial time algorithm for learning noisy decision trees with a polynomial sample complexity."}
{"text": "A novel measure, termed Layer Saturation, is introduced to evaluate the learned representations within neural network layers. This metric is defined as the ratio of eigenvalues required to account for 99% of the variance in the latent representations. Grounded in spectral analysis, Layer Saturation offers an efficient computation method, enabling real-time monitoring of representation dynamics during the training phase. The potential future uses of this metric are explored by detailing its characteristics across various neural architectures and tasks. Additionally, it is demonstrated that Layer Saturation is indicative of neural network generalization and predictive capabilities."}
{"text": "In recent times, a considerable body of research has focused on the critical role of situational awareness in the realm of connected and autonomous vehicles (CAVs), given its direct impact on driver safety. Cooperative mechanisms, leveraging high-speed wireless vehicular networks, have emerged as a promising solution to enhance situational awareness, addressing issues like occlusion and sensor range constraints. Yet, the network's capacity to handle the volume of shared information remains a limiting factor. Building on our prior work that introduced the concept of feature sharing to balance computational and communication demands, this study advances the field by proposing a flexible adaptation to communication channel capacity and a decentralized data alignment technique for cooperative object detection. These enhancements aim to optimize cooperative object detection performance. The efficacy of our proposed framework is substantiated through experiments conducted on the Volony dataset, demonstrating superior average precision compared to our earlier cooperative object detection method, FS-COD."}
{"text": "The potential of deep structured output learning is notably evident in applications such as semantic image segmentation. In this work, we introduce an innovative and efficient approach to learning deep structured models, specifically demonstrating how Convolutional Neural Networks (CNNs) can be utilized to approximate the messages involved in the message passing inference process for structured prediction with Conditional Random Fields (CRFs). By employing CNNs as message estimators, we eliminate the requirement for learning or evaluating potential functions, which are typically necessary for message computation. This results in a substantial efficiency gain during the learning phase, as structured learning for CRFs with CNN potentials would otherwise necessitate costly inference for each stochastic gradient iteration. Notably, the output dimension of the network for message estimation matches the number of classes, diverging from the exponential output size of general CNN potential functions in CRFs, which is contingent on the order of the potentials. This discrepancy leads to a reduction in network parameters, making the CNN message learning approach more scalable, particularly in scenarios with a substantial number of classes. To showcase the efficacy of our method, we apply it to semantic image segmentation on the PASCAL VOC 2012 dataset, achieving an intersection-over-union score of 73.4 on the test set. This score represents the highest reported result for methods that solely utilize the VOC training images, underscoring the superior performance and practical value of our CNN message learning technique."}
{"text": "In this work, we introduce EA-ConvNets, an innovative end-to-end deep learning architecture designed for the early classification of time series data. Unlike conventional approaches that rely on predefined, often manually engineered features, our framework is capable of simultaneous feature learning and dynamic truncation. This dual capability enables the architecture to identify the critical characteristics within each time series and concentrate on its early segments, leading to highly accurate early predictions. Our method surpasses state-of-the-art early time series classification techniques and matches the performance of leading time series classification algorithms that operate on complete data. To our knowledge, EA-ConvNets is the pioneering framework to employ data-driven deep feature learning for early time series classification. Through a series of extensive experiments on benchmark datasets, we substantiate that our approach outperforms existing early classification methods significantly in terms of prediction accuracy. Moreover, the deep shapelets-based features learned by our model are not only highly accurate but also offer enhanced interpretability, providing deeper insights into the intrinsic properties of time series data."}
{"text": "The investigation of instance-based interpretation techniques, particularly their role in elucidating the prediction mechanisms of black-box neural networks, has garnered significant attention in the realm of supervised learning. Yet, their application and understanding within unsupervised learning paradigms are less explored. This study delves into the application of influence functions [20], a prominent instance-based interpretation approach, to variational auto-encoders (VAE), a category of deep generative models. We precisely define the counterfactual inquiry addressed by influence functions in this context and, through rigorous theoretical examination, explore their insights into the influence of training instances on conventional unsupervised learning algorithms. Building on the work of Pruthi et al. [28], we introduce VAE-TracIn, a method that is both computationally viable and theoretically robust, specifically designed for VAEs. Our assessment of VAE-TracIn is comprehensive, encompassing both quantitative and qualitative analyses across a variety of real-world datasets."}
{"text": "Deep generative models have excelled in handling continuous data, yet the challenge of representing discrete structures, such as computer programs and molecular configurations, with formal grammars and semantics persists. The generation of data that is both syntactically and semantically accurate remains a largely unresolved issue. Drawing inspiration from compiler theory, where syntax and semantics are verified through syntax-directed translation (SDT), we introduce a new model, the syntax-directed variational autoencoder (SD-VAE), which incorporates stochastic lazy attributes. This methodology transforms the post-hoc SDT validation into real-time guidance for the decoder, imposing constraints during the generation process. Unlike existing methods, our approach ensures that the output is not only syntactically correct but also semantically meaningful by enforcing restrictions on the output space. We assess the efficacy of the proposed model through applications in programming language and molecular domains, focusing on reconstruction and program/molecule optimization tasks. The outcomes highlight the model's superior performance in integrating syntactic and semantic constraints within discrete generative models, outperforming contemporary state-of-the-art techniques."}
{"text": "This thesis revisits the continuous optimization framework known as NOTEARS for Bayesian network learning, presenting a fresh perspective on its methodology. Initially, the work broadens the algebraic definitions of acyclicity to encompass a broader category of matrix polynomials. Subsequently, the analysis zeroes in on a scenario where each edge is associated with a single parameter, revealing that the Karush-Kuhn-Tucker (KKT) optimality criteria for the NOTEARS model are unattainable, except in a simplistic scenario. This insight elucidates the operational characteristics of the algorithm. The thesis then proceeds to derive the KKT conditions for an alternative, equivalent formulation, demonstrating their necessity and linking them to the explicit requirement for the absence of specific edges in the graph structure. When the scoring function exhibits convexity, these KKT conditions not only are necessary but also sufficient for achieving local minimality, despite the non-convex nature of the constraints. Motivated by these findings, a post-processing algorithm based on local search is introduced, which is shown to significantly enhance the structural Hamming distance of all evaluated algorithms, often by a factor of at least two. Remarkably, certain configurations augmented with local search outperform the original NOTEARS in terms of both accuracy and computational efficiency."}
{"text": "Braille has significantly enhanced the reading and writing capabilities of the visually impaired community. However, it has inadvertently created a communication barrier due to the lack of Braille literacy among the general population. This has spurred research into Optical Braille Recognition (OBR) methods aimed at converting Braille documents into readable text. The core objective of this study is to bridge the communication divide in educational settings by facilitating the translation of personal documents belonging to visually challenged students. This is achieved through the development of a cost-effective and efficient method that leverages a smartphone camera to digitize Braille documents. The proposed technique employs a dot detection algorithm based on the Hough transform, which is robust against skew, noise, and other potential distortions. Subsequently, the identified dots are grouped into Braille cells using a distance-based clustering method. Following this, the standard physical attributes of each Braille cell are determined for feature extraction and classification into corresponding natural language characters. The thorough assessment of this approach on a dataset comprising 54 Braille scripts has resulted in an accuracy rate of 98.71%."}
{"text": "In addressing the challenge of relocalization within extensive point clouds, we introduce a pioneering methodology that consolidates global place recognition with local 6DoF pose refinement. This is achieved through the development of a Siamese network, specifically tailored to learn 3D local feature detection and description directly from raw 3D data. The network incorporates FlexConv and Squeeze-and-Excitation (SE) mechanisms to ensure the local descriptor captures comprehensive geometric information and channel-wise dependencies. The discriminativeness of the local descriptors for keypoint detection is predicted in an unsupervised fashion. An efficient attention mechanism is then employed to aggregate these descriptors into a global representation, enabling the inference of both local and global 3D descriptors within a single forward pass. Our experimental evaluations on diverse benchmarks showcase competitive outcomes for both global point cloud retrieval and local point cloud registration, outperforming existing state-of-the-art methods. To underscore the versatility and resilience of our 3D keypoints, we exhibit favorable performance on point clouds derived from a visual SLAM system, without the need for fine-tuning. Access to the code and supplementary materials can be found at https://vision.in.tum.de/research/vslam/dh3d."}
{"text": "This document introduces GeoCLR, a georeferenced contrastive learning technique for visual representation, designed to optimize the training of Convolutional Neural Networks (CNNs) in an efficient manner. By utilizing georeferenced data, GeoCLR creates pairs of images from proximate locations, contrasting them with pairs from distant sites, under the premise that images captured in close proximity are more likely to exhibit similar visual characteristics. This assumption is particularly relevant in seafloor robotic imaging, where image footprints are confined to a few meters in edge length and are taken with overlap along the vehicle's path, while seafloor habitats and substrates have significantly larger patch sizes. A notable feature of GeoCLR is its self-supervised nature, eliminating the need for human intervention in the training of CNNs. Computationally, the method is efficient, enabling the generation of results between dives during multi-day Autonomous Underwater Vehicle (AUV) missions, utilizing computational resources typically available during oceanic field trials. Applied to habitat classification on a dataset comprising approximately 86,000 images collected by an AUV, GeoCLR showcases its ability to enhance human annotation efforts through the latent representations it generates. The semi-supervised framework, when compared to state-of-the-art transfer learning using the same CNN and a similar number of human annotations for training, yields an average improvement in classification accuracy of 11.8%."}
{"text": "The essence of video understanding hinges on a model's capacity to discern and learn the intricate relationship between the static elements of a scene and its dynamic aspects. Specifically, this involves forecasting the temporal evolution of a scene depicted in an image, as well as interpreting a video sequence in terms of its static visual content and the dynamic attributes that are not captured in the initial frame. This dual perspective implies a reciprocal transformation capability between the video domain and the amalgamation of static imagery and residual information. Unlike conventional stochastic image-to-video generation methods that produce random video continuations from a given image, our proposed model establishes a deterministic correspondence between the residual features and the video output, albeit with stochastic variability introduced through sampling. This is achieved through the application of a conditional invertible neural network (cINN), which facilitates the disentanglement of static and dynamic video components, thereby enabling controlled video generation. The efficacy of our methodology is substantiated through experiments on a variety of video datasets, showcasing superior performance in terms of both the quality and the diversity of the synthesized videos. Further details and results can be found on our project page at https://bit.ly/3t66bnU."}
{"text": "The burgeoning enthusiasm for transformers has illuminated their prospective dominance as versatile models for a plethora of computer vision applications, encompassing classification, detection, and segmentation. While the majority of these explorations have centered on discriminative models, our study ventures into the less-trodden domain of generative adversarial networks (GANs), utilizing transformers as the core architecture. Our objective is to pioneer a GAN framework that eschews convolutions entirely, relying solely on transformer-based designs. We introduce TransGAN, a foundational GAN structure featuring a transformer-driven generator that incrementally enhances feature resolution, complemented by a multi-scale discriminator adept at concurrently grasping semantic information and fine-grained textures. To address the memory constraints inherent in such architectures, we incorporate a novel grid self-attention module, enabling TransGAN to scale effectively for high-resolution image generation. Moreover, we devise a specialized training methodology that includes data augmentation, adjusted normalization, and relative position encoding, all aimed at mitigating the instability often encountered during TransGAN's training phase. Our most refined TransGAN variant delivers competitive outcomes, rivaling the performance of leading GANs that rely on convolutional architectures. Notably, TransGAN establishes a new benchmark with an inception score of 10.43 and a FID of 18.28 on the STL-10 dataset, surpassing StyleGAN-V2. In high-resolution generation tasks, such as those on CelebA-HQ and LSUN-Church at 256 x 256 dimensions, TransGAN consistently generates a wide array of visually compelling examples with exceptional clarity and intricate texture details. Furthermore, we undertake an in-depth analysis of transformer-based generative models, contrasting their operational dynamics with those of convolutional models through visualizations of the training process. The source code for TransGAN is accessible at https://github.com/VITA-Group/TransGAN."}
{"text": "In this work, we present SalsaNext, an advanced model for real-time, uncertainty-aware semantic segmentation of complete 3D LiDAR point clouds. As the successor to SalsaNet [1], which features an encoder-decoder structure with ResNet blocks in the encoder and upsampled residual block features in the decoder, SalsaNext introduces several enhancements. These include a novel context module, the substitution of ResNet encoder blocks with a progressive receptive field residual dilated convolution stack, and the incorporation of a pixel-shuffle layer in the decoder. Moreover, we transition from stride convolution to average pooling and implement central dropout. To optimize the Jaccard index directly, we combine the weighted cross-entropy loss with the Lovasz-Softmax loss [2]. Furthermore, we incorporate a Bayesian approach to estimate both epistemic and aleatoric uncertainties for each point in the cloud. Our comprehensive quantitative analysis on the Semantic-KITTI dataset [3] confirms that SalsaNext surpasses existing state-of-the-art semantic segmentation networks, securing the top position on the Semantic-KITTI leaderboard. The source code for SalsaNext is available at https://github.com/TiagoCortinhal/SalsaNext."}
{"text": "The critical management of blood sugar levels is a fundamental aspect in intensive care settings, yet devising personalized optimal strategies for glycemic control remains a complex endeavor due to the lack of relevant studies. This research endeavors to address this gap by employing data-driven methodologies to learn and propose individualized glycemic trajectories for critically ill patients suffering from sepsis, aiming to guide healthcare providers in setting effective target blood glucose levels. Utilizing a sparse autoencoder for patient state representation and a reinforcement learning approach through policy iteration, the study was able to derive an optimal policy from historical data. The expected return under this learned policy, calculated from documented glycemic trajectories, revealed a correlation between actual blood glucose values and 90-day mortality rates. This analysis suggests that implementing the learned optimal policy could potentially lower the projected 90-day mortality rate by 6.3%, moving from 31% to 24.7%. These findings underscore the potential of reinforcement learning, when coupled with accurate patient state encoding, to offer customized glycemic control strategies that could significantly enhance patient outcomes in septic conditions."}
{"text": "In the realm of reinforcement learning, real-world states are typically modeled using feature vectors. Yet, it is often the case that not every feature is relevant to the task at hand. To address this issue, we introduce Feature Selection Explore and Exploit (FS-EE), a novel algorithm designed to identify and utilize only the essential features during the learning process of a Factored Markov Decision Process. We demonstrate that, under reasonable conditions, the sample complexity of FS-EE is proportional to the in-degree of the dynamics associated with the critical features alone, rather than the in-degree of the entire feature set. This selective approach can lead to a significant improvement in sample complexity when the in-degree of the pertinent features is substantially lower than that of the complete feature set."}
{"text": "In autonomous systems, depth estimation from visual data is a critical capability, often achieved through single or multiple camera setups. However, obtaining dense depth information in a monocular configuration typically necessitates the use of costly LiDARs, such as those with 64 beams, or camera-only techniques that are plagued by scale uncertainty and infinite-depth challenges. This paper introduces a novel approach to estimate dense, metric depth by synergizing a monocular camera with a lightweight LiDAR, such as those with 4 beams, commonly found in contemporary automotive-grade mass-produced laser scanners. Drawing inspiration from recent advancements in self-supervised learning, we present LiDARTouch, a framework designed to infer dense depth maps from monocular images, leveraging sparse LiDAR \"touches\" as a guide, thereby eliminating the reliance on dense ground-truth depth. The minimal LiDAR input is utilized in three distinct ways: as an auxiliary input to the model, within a self-supervised LiDAR reconstruction loss function, and for the estimation of pose changes, a critical element in self-supervised depth estimation architectures. Our LiDARTouch framework sets a new benchmark for self-supervised depth estimation on the KITTI dataset, validating the effectiveness of integrating sparse LiDAR signals with visual cues. Furthermore, we illustrate that the incorporation of a few-beam LiDAR mitigates the scale ambiguity and infinite-depth issues characteristic of camera-only methods. Additionally, we showcase the adaptability of fully-supervised depth-completion techniques to a self-supervised setting with minimal LiDAR input."}
{"text": "A reinforcement learning approach is introduced to address the challenge where a soccer agent must navigate from the start to the end of a designated area while maintaining control of the ball, under the pressure of an opponent trying to intercept it. The opposing agent employs a fixed strategy, whereas the ball-dribbling agent learns to select the most effective action at every decision-making juncture. By identifying pertinent variables to characterize the state space and integrating domain expertise through the definition of high-level macro-actions, we detail the implementation of the reinforcement learning algorithm, augmented with CMAC for function approximation. Experimental outcomes reveal that, post-training, the dribbling agent succeeds in fulfilling its objective against a formidable adversary in approximately 58% of the trials."}
{"text": "The potential of remote photoplethysmography (rPPG) for measuring heart activity non-invasively is significant, particularly in remote healthcare settings. However, current end-to-end rPPG and heart rate (HR) measurement techniques from facial videos are susceptible to performance degradation in less controlled environments, such as those with head motion or poor lighting. This paper investigates the factors contributing to the suboptimal performance of existing end-to-end models under challenging conditions and introduces a robust baseline method, AutoHR, for remote HR measurement using neural architecture search (NAS). The proposed methodology comprises three key components: 1) a potent backbone architecture identified through NAS, featuring the innovative Temporal Difference Convolution (TDC) to discern intrinsic rPPG-sensitive cues across video frames; 2) a dual-domain loss function that integrates constraints from both temporal and frequency perspectives; and 3) enhanced spatio-temporal data augmentation techniques to facilitate more effective representation learning. Extensive evaluations on three standard datasets demonstrate the superiority of our approach in both intra- and cross-dataset testing scenarios."}
{"text": "The domain of reinforcement learning (RL), particularly the burgeoning subfield of deep reinforcement learning (DRL), is witnessing a surge in innovative contributions. Yet, several scientific and technical hurdles persist, including the abstraction of actions and the exploration of environments, which can be mitigated through the application of intrinsic motivation (IM). This paper offers an extensive review of IM's significance in DRL, systematically classifying various forms of intrinsic motivation and elucidating their respective merits and constraints in relation to the aforementioned challenges. Moreover, we delve into significant research inquiries that are either currently being explored or have yet to be addressed within the DRL field. Our review adopts a task-acquisition perspective, positing that overcoming these challenges could pave the way for a more comprehensive developmental framework capable of handling a broader spectrum of tasks. This framework is conceptualized as an assembly of components, featuring a RL algorithm and an IM module designed to condense information."}
{"text": "Aiming to enhance visual quality, video super-resolution (VSR) techniques reconstruct high-resolution (HR) video frames from their low-resolution (LR) counterparts, incorporating information from adjacent frames. The misalignment between the central LR frame and its neighboring frames, caused by motion, poses a significant challenge for temporal alignment in VSR. Traditionally, this alignment is achieved through optical flow-based warping, which aligns each supporting frame to the reference frame. However, the accuracy of optical flow estimation critically influences the quality of the warped frames, and any inaccuracies can introduce defects that are subsequently reflected in the reconstructed HR frame. To address this issue, we introduce a Temporal Deformable Alignment Network (TDAN) that performs feature-level alignment between the reference frame and each supporting frame, bypassing the need for optical flow computation. TDAN dynamically predicts the offsets for sampling convolution kernels using features from both the reference and supporting frames. These kernels are then applied to deform the supporting frames, aligning them with the reference frame. A reconstruction network, fed with the aligned frames and the reference frame, is employed to generate the HR video frame. Our experimental outcomes validate the superiority of the TDAN-based VSR model in producing high-quality video frames."}
{"text": "Recent advancements in deep learning for graph representation learning predominantly employ a neighborhood aggregation mechanism. This paper critically evaluates key characteristics of these models and introduces a novel approach to address their limitations. Specifically, the extent of \"neighborhood\" nodes contributing to a node's representation is heavily influenced by the graph's topology, mirroring the diffusion pattern of a random walk. To accommodate the diverse properties of local neighborhoods and enhance task performance, we investigate a design known as jumping knowledge (JK) networks. This architecture allows each node to selectively utilize representations from varying neighborhood ranges, thereby facilitating more nuanced, structure-sensitive embeddings. Through a series of experiments on social, bioinformatics, and citation networks, we showcase the superior performance of our model compared to existing state-of-the-art methods. Moreover, integrating the JK framework with prominent models such as Graph Convolutional Networks, GraphSAGE, and Graph Attention Networks leads to a consistent enhancement in their effectiveness."}
{"text": "This document serves as a concise summary of our contribution to the Recognizing Families In the Wild Data Challenge's fourth edition, which was held in tandem with the FG 2020 Forum. The field of automatic kinship recognition has piqued the interest of numerous researchers due to its broad applicability, yet it remains a daunting task given the scarcity of discernible features to ascertain familial relationships between facial pairs. Our study involved an in-depth examination of existing methodologies and the development of a novel approach. We experimented with various techniques, including those rooted in deep metric learning, to generate deep embedding features for each image. The determination of kinship was made through the calculation of Euclidean distances or by leveraging class-based methods. We discovered that certain strategies, such as increasing the number of negative samples and utilizing high-resolution images, significantly enhanced performance. Additionally, we introduced a symmetric network coupled with a binary classification technique, which proved to be our most effective solution across all tasks."}
{"text": "A novel viewpoint on video comprehension is introduced, framing the video recognition challenge as an image recognition problem. We demonstrate that, counterintuitively, a solitary image classifier can effectively handle video understanding tasks without the need for temporal modeling. Our methodology is straightforward and broadly applicable, involving the amalgamation of video frames into a composite 'super image' to train an image classifier for action recognition, mirroring the process of image classification. The feasibility of this concept is substantiated through impressive and encouraging outcomes on four widely recognized datasets: Kinetics400, Something-to-something (V2), MiT, and Jester, utilizing a cutting-edge vision transformer. Additionally, we conduct experiments with the widely-used ResNet image classifiers in the field of computer vision to further corroborate our proposition. Notably, the performance on Kinetics400 is on par with the leading CNN methods that rely on spatio-temporal modeling. Our source code and models will be accessible at https://github.com/IBM/sifar-pytorch."}
{"text": "Experimental observations have highlighted that the efficacy of distributed training utilizing stochastic gradient descent (SGD) is significantly influenced by the batch size and, in asynchronous implementations, by the gradient staleness. Notably, there is a point at which increasing the batch size or allowing larger delays no longer enhances the speedup. We have pinpointed a parameter, contingent on the data, that elucidates the saturation of speedup in both scenarios. Our extensive theoretical examination, encompassing strongly convex, convex, and non-convex settings, consolidates and extends previous research that typically concentrated on just one of these factors. Specifically, our methodology enables us to establish more favorable speedup outcomes under commonly assumed sparsity conditions. The insights gained from our study lead to practical, theory-backed recommendations for adjusting learning rates. We validate the accuracy of our findings and demonstrate pivotal observations through numerical experiments."}
{"text": "In recent years, the proliferation of camera networks has sparked a growing interest in the analysis of video data from multi-camera systems, particularly in scenarios where cameras do not share overlapping fields of view. This encompasses a range of tasks, including object detection, attribute recognition, and the tracking of vehicles or individuals across various camera feeds. Existing video management frameworks are typically tailored for closed datasets with a limited number of cameras, where the surveillance environment is relatively stable and well-understood. Moreover, these frameworks are often geared towards post-event analysis, with human operators guiding the process for forensic purposes. This work introduces a novel teamed classifier framework designed to handle the complexities of video analytics in large, heterogeneous camera networks under challenging conditions, such as varying scales, resolutions, occlusions, blurring, and orientations. Specifically, we focus on vehicle tracking and re-identification (re-id), implementing a zero-shot learning (ZSL) system capable of continuously tracking all vehicles in real-time. Our empirical results on the VeRi-776 and Cars196 datasets demonstrate the robustness of the teamed classifier framework in adversarial conditions, its adaptability to evolving video characteristics, such as new vehicle models and brands, and its superior real-time performance compared to traditional offline video analytics methods."}
{"text": "In contrast to most Siamese network-based trackers that track without updating their models and lack the ability to adaptively learn target-specific variations, our proposed approach, the Rotation-Scale Invariant Network (RSINet), is designed to overcome these limitations. Siamese-based trackers typically use axis-aligned bounding boxes to infer the new state of objects being tracked, which can introduce additional background noise and fail to accurately account for the rotation and scale changes of moving objects, potentially degrading tracking performance. To address this, RSINet integrates a target-distractor discrimination branch and a rotation-scale estimation branch, enabling the explicit learning of rotation and scale through a multi-task learning framework in an end-to-end fashion. Furthermore, the tracking model is optimized and updated in a controlled manner under spatio-temporal energy regulation, ensuring the model's stability, reliability, and high efficiency. Extensive evaluations on the OTB-100, VOT2018, and LaSOT benchmarks showcase that RSINet outperforms recent trackers in terms of tracking accuracy while maintaining real-time performance at approximately 45 frames per second."}
{"text": "The computation of the Slope Difference Distribution (SDD) for one-dimensional curves demonstrates robustness in identifying logical partitioning points and calculating clustering centers for each segment of the curve. Initially proposed for image segmentation, SDD has proven superior to conventional methods, as evidenced by our comparative analysis, which is accessible through Matlab Central. Given the resemblance between an object's contour and an image's histogram, SDD's application for feature detection from object contours is viable. In this paper, we define SDD features that constitute a sparse representation of the object's contour. By constructing a reference model based on these features, we employ model matching for real-time object recognition, yielding promising results. Specifically, SDD achieved perfect accuracy (100%) in gesture recognition for both the NUS and near-infrared datasets, and similarly, in object recognition for the Kimia 99 dataset."}
{"text": "The development of machine learning models utilizing Electroencephalogram (EEG) data collected in non-laboratory environments necessitates robust methodologies that can cope with data noise and the random absence of channels. This requirement is especially pronounced when dealing with sparse EEG configurations (1-6 channels), a common scenario in consumer or mobile EEG devices. Neither conventional machine learning algorithms nor deep neural networks, which are typically trained on pristine EEG data, are inherently designed or evaluated for resilience against data corruption, particularly in the context of randomly missing channels. Although strategies for handling missing channel data have been suggested, they are often impractical for sparse montages and scenarios with limited computational resources, such as wearable devices and smartphones. To address this challenge, we introduce dynamic spatial filtering (DSF), a modular multi-head attention mechanism that can be seamlessly integrated before the initial layer of a neural network. DSF is designed to manage missing EEG channels by learning to prioritize reliable channels and disregard compromised ones. We evaluated DSF on a public EEG dataset comprising approximately 4,000 recordings with simulated channel corruption, as well as on a proprietary dataset of around 100 at-home mobile EEG recordings with naturally occurring corruption. Our proposed solution matches the performance of baseline models in noise-free conditions, but significantly outperforms them by up to 29.4% in accuracy when substantial channel corruption is introduced. Additionally, the outputs of DSF are interpretable, enabling real-time monitoring of channel significance. This innovation holds promise for enhancing EEG analysis in demanding environments where channel corruption can severely impact the quality of brain signal readings."}
{"text": "Objective: Image classification stands as a cornerstone in the field of imaging artificial intelligence, yet the process of annotating images is laborious and time-intensive. Building on our recent success in utilizing reinforcement learning (RL) to accurately classify 2D MRI brain slices, this study advances image classification efficiency through two pivotal innovations: first, by automating the extraction of class labels from clinical reports, and second, by expanding our previous 2D classification methodology to encompass full 3D image volumes sourced from our institution. The methodology proceeds in two stages: initially, labels are automatically derived from reports using the SBERT natural language processing technique. Subsequently, these labels are employed alongside RL to train a Deep-Q Network (DQN) for the classification of 3D image volumes. \nMethodology: In the initial phase, SBERT was trained on a dataset of 90 radiology report impressions, and the trained model was then utilized to forecast class labels for the subsequent phase. The second phase involved the application of multi-step image classification, integrating Deep-Q learning with 3D convolutions and TD(0) Q learning. A set of 90 images was used for training, while a distinct set of 61 images was reserved for testing, with the labels predicted from patient reports by the SBERT model in the first phase. To establish a benchmark, a supervised deep learning classification network was also trained and tested on the identical sets of images using the same labels.\nFindings: In the first phase, the SBERT model, after being trained on the radiology report corpus, achieved a perfect accuracy of 100% in identifying both normal and metastasis-containing scans. In the second phase, while the supervised approach rapidly overfitted the training data and, as anticipated, exhibited subpar performance on the testing set (66% accuracy, marginally above random guessing), the reinforcement learning strategy attained a remarkable accuracy of 92%. The statistical significance of these results was confirmed, with a p-value of 3.1 x 10^-5."}
{"text": "This study introduces Impressions2Font (Imp2Font), an innovative approach to generating font images that embody specific impressions, often described through a variety of words. Imp2Font is an advanced adaptation of conditional generative adversarial networks (GANs), designed to take an unlimited number of impression words as input to condition the font image generation process. To achieve this, an impression embedding module, grounded in word embedding technology, transforms these impression words into a soft-constraint vector. The effectiveness of Imp2Font is substantiated through both qualitative and quantitative analyses, demonstrating its superior capability to produce high-quality font images, even when fed with multiple or previously untrained impression words, compared to existing methods."}
{"text": "Monocular depth estimation and semantic segmentation are pivotal components in the realm of scene interpretation. The synergy between these tasks has been explored in numerous studies, aiming to enhance performance through joint learning. Yet, a common shortcoming is the underutilization of semantic labels, which are often restricted to supervising segmentation predictions, thereby overlooking their potential to enrich context and structure. To address this limitation, we introduce a novel architecture, the Contextual Information Network (CI-Net), designed to fully harness the power of semantic labels. Our approach incorporates a self-attention mechanism within the encoder to produce an attention map, which is then refined by the guidance of an ideal map derived from semantic labels. This process imbues the network with contextual awareness, enabling it to leverage relevant features for precise predictions. Additionally, a feature sharing component is integrated to facilitate the deep integration of task-specific features, complemented by a consistency loss to ensure mutual guidance between the features. The efficacy of CI-Net is demonstrated through experiments on the NYU-Depth-v2 and SUN-RGBD datasets, where it significantly boosts the accuracy of both semantic segmentation and depth estimation tasks."}
{"text": "The advancement of deep learning techniques, particularly the growing computational capabilities, has led to increasingly lifelike outcomes in image colorization, making it increasingly challenging for humans to discern between genuine and artificially colored images. This study introduces a pioneering forensic technique to differentiate between natural images (NIs) and colorized images (CIs) utilizing a convolutional neural network (CNN). Our proposed method excels in achieving high accuracy in classification and is particularly adept at blind detection, where no training examples from an \"unknown\" colorization algorithm are provided during the testing phase, a scenario that tests the method's generalization capabilities. Initially, we develop a foundational network that outperforms existing methods in terms of classification accuracy and generalization in most instances. To further enhance these capabilities, we introduce a novel component that scrutinizes smaller segments of the extracted features, integrating it into the base network. This enhancement not only boosts classification accuracy but also significantly improves generalization across the board. In pursuit of optimizing blind detection performance, we propose an automated method to generate negative samples by linearly interpolating paired natural and colorized images. These samples are incrementally incorporated into the training dataset, and the network is retrained. Our experimental findings substantiate the method's ability to maintain a consistently high level of generalization when evaluated against a variety of cutting-edge colorization algorithms."}
{"text": "The World Health Organization (WHO) offers guidelines for controlling Particulate Matter (PM) levels, as elevated PM concentrations pose a significant risk to human health. To manage these levels effectively, a reliable method for measuring PM values is essential. The Tapered Element Oscillating Microbalance (TEOM)-based PM measuring sensors are favored for their cost-effectiveness over Beta Attenuation Monitor (BAM)-based sensors. Yet, TEOM-based sensors are more prone to malfunctions compared to their BAM-based counterparts. In this thesis, we refer to any form of malfunction as an anomaly, and our objective is to develop a method for detecting these anomalies to ensure the proper functioning of PM measuring sensors. To achieve this, we introduce a new framework, the Hypothesis Pruning Generative Adversarial Network (HP-GAN), designed specifically for anomaly detection. Through experimental comparisons with other anomaly detection architectures, we demonstrate the superior performance of our proposed method."}
{"text": "In this work, we introduce methodologies for training convolutional neural networks (CNNs) with binary weights and activations, resulting in models that are highly optimized for mobile devices with constrained power and computational resources. Unlike prior approaches that focus on value approximation by using discrete values to mimic floating-point data, often maintaining the original full-precision network architecture, we adopt a novel perspective termed \"structure approximation.\" This perspective suggests that custom architectures tailored for low-bit networks could potentially yield superior performance. To this end, we present a \"network decomposition\" technique, coined Group-Net, which involves partitioning the network into distinct groups. Each of these groups, originally in full precision, can be reconstructed through the aggregation of multiple binary branches that share similar characteristics. Additionally, we learn efficient inter-group connections to enhance the model's representational power. The versatility of Group-Net is further demonstrated through its application to various tasks, such as enhancing semantic segmentation accuracy by integrating contextual information into the binary framework, and pioneering the use of binary neural networks in object detection. Extensive experiments across classification, semantic segmentation, and object detection tasks showcase the superior efficacy of our proposed methods compared to existing quantized networks. Our techniques surpass previous state-of-the-art binary neural networks in both accuracy and computational efficiency."}
{"text": "The objective of dense video captioning is to pinpoint and narrate significant occurrences within unedited video clips. Traditionally, this challenge has been addressed by leveraging solely visual information, with the audio component often overlooked. Despite some recent efforts to integrate both visual and auditory data, these approaches either yield subpar outcomes or validate their significance on domain-specific datasets. In this contribution, we unveil the Bi-modal Transformer, an adaptation of the Transformer framework designed to process dual-modal inputs. Our model's efficacy is showcased on the dense video captioning task, utilizing both audio and visual data, although it is versatile enough to handle any two modalities in a sequence-to-sequence framework. Additionally, we illustrate that the pre-trained bi-modal encoder, embedded within the Bi-modal Transformer, can serve as a potent feature extractor for a rudimentary proposal generation mechanism. Our model's prowess is substantiated on the arduous ActivityNet Captions dataset, where it attains exceptional performance. The source code is accessible at v-iashin.github.io/bmt."}
{"text": "The remarkable progress in deep learning has significantly enhanced the performance of computer vision tasks, including object detection and segmentation. Nevertheless, in critical applications like autonomous driving, the consequences of inaccurate object predictions can be severe. Traditional object detection models, such as those in the YOLO family, tend to exhibit excessive confidence in their predictions and fail to account for uncertainty, particularly when dealing with out-of-distribution data. This thesis introduces a novel and efficient method for uncertainty quantification in object detection and segmentation tasks, utilizing Monte-Carlo DropBlock (MC-DropBlock) for inference. The proposed methodology integrates drop-block during both training and testing phases on the convolutional layers of deep learning models like YOLO, effectively transforming them into Bayesian convolutional neural networks capable of estimating epistemic uncertainty. Aleatoric uncertainty is quantified through a Gaussian likelihood. The efficacy of the MC-DropBlock approach in uncertainty modeling for object detection and segmentation is showcased through out-of-distribution experiments. The results indicate that MC-DropBlock enhances the generalization, calibration, and uncertainty handling of YOLO models in these critical tasks."}
{"text": "In the contemporary era, deep neural networks (DNNs) have attained remarkable achievements in various low-level vision tasks. Nonetheless, the leading-edge outcomes are predominantly realized by exceedingly deep architectures, encompassing tens of layers and tens of millions of parameters. To facilitate the deployment of DNNs on resource-constrained platforms, it is imperative to mitigate the compromise between performance and efficiency. This paper introduces a novel activation unit, specifically tailored for image restoration applications. Unlike ubiquitous per-pixel activation functions, such as ReLUs and sigmoids, our proposed unit executes a learnable nonlinear function with spatial interconnections. This feature empowers the network to discern significantly more intricate features, thereby necessitating a substantially reduced number of layers to attain equivalent performance. The efficacy of our units is substantiated through experimental evaluations with cutting-edge networks for denoising, de-raining, and super-resolution tasks, which are already deemed compact. Employing our methodology, we achieve a further diminution of these models by approximately 50%, without compromising performance."}
{"text": "In the realm of self-supervised learning, models are trained to tackle a pretext task on datasets without requiring human annotations, with the ultimate goal of transferring this model to a different domain or task. Traditionally, the most successful transfer strategy has been fine-tuning, which confines the user to employ the same model, or components thereof, for both the pretext and target tasks. This paper introduces a groundbreaking framework that addresses the limitations in the design and comparison of various tasks, models, and data domains within self-supervised learning. Specifically, our framework separates the architecture of the self-supervised model from the final, task-specific fine-tuned model. This separation enables us to: 1) quantitatively evaluate models that were previously incommensurable, including those based on handcrafted features; 2) demonstrate that more complex neural network models can derive superior representations from the same pretext task; 3) transfer knowledge acquired by a deep model to a less complex one, thereby enhancing its learning capabilities. Utilizing this framework, we have engineered a new self-supervised task that outperforms current state-of-the-art methods on standard benchmarks in PASCAL VOC 2007, ILSVRC12, and Places by a considerable margin. Notably, our learned features have reduced the mAP gap between models trained through self-supervised learning and those trained with supervised learning from 5.9% to 2.6% in object detection on PASCAL VOC 2007."}
{"text": "Traditionally, the design and experimentation of deep neural network architectures have been a labor-intensive endeavor, involving a protracted trial-and-error phase guided by human expertise. This approach is characterized by significant demands on time, specialized knowledge, and resources. In response to these challenges, we introduce an innovative algorithm aimed at automating the discovery of optimal hyperparameters for deep network architectures, thereby streamlining the design process. Our focus is specifically on the development of neural architectures tailored for medical image segmentation. The proposed methodology leverages policy gradient reinforcement learning, with the segmentation evaluation utility, specifically the dice index, serving as the reward function. We demonstrate the effectiveness of our approach through its reduced computational overhead when compared to leading medical image segmentation networks. Additionally, we introduce a densely connected encoder-decoder CNN as a robust baseline architecture, which we then enhance by applying our hyperparameter search algorithm to each of its layers. As a practical application, we train our system using cine cardiac MR images from the Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. Starting with a baseline segmentation architecture, the resultant network achieves state-of-the-art accuracy, without the need for manual trial-and-error architecture design or meticulous monitoring of hyperparameter adjustments."}
{"text": "Graph neural networks (GNNs) have garnered significant attention across various domains that utilize graph data, yet the absence of a standardized training framework impedes equitable evaluation among emerging methodologies, encompassing diverse model architectures and data enhancement strategies. To address this gap, we propose a replicable benchmark for node classification tasks, facilitating uniform application of training parameters across different models. This benchmark encompasses nine datasets, ranging from small to medium sizes and spanning multiple disciplines, alongside seven distinct models. For small-scale datasets, we devise a k-fold evaluation approach, and for all datasets, we establish a consistent model training protocol, thereby streamlining the experimental process for GNNs and promoting fair architectural comparisons. To explore the impact of input features on model performance, we employ node2vec and Laplacian eigenvectors for data augmentation, revealing the critical role of topological information in node classification. Our findings indicate that augmenting model depth does not universally enhance performance, except in cases where graphs are disconnected, as observed in the PATTERN and CLUSTER datasets. Notably, data augmentation proves highly beneficial, with node2vec in particular yielding significant enhancements in baseline performance."}
{"text": "The significant advancements in smart home technology, autonomous vehicles, healthcare, and robotics, coupled with stringent legal frameworks, have catalyzed a surge in academic interest towards explainable machine learning. Numerous scholars have devised methodologies to elucidate the decision-making processes of any opaque model in classification tasks. However, a limitation of these model-agnostic explainers is the generic approach to neighborhood generation, which may not ensure genuine proximity between the synthesized neighbors and the target instance. This research proposes a novel strategy for interpreting the choices made by neural networks on a localized level. The method actively integrates the neural network's structure into the creation of an instance's neighborhood, thereby securing the adjacency between the generated neighbors and the instance in question."}
{"text": "In contemporary robotics, LiDAR is widely utilized as the primary sensory input due to its geometric precision. Rolling shutter LiDARs, characterized by a rotating base that scans the environment with a series of lasers, are particularly prevalent. These devices emit data points in packets, each representing a segment of the 360-degree field of view. However, conventional perception algorithms introduce a latency by waiting for the complete sweep to be assembled before processing, typically 100ms for a 10Hz LiDAR. This delay results in outputs that do not accurately represent the current state of the environment, a significant issue for robotics applications that demand rapid response times to ensure safety-critical maneuvers can be promptly executed. To address this challenge, we introduce StrObe, an innovative method that significantly reduces latency by processing LiDAR packets in real-time, generating a continuous stream of detections without the need for a full sweep. StrObe leverages computations from prior packets to iteratively refine a latent spatial model of the scene, serving as a dynamic memory that updates with incoming data, thus achieving precise and low-latency perception. Our method's efficacy is substantiated through extensive testing on a substantial real-world dataset, where StrObe significantly outperforms existing techniques in latency-sensitive scenarios while maintaining comparable performance in traditional settings."}
{"text": "The proposed framework introduces a weakly supervised method for top-down saliency modeling, which generates probability maps highlighting target locations relevant to specific tasks, such as object detection, using only binary labels that denote the existence or absence of objects in images, as opposed to the fully supervised training that typically requires pixel-level object annotations. Initially, the framework calculates the probabilistic contribution of each image region to the confidence of a CNN-based image classifier through a backtracking strategy, thereby creating top-down saliency. Subsequently, from a collection of saliency maps generated by rapid bottom-up saliency methods for a given image, the most suitable map for the top-down task is chosen. This selected bottom-up saliency map is then merged with the top-down saliency map. Features with high combined saliency are utilized to train a linear SVM classifier for estimating feature saliency, which is subsequently integrated with the combined saliency and refined via multi-scale superpixel-averaging of the saliency map. The performance of the proposed weakly supervised top-down saliency approach is assessed on seven challenging datasets, demonstrating results that are competitive with those of fully supervised methods. Comparative quantitative analyses are conducted against 40 related approaches across four distinct applications."}
{"text": "The susceptibility of 2D object detection models to adversarial attacks remains a concern, despite their proficiency in processing clean images. Previous efforts to bolster the resilience of these models through adversarial training have often resulted in a substantial decline in average precision (AP) on unaltered images. This paper introduces a novel strategy that combines feature alignment at the intermediate layer to enhance both the AP on clean images and the robustness against adversarial attacks in object detection tasks. Building upon adversarial training, we introduce two feature alignment mechanisms: the Knowledge-Distilled Feature Alignment (KDFA) module and the Self-Supervised Feature Alignment (SSFA) module. These modules are designed to steer the network towards producing more potent features. To substantiate the efficacy of our proposed methodology, we have carried out comprehensive experiments on the PASCAL VOC and MS-COCO datasets. The source code for our experiments can be accessed at https://github.com/grispeut/Feature-Alignment.git."}
{"text": "The word2vec technique, introduced by Mikolov in 2013, is a prevalent method for word embedding in the field of natural language processing. Despite its widespread adoption and empirical success, a solid theoretical foundation for word2vec has remained elusive. Our paper's principal contribution is to present a comprehensive analysis of the complex, nonlinear function at the heart of word2vec. Our findings indicate that the core of word2vec's effectiveness may be attributed to a spectral method that underlies its operation. This revelation could pave the way for establishing verifiable assurances for the performance of word2vec. We corroborate our analysis through empirical testing. An intriguing avenue for future research is to explore whether the nonlinear aspects of word2vec that exceed the scope of the spectral method confer additional benefits, and if so, to elucidate the mechanisms through which these advantages are realized."}
{"text": "The advancements in reinforcement learning, a pivotal domain within machine learning, have been substantial and noteworthy. Contrasted with traditional reinforcement learning strategies that hinge on a predefined system model, the approach that employs a reinforcement learning framework grounded in an unknown model showcases a markedly wider scope of versatility and adaptability. This study introduces an innovative reinforcement learning framework, coined as the \"neural network iterative linear quadratic regulator (NNiLQR)\", which is built upon the iterative linear quadratic regulator (iLQR) but operates independently of any preconceived system model. This novel methodology establishes a fresh, non-parametric protocol for deriving the optimal policy through the iterative enhancement of the neural network system, solely reliant on empirical data. A key advantage of this approach is its superior performance over the conventional iLQR technique, as it incorporates additional exploration within its algorithm, leading to enhanced outcomes in the objective function. The superiority of the NNiLQR method is substantiated through the outcomes of two demonstrative examples, vividly illustrating its pronounced benefits."}
{"text": "The scope of composed image retrieval, a task involving queries that combine an image with a concise textual instruction for its alteration, has been confined to simplistic images within specialized domains, such as fashion items, thus far. This restriction has impeded comprehensive exploration of intricate visual reasoning in diverse and rich visual and textual contexts. To mitigate this limitation, we have assembled the Compose Image Retrieval on Real-life images (CIRR) dataset, featuring more than 36,000 combinations of open-domain images sourced from the public and accompanied by human-composed modification directives. To facilitate the application of current techniques to a broader range of domains, we introduce CIRPLANT, a model based on transformers that harnesses the power of pre-trained vision-and-language (V&L) knowledge to adjust visual features in response to natural language instructions. The retrieval process is executed by identifying the nearest neighbors of the adjusted features. Our results showcase that, despite its straightforward design, CIRPLANT surpasses previous methods in open-domain image retrieval while achieving parity with the best accuracy on established narrow datasets, including fashion. With the unveiling of the CIRR dataset and our model, we anticipate stimulating additional research in the field of composed image retrieval."}
{"text": "Malicious URLs, often referred to as harmful websites, pose a significant and prevalent risk to cybersecurity, serving as hosts for unsolicited content such as spam, phishing attempts, and drive-by exploits. These URLs ensnare naive users, leading to financial losses, theft of personal information, and malware infections, resulting in annual damages amounting to billions of dollars. Prompt detection and response to these threats are crucial. Historically, detection methods have relied heavily on blacklists; however, these lists are inherently incomplete and incapable of identifying newly created malicious URLs. To enhance the effectiveness of malicious URL detection, there has been a growing interest in employing machine learning approaches. This paper endeavors to offer a thorough review and a structured perspective on the application of machine learning in detecting malicious URLs. We formally define the problem of malicious URL detection within the context of machine learning and systematically evaluate the literature that addresses various aspects of this challenge, including feature representation and algorithm design. Moreover, this review is intended for a broad audience, encompassing machine learning experts and engineers in academia, as well as cybersecurity professionals and practitioners, to aid in their comprehension of current advancements and to support their research and practical endeavors. Additionally, the paper addresses practical considerations in system design, highlights unresolved research issues, and suggests promising avenues for future investigation."}
{"text": "In this work, we introduce an end-to-end Feature Fusion Attention Network (FFA-Net) designed to directly recover a haze-free image. The architecture of FFA-Net is comprised of three pivotal components: first, a distinctive Feature Attention (FA) module that integrates Channel Attention and Pixel Attention mechanisms. This module acknowledges the varying significance of channel-wise features and the uneven distribution of haze across different image pixels, thereby treating features and pixels non-uniformly. This approach enhances the representational capacity of Convolutional Neural Networks (CNNs) by accommodating diverse types of information more flexibly. Second, a fundamental block design that combines Local Residual Learning with Feature Attention, enabling the network to bypass less critical information, such as thin haze regions or low-frequency details, through multiple local residual connections. This ensures that the main network architecture can concentrate on more impactful information. Third, an Attention-based multi-level Feature Fusion (FFA) structure, where feature weights are learned adaptively from the FA module, prioritizing significant features. This structure facilitates the retention of shallow layer information and its transmission to deeper layers. Our experimental outcomes reveal that FFA-Net outperforms existing state-of-the-art single image dehazing techniques significantly, both quantitatively and qualitatively. Specifically, it elevates the peak signal-to-noise ratio (PSNR) metric from 30.23dB to 36.39dB on the SOTS indoor test dataset, surpassing the best published results. The source code for FFA-Net is accessible on GitHub."}
{"text": "The focus of this document is the vision-driven robotic picking system crafted by Team Applied Robotics, our group, for the 2016 Amazon Picking Challenge. The contest tasked participants with engineering a robotic solution capable of selecting a broad assortment of items from shelves or totes. This paper delves into our design rationale and approach, the high-definition 3D vision system we employed, the integration of texture and shape-based object recognition algorithms, as well as the innovations in robot path planning and object handling mechanisms that we devised."}
{"text": "This thesis delves into the challenge of deep face recognition under an open-set protocol, where the goal is to engineer facial features that exhibit a smaller maximum intra-class distance compared to the minimum inter-class distance within a selected metric space. Hyperspherical face recognition, a burgeoning and increasingly popular research area, has emerged as a focal point in the field of face recognition. SphereFace, a pioneering work in this domain, introduced the concept of learning face embeddings with a significant inter-class angular margin. Despite its contributions, SphereFace grapples with notable training instability, hindering its practical applicability. To mitigate this issue, we present a comprehensive framework to elucidate the concept of large angular margin in hyperspherical face recognition. Within this framework, we build upon the foundations of SphereFace, introducing SphereFace-R, a variant that significantly enhances training stability. Our approach involves proposing two innovative methods for implementing the multiplicative margin and conducting a comparative analysis of SphereFace-R under three distinct feature normalization strategies: no feature normalization, hard feature normalization, and soft feature normalization. Additionally, we introduce a technique termed \"characteristic gradient detachment\" to ensure stable training. Our empirical evaluations demonstrate that SphereFace-R consistently outperforms or matches the performance of current state-of-the-art methods."}
{"text": "The field of human action recognition from skeleton data, significantly advanced by Graph Convolutional Networks (GCNs), has garnered substantial interest due to GCNs' potent ability to model non-Euclidean structured data. Nevertheless, a common limitation of existing GCN approaches is the use of a predefined graph that remains static throughout the network, potentially overlooking inherent joint correlations. Additionally, the prevalent spectral GCN is often approximated by a single-hop connection, leading to an underrepresentation of higher-order relationships. Consequently, there is a pressing need to develop a more sophisticated GCN architecture. In response to these challenges, we leverage Neural Architecture Search (NAS) to introduce the first automatically generated GCN tailored for action recognition from skeleton data. Our approach expands the search space by incorporating various dynamic graph modules, following a thorough investigation of the spatial-temporal interrelations among nodes. We also incorporate multi-hop modules to overcome the constraints imposed by the one-hop approximation, aiming to enhance the model's representational capacity. Furthermore, we devise an efficient evolutionary strategy that optimizes sampling and memory usage to discover the optimal architecture for this task. The resulting architecture validates the efficacy of higher-order approximations and dynamic graph modeling with temporal interactions, a topic that has received scant attention previously. To assess the performance of our searched model, we perform comprehensive experiments on two large-scale datasets, demonstrating that our model achieves state-of-the-art results."}
{"text": "Attention-driven scene text recognition models have achieved significant advancements, utilizing a compact intermediate representation to facilitate the learning of 1D or 2D attention through a recurrent neural network (RNN) based encoder-decoder framework. Nonetheless, these models are plagued by the attention-drift issue, stemming from the high similarity among encoded features that confounds the RNN-based local attention mechanism. Additionally, RNN-based approaches are inherently inefficient due to their limited parallelization capabilities. To address these limitations, we introduce MASTER, a self-attention driven scene text recognizer that (1) not only captures input-output attention but also learns self-attention, encoding feature-feature and target-target relationships within the encoder and decoder, (2) develops a more potent and resilient intermediate representation against spatial distortions, and (3) boasts superior training efficiency due to high parallelization and expedited inference facilitated by an efficient memory-cache mechanism. Comprehensive evaluations on diverse benchmarks showcase MASTER's outstanding performance in recognizing both regular and irregular scene text. The Pytorch implementation is available at https://github.com/wenwenyu/MASTER-pytorch, while the Tensorflow version can be accessed at https://github.com/jiangxiluning/MASTER-TF."}
{"text": "The application of learning methodologies has proven highly effective in enhancing image resolution from low-quality inputs. Video super-resolution, an extension of this concept, seeks to leverage information across multiple frames. Traditionally, this is achieved through the alignment of frames using optical flow and subsequent image warping. In our contribution, we present an integrated video super-resolution network that, unlike prior art, incorporates optical flow estimation within its architecture. Our investigation reveals that conventional image warping techniques, when applied off-the-shelf, offer limited enhancement to video super-resolution through optical flow. To address this, we introduce a motion compensation operation that directly warps images from low to high resolution. Our results demonstrate that this network design significantly benefits from optical flow, achieving cutting-edge performance on established benchmark datasets. Additionally, we highlight the substantial improvement in accuracy attributed to processing entire images as opposed to isolated patches."}
{"text": "The annotation of histopathological images is a task that is both time-consuming and laborious, necessitating the meticulous scrutiny of extensive whole-slide images by highly qualified pathologists, from cellular to tissue levels. Despite the recent advancements in transfer learning techniques, which have been extensively explored for image understanding tasks with scarce annotations, they often struggle to maintain performance when applied to histology image analysis due to the domain discrepancy between the source training dataset and the target dataset. This discrepancy can manifest in various forms, such as differences in tissue types, staining variations, and imaging equipment. To address this challenge, we introduce a novel unsupervised domain adaptation method for histopathological image analysis. This method is anchored in a backbone network that projects input images into a feature space, complemented by a graph neural layer that facilitates the propagation of supervision signals from labeled images. The graph model is constructed by linking each image to its nearest neighbors within the embedded feature space. Subsequently, a graph neural network is utilized to synthesize novel feature representations for each image. During the training phase, target samples that yield confident predictions are dynamically assigned pseudo labels. The cross-entropy loss function is employed to regulate the predictions of source samples with manually annotated labels and target samples with pseudo labels. Additionally, the maximum mean discrepancy is utilized to promote the extraction of domain-invariant feature representations, while contrastive learning is harnessed to augment the discriminative power of the learned features across categories. Our method, when evaluated in unsupervised domain adaptation tasks for histopathological image classification, demonstrates superior performance on four publicly available datasets."}
{"text": "Recent advancements in geometric representation learning have demonstrated significant potential across various machine learning domains, encompassing relational learning, language processing, and generative models. This study focuses on the manifold-valued regression problem onto a hyperbolic space, serving as a pivotal component for several machine learning applications. Specifically, by casting the prediction of tree nodes as a regression task in hyperbolic space, we introduce a fresh viewpoint on two complex problems: hierarchical classification through label embeddings and the extension of hyperbolic representations for taxonomy. To tackle the regression challenge, we not only review existing methodologies but also introduce two innovative, computationally efficient approaches: a deep learning model parameterized with insights from the target space's geodesics, and a kernel-method that is non-parametric, for which we establish excess risk bounds. Empirical results highlight the promise of leveraging hyperbolic geometry, particularly in taxonomy expansion, where hyperbolic-based estimators markedly surpass methods that conduct regression in the ambient Euclidean space."}
{"text": "A critical aspect of ensuring the safety of machine learning (ML) systems involves systematically addressing vulnerabilities in neural networks, particularly when they are intended for use in high-stakes applications. A common safety concern arises from learned shortcuts, which are spurious correlations that a network may utilize for decision-making, despite having no intrinsic relevance to the task at hand. Networks that depend on such shortcuts are prone to poor generalization when confronted with novel data. While explainability methods are instrumental in revealing these hidden vulnerabilities, their applicability is often limited in black-box scenarios, where direct access to the network is restricted. This is particularly relevant when utilizing third-party ML components. To circumvent this limitation, we propose an approach that employs an interpretable-by-design network as a surrogate for the black-box model under scrutiny. By capitalizing on the surrogate's inherent introspective capabilities, we are able to automatically identify potential learned shortcuts. The transferability of these shortcuts to the black-box model is then rigorously assessed. Specifically, we opt for a BagNet as the surrogate model, which makes decisions based solely on local image patches. Our experiments on the A2D2 autonomous driving dataset illustrate that the patch shortcuts extracted from the BagNet have a substantial impact on the black-box model's behavior. This method of efficiently pinpointing patch-based vulnerabilities contributes to the development of more secure ML systems."}
{"text": "The field of human-computer interaction has seen significant research efforts in object detection, with skin area detection playing a pivotal role in various recognitions, including face recognition, human motion detection, and the prediction of pornographic and nude images. Historically, the majority of skin detection studies have utilized datasets featuring individuals of African, Mongolian, and Anglo-Saxon ethnicities. Despite the existence of intensity invariant methods for skin detection, the unique skin tones of the Indian sub-continent have received less attention. This study aims to conduct a comparative analysis of three image segmentation techniques applied specifically to Indian sub-continental human images, with the goal of refining detection criteria and identifying effective parameters for skin area detection. Experimental results indicate that the HSV color model-based approach is particularly well-suited for Indian sub-continental skin detection, achieving a notable success rate of 91.1% true positives and 88.1% true negatives."}
{"text": "The proposed framework, DeepV2D, is an end-to-end deep learning model designed for depth prediction from video sequences. It integrates the powerful representation capabilities of neural networks with the fundamental geometric laws that govern the formation of images. By assembling a set of traditional geometric algorithms and transforming them into trainable components, we create an entirely differentiable architecture. DeepV2D operates through an iterative process that alternates between motion and depth estimation, allowing for the refinement of depth predictions until they reach a high level of accuracy. The source code for DeepV2D can be accessed at https://github.com/princeton-vl/DeepV2D."}
{"text": "The creation of highly authentic fake images through technologies like Deepfake or Generative Adversarial Networks (GANs) has introduced significant social upheaval. Although various techniques have been developed to identify such fabricated images, they are susceptible to adversarial perturbations, which are strategically added distortions that can mislead detection mechanisms. Typically, methods for undermining fake image detectors involve generating perturbations that affect the entire image, an approach that is both inefficient and enhances the visibility of the alterations. In this work, we introduce a new strategy to hinder fake image detection by pinpointing and targeting only the critical pixels that are significant to the detector, leading to a substantial reduction in the L_0 and L_2 norms of the adversarial perturbations compared to current methodologies. Our experimental results, using two publicly available datasets and three fake image detectors, demonstrate that our proposed method outperforms existing techniques in both white-box and black-box attack scenarios, achieving the best performance to date."}
{"text": "The compelling demand for 3D perception through sensors that adhere to automotive industry standards is a critical requirement for the advancement of autonomous driving technology. The emergence of MEMS LiDAR is a significant development due to its cost-effectiveness, robustness, and alignment with mass-production criteria. However, its narrow field of view (FoV) poses a limitation, hindering its widespread adoption. In this thesis, we introduce LEAD, an acronym for LiDAR Extender for Autonomous Driving, which aims to enhance the capabilities of MEMS LiDAR by augmenting its FoV and range through the integration of image data. Our approach involves a multi-stage propagation strategy, leveraging depth distributions and uncertainty maps, demonstrating its efficacy in extending depth information. Furthermore, our depth propagation network adopts a teacher-student training paradigm, effectively transferring depth estimation capabilities to the depth completion network without introducing scale errors. To assess the quality of LiDAR extension, we employ a high-precision laser scanner to create a ground-truth dataset. Both quantitative and qualitative analyses confirm that our method surpasses state-of-the-art techniques by a considerable margin. We are confident that the proposed LEAD system, coupled with the accompanying dataset, will contribute significantly to the field of depth research within the academic and industrial communities."}
{"text": "The recent surge in interest towards neural network compression is driven by the substantial computational demands of contemporary deep learning models. This study aims to distill the knowledge from a complex, high-precision model into a more compact version. Our innovations are three-pronged: firstly, we introduce an adversarial compression technique that facilitates the training of the smaller, or 'student', network to emulate the behavior of the larger, or 'teacher', network, all without requiring labels in the training phase. Secondly, we devise a regularization mechanism to avert the creation of an overly powerful discriminator, ensuring that the network's capacity is not compromised. Lastly, our methodology is versatile, applicable across various teacher-student model configurations. Through a comprehensive assessment on five benchmark datasets, we validate that our student model exhibits minimal loss in accuracy, outperforms alternative knowledge transfer techniques, and even outshines the same network when trained solely with labels. Moreover, our approach yields superior outcomes when juxtaposed with other prevailing compression methods."}
{"text": "In this work, we explore the challenge of fashion image retrieval through a conversational interface, utilizing multi-turn natural language feedback, a scenario largely unaddressed in prior research which predominantly focuses on single-turn interactions. Current approaches to multi-turn conversational fashion retrieval are constrained by the use of conventional models, resulting in suboptimal performance. To address these limitations, we introduce an innovative framework designed to efficiently manage fashion image retrieval in a multi-turn conversational context, incorporating natural language feedback. A distinctive feature of our framework is its ability to identify potential images by integrating the encoded data from the reference image, feedback text, and the entire conversation history. Additionally, we employ a mutual attention mechanism to effectively utilize the fashion attribute information of images. Given the absence of a suitable dataset for our multi-turn task, we create a comprehensive, large-scale fashion dataset by augmenting an existing single-turn dataset with additional manual annotations. Our experimental results demonstrate that the proposed model achieves a substantial improvement over current state-of-the-art methods in this domain."}
{"text": "The challenge of pose tracking, which involves recognizing and linking distinct human postures across sequential video frames, remains significant due to the complexity of modeling temporal dynamics and the high computational demands of current methodologies, often necessitating offline processing. In this work, we introduce KeyTrack, an innovative multi-person pose tracking approach that operates in real-time by exclusively utilizing keypoint data, bypassing the need for RGB or optical flow information. Our method employs Pose Entailment, a technique that samples pose estimates from disparate video frames, tokenizes them, and then employs a Transformer-based network to determine if one pose chronologically succeeds another in a binary classification task. Additionally, we enhance our top-down pose estimation process with a new, parameter-free keypoint refinement strategy, which boosts the accuracy of the keypoints utilized in the Pose Entailment phase. KeyTrack outperforms existing methods on the PoseTrack'17 and PoseTrack'18 benchmarks, achieving state-of-the-art performance with a markedly reduced computational footprint for tracking calculations."}
{"text": "The field of early time series classification, aimed at reducing prediction latency in critical domains like healthcare and finance, has garnered significant attention. The core objective is to accurately categorize incomplete time series data at the earliest feasible moment. Over recent years, a plethora of methodologies for achieving early classification have emerged. Given the diversity of these approaches, each addressing the classification challenge from unique angles, a comprehensive analysis of the existing landscape is imperative to gauge the field's progression. These methodologies have been successfully applied across various sectors, encompassing human activity recognition, health diagnostics based on gene expression, and industrial surveillance. This paper offers a structured overview of the contemporary literature on early classification techniques for both single and multi-dimensional time series. We categorize the reviewed methods into four distinct groups: prefix-based, shapelet-based, model-based, and miscellaneous, based on their underlying strategies. Additionally, the paper explores the practical implications of early classification in sectors such as industrial monitoring, intelligent transportation systems, and medicine. Concluding the review, we synthesize the findings from the current body of work and outline potential avenues for future research."}
{"text": "The integration of text onto images is a critical aspect of crafting visually appealing designs. Automating this process necessitates the accurate determination of the text's position, orientation, and style, which in turn demands an understanding of the image's content. This specific endeavor, which we term \"copyspace detection,\" is separate from the task of distinguishing between foreground and background elements. Our approach involves the creation of algorithms based on single and dual-stage object detection techniques, trained on a dataset meticulously annotated by experts. During the workshop, we will explore these copyspace detection algorithms and showcase their practical implementation within generative design frameworks, including the Einstein Designer tool."}
{"text": "The widespread use of social media has accelerated the fashion cycle, with trends inspired by celebrities, designers, and influencers now emerging at a rapid pace. However, the abundance of fashion-related content and the vast number of user-generated fashion images pose a significant challenge for designers seeking to curate the latest trends from social media platforms. This situation demands a sophisticated analysis of fashion images to identify and categorize multiple fashion items within a single photograph. Despite the availability of large datasets for object detection competitions like MSCOCO, obtaining extensive labeled datasets for fast fashion items remains a daunting task. Furthermore, current state-of-the-art object detectors lack the capability to leverage the vast amounts of unlabeled data on social media to refine their performance with labeled datasets. In this thesis, we demonstrate the application of a versatile object detector, which can be pre-trained in an unsupervised manner, on 24 categories from the recently published Open Images V4 dataset. Initially, the base architecture of the object detector is trained using unsupervised learning on 60,000 unlabeled images from 24 categories sourced from social media. This is followed by fine-tuning on 8,200 labeled images from the Open Images V4 dataset. Our model achieves a mean average precision (mAP) of 72.7% on a test set of 2,400 images, with input sizes of 300x300 pixels, outperforming state-of-the-art object detectors by 11% to 17%. This enhancement is attributed to our architectural choice, which facilitates unsupervised learning and exhibits superior performance in detecting smaller objects."}
{"text": "Previous research indicates that the crux of effective face anti-spoofing techniques hinges on identifying subtle image characteristics, referred to as \"spoof traces,\" which include phenomena such as color distortion, 3D mask edges, Moire patterns, and others. Developing a versatile face anti-spoofing model capable of discerning these spoof traces not only enhances the detection system's generalizability but also increases the transparency of the model's decision-making process. However, this endeavor is fraught with challenges due to the wide variety of spoofing methods and the scarcity of definitive spoof trace data. In this thesis, we introduce an innovative adversarial learning framework designed to separate spoofed faces into their constituent spoof traces and corresponding genuine facial components. Inspired by physical principles, spoof creation is modeled as a dual process: an additive process and an inpainting process. The additive process characterizes spoofing as the introduction of additional patterns by spoof materials, allowing for the recovery of the authentic facial features by eliminating these extraneous patterns. Conversely, the inpainting process views spoofing as the complete overlay of certain areas by spoof materials, necessitating the \"reconstruction\" of the genuine facial features in those regions. We employ a combination of three additive components and one inpainting component to represent spoof traces across various frequency bands. The disentangled spoof traces are then utilized to generate realistic spoof faces, following appropriate geometric adjustments, which can be incorporated into training datasets to enhance the spoof detection system's robustness. Our methodology showcases outstanding performance in spoof detection across three evaluation scenarios: known attacks, unknown attacks, and open-set attacks, while also offering a visually compelling approximation of the spoof traces. The source code and pre-trained models will be made accessible to the public upon the publication of this work."}
{"text": "This work introduces an access control technique that leverages the spatially invariant permutation of feature maps, secured by a secret key, to safeguard semantic segmentation models. The models are trained and evaluated by permuting chosen feature maps using a secret key, enabling authorized users with the correct key to fully utilize the model while impairing the functionality for unauthorized users. Unlike traditional access control approaches that have solely concentrated on image classification, this novel method is the first to be applied to semantic segmentation tasks. Experimental results show that the protected models enable legitimate users to achieve performance nearly identical to that of unprotected models, while effectively resisting unauthorized access. Moreover, it was confirmed that a conventional method employing block-wise transformations exhibits diminished performance when applied to semantic segmentation models."}
{"text": "In conventional convolutional layers, the learned filters remain static post-training. Conversely, we propose a novel architecture, the Dynamic Filter Network, where filters are dynamically generated in response to the input. This design offers enhanced adaptability and versatility, without significantly inflating the model's parameter count. The architecture is capable of learning a broad spectrum of filtering operations, encompassing local spatial transformations, selective (de)blurring, and adaptive feature extraction. Furthermore, these layers can be integrated in a recurrent configuration. The efficacy of the dynamic filter network is showcased through its superior performance on video and stereo prediction tasks, achieving state-of-the-art results on the moving MNIST dataset with a markedly reduced model size. Visual analysis of the learned filters reveals that the network has autonomously captured flow information from unlabelled training data. This finding implies the potential of the network for unsupervised pretraining of models for diverse supervised tasks, such as optical flow and depth estimation."}
{"text": "An enhancement to the model-free anomaly detection technique, Isolation Forest, is introduced, termed Extended Isolation Forest (EIF), which addresses the shortcomings in the anomaly score assignment to data points. The motivation for this enhancement is illustrated through heat maps of anomaly scores, which exhibit distortions due to the branching criteria of the binary tree. This issue is comprehensively explained, and its visual manifestation is demonstrated. To mitigate this problem, two strategies are proposed. The first involves random data transformation prior to the creation of each tree, which helps in reducing the bias through averaging. The second, and more favored, approach entails permitting the data slicing to utilize hyperplanes with random slopes, effectively eliminating the artifacts observed in the anomaly score heat maps. The enhanced robustness of the algorithm is showcased by examining the variance of scores for data points situated along constant level sets. The performance of EIF is evaluated using AUROC and AUPRC metrics on both synthetic and real-world benchmark datasets. It is observed that EIF does not significantly alter the convergence rate or computational time compared to the conventional Isolation Forest."}
{"text": "A new attention mechanism is introduced, capable of precisely focusing on diverse-sized and shaped objects within images. This model is educated to systematically diminish the significance of unrelated areas through a multilayered, progressive attention mechanism within a convolutional neural network. At every layer, the attention process decides on the transmission or inhibition of features at specific spatial points for subsequent layers' utilization. The suggested progressive attention technique, particularly when merged with hard attention, demonstrates superior performance. Additionally, local contexts are utilized to integrate the surrounding characteristics of each position, enhancing the accuracy of the attention probability map. Experimental outcomes from both synthetic and real-world datasets confirm that the proposed attention networks surpass conventional attention approaches in the prediction of visual attributes."}
{"text": "Graph Neural Networks (GNNs) have demonstrated significant advancements in the field of graph representation learning. However, the reliance of contemporary GNNs on the full-scale loading of attributed graphs into memory for processing poses a limitation, particularly in scenarios with constrained memory resources, more so when dealing with extensive graphs. In this work, we introduce a pioneering approach, the Binary Graph Convolutional Network (Bi-GCN), which innovates by binarizing both the network parameters and the input node features. Additionally, the conventional matrix multiplications are substituted with binary operations to enhance computational efficiency. Our theoretical analysis reveals that Bi-GCN can achieve a remarkable reduction in memory usage by approximately 30 times for both network parameters and input data, alongside a notable acceleration in inference speed by roughly 47 times, specifically on citation networks. To ensure effective training of our Bi-GCN, we have devised a novel gradient approximation-based back-propagation technique. Comprehensive experimental evaluations confirm that Bi-GCN delivers performance on par with full-precision benchmarks. Moreover, the binarization strategy we propose can be readily integrated into other GNN architectures, a claim substantiated by our experimental findings."}
{"text": "The Single Image Super-Resolution (SISR) challenge involves training a model to map low-resolution images to their high-resolution equivalents, a task that is inherently complex due to its ill-posed nature. While Convolutional Neural Networks (CNNs) have emerged as leading performers in SISR, they often fall short in reproducing the fine details of images. To address this limitation, Generative Adversarial Networks (GANs) have been introduced to enhance detail recovery, but they come with their own drawbacks, including training instability and the introduction of artifacts in the high-resolution output. In this work, we introduce a novel approach that extends the capabilities of CNNs by encouraging them to align images across various spaces, not limited to the pixel domain. This is achieved through the design of a specialized space utilizing convex optimization, which facilitates the learning of both high- and low-frequency components of the images. Our results demonstrate that this method not only recovers the intricate details of images effectively but also ensures a stable training phase."}
{"text": "Multi-label classification of images and videos represents a cornerstone yet arduous endeavor in the field of computer vision, primarily due to the intricacies involved in discerning spatial and temporal interrelations among labels, alongside pinpointing the precise locations of distinctive features for each category. To address these complexities, we introduce a novel approach that integrates cross-modality attention mechanisms with semantic graph embedding for multi-label classification tasks. Our methodology involves the creation of a label graph, upon which we base our adjacency-based similarity graph embedding technique to derive semantic label embeddings. This process explicitly leverages the interconnections between labels. Subsequently, we generate unique cross-modality attention maps, guided by the learned label embeddings. Our experimental outcomes on the MS-COCO and NUS-WIDE datasets for multi-label image classification illustrate that our proposed method surpasses current state-of-the-art techniques. Furthermore, we assess the efficacy of our approach on the YouTube-8M Segments dataset, a substantial multi-label video classification repository, and the resultant evaluations affirm the robust generalization potential of our method."}
{"text": "In the context of image inpainting, where an incomplete image is to be filled in, the absence of additional constraints inherently permits a multitude of plausible solutions. Recent advancements have explored this multiplicity, showcasing the potential for generating a range of outcomes. However, these approaches often struggle to maintain the quality of each individual solution, frequently resulting in structural distortions or texture blurriness. To address this, we introduce a novel two-stage framework for achieving diverse inpainting results. The initial stage involves the creation of multiple, albeit coarse, images, each with a distinct structure. Subsequently, the second stage refines these preliminary outcomes by separately enhancing the texture, thereby improving the overall quality. Our model draws inspiration from the hierarchical vector quantized variational auto-encoder (VQ-VAE), leveraging its architecture to disentangle structural and textural information effectively. The vector quantization feature of the VQ-VAE facilitates autoregressive modeling of the discrete distribution over structural details. Sampling from this distribution yields a variety of high-quality structures, forming the basis of the first stage in our approach. For the second stage, we incorporate a structural attention module within the texture generation network. This module exploits structural information to capture long-range correlations, enhancing the coherence of the generated images. To further refine the structure and texture realism, we employ the VQ-VAE to compute two feature losses. Our experimental outcomes on the CelebA-HQ, Places2, and ImageNet datasets demonstrate that our method not only increases the diversity of inpainting solutions but also significantly enhances the visual quality of the multiple images produced. The code and models are accessible at: https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting."}
{"text": "Chromosomal karyotype analysis holds significant clinical value in diagnosing and managing diseases, particularly genetic disorders. Given the labor-intensive nature of manual analysis, automated image-based methods have become standard practice to enhance both the efficiency and precision of karyotyping. However, the elongated morphology of chromosomes often leads to their overlap in images, which can compromise the accuracy of subsequent analysis. Traditional segmentation techniques for overlapping chromosomes, which rely on manually engineered features, are prone to performance fluctuations due to variations in image quality, such as resolution and illumination. In response to this challenge, this paper introduces an adversarial multiscale feature learning framework designed to boost the accuracy and robustness of chromosome segmentation in overlapping scenarios. Initially, a nested U-shaped network with dense skip connections is utilized as the generator to uncover the optimal representation of chromosomal images by leveraging features at multiple scales. Subsequently, a conditional generative adversarial network (cGAN) is employed to synthesize images that closely resemble the originals, with enhanced training stability achieved through the application of the least-square GAN objective. To further refine the model's performance, Lovasz-Softmax is integrated to facilitate convergence in a continuous optimization environment. When benchmarked against existing algorithms using public datasets, our framework demonstrates superior performance across eight evaluation metrics, underscoring its potential for advancing the field of overlapping chromosome segmentation."}
{"text": "A new unsupervised representation learning framework, termed Robust Block-Diagonal Adaptive Locality-constrained Latent Representation (rBDLR), is introduced in this work. rBDLR is designed to simultaneously recover multi-subspace structures and extract adaptive, locality-preserving salient features. By leveraging the Frobenius-norm based latent low-rank representation, rBDLR co-learns coding coefficients and salient features, enhancing robustness to outliers and errors in the data, while adaptively preserving local information of the features and ensuring the block-diagonal structure of the coefficients. To bolster robustness, latent representation and adaptive weighting are executed in a reconstructed, clean data space. To enforce a block-diagonal structure on the coefficients, auto-weighting is applied by minimizing the reconstruction error based on salient features, subject to a block-diagonal regularizer. This ensures the acquisition of a strict block-diagonal weight matrix and confers adaptive locality preservation on the salient features. By minimizing the discrepancy between the coefficient and weight matrices, a block-diagonal coefficients matrix is obtained, facilitating the propagation and exchange of useful information between salient features and coefficients. Comprehensive experimental outcomes substantiate the superior performance of rBDLR compared to contemporary methods."}
{"text": "In the realms of domain generalization (DG) and unsupervised domain adaptation (UDA), the strategy of cross-domain feature alignment has been extensively utilized to harmonize the feature distributions across different domains, aiming to cultivate representations that are invariant to domain shifts. Nevertheless, this alignment process often overlooks task-specific nuances, potentially compromising the discriminative efficacy of the feature representations and, consequently, performance. To address these limitations, we introduce a comprehensive approach, coined Feature Alignment and Restoration (FAR), designed to concurrently maintain high generalization and discriminative capabilities of neural networks for enhanced DG and UDA outcomes. Our method involves a two-pronged strategy: first, we execute feature alignment (FA) by equalizing the statistical moments of attentively chosen features across domains to minimize inter-domain discrepancies. To preserve discriminative power, we introduce a Feature Restoration (FR) mechanism that extracts task-specific features from the residual information, which are then integrated to augment the aligned features. To ensure effective disentanglement, we impose a dual ranking entropy loss during the FR phase, promoting the segregation of task-relevant and task-irrelevant features. The robustness and superior performance of our FAR framework for both domain generalization and unsupervised domain adaptation are substantiated through comprehensive evaluations on a variety of classification datasets."}
{"text": "The creation of point clouds, such as molecular structures, in various orientations, positions, and counts, continues to pose a significant challenge. Concurrently, neural networks that incorporate symmetry invariant layers have demonstrated their capability to optimize training objectives with high data efficiency. Inspired by this, we introduce an architectural design that facilitates the generation of valid Euclidean distance matrices, inherently immune to rotational and translational variations of the represented object. With the aim of synthesizing molecular structures within Cartesian space, we employ this design to develop a Wasserstein GAN, featuring a permutation invariant critic network. This approach enables the direct generation of molecular structures by creating Euclidean distance matrices that can be embedded in three dimensions, achieving a one-shot synthesis."}
{"text": "In numerous machine learning endeavors, the efficacy of a solution often hinges on the quality of the data representation. This is due to the reliance of most learning algorithms on features to construct models for the data. For example, the performance of classification can be enhanced by projecting the data into a domain where classes are more distinct, and regression tasks can be simplified by identifying the data's manifold within the feature space. Typically, feature transformation is achieved through statistical methodologies, such as principal component analysis, or manifold learning techniques, including Isomap and locally linear embedding. Among the myriad of representation learning approaches, autoencoders stand out as particularly adaptable instruments. This thesis endeavors to illustrate how to guide the representations learned by autoencoders to attain desired learning outcomes. To achieve this, a range of learning tasks are introduced: data embedding for visualization, image denoising, semantic hashing, anomaly detection, and instance generation. Each task is approached from a representation learning viewpoint, adhering to the latest methodologies in their respective domains. An autoencoder-based solution is proposed for every task, serving as the sole learning mechanism. The theoretical concepts are validated through practical applications using a variety of datasets for the distinct problems, followed by a critical analysis of the outcomes in each case study and a concise overview of six additional learning applications. The thesis also delves into the current obstacles and strategies for enhancing explainability in the context of autoencoders. Collectively, these investigations underscore the potential of autoencoders to serve as the cornerstone for addressing a multitude of issues that can be conceptualized as transformations of the feature space, through modifications to their architecture and objective function."}
{"text": "Estimating the interdependencies among observations from various time series is essential for tasks including anomaly detection, financial risk assessment, causal inference, and demand prediction. Yet, the computational and numerical challenges associated with inferring time-evolving and high-dimensional covariance matrices often confine conventional methodologies to dimensions in the hundreds or necessitate stringent assumptions about the inter-series relationships. To address these limitations, we introduce a novel approach that merges an RNN-driven time series model with a Gaussian copula process output model, featuring a low-rank covariance structure. This integration not only mitigates computational complexity but also accommodates non-Gaussian marginal distributions, enabling a substantial reduction in the number of parameters required. Consequently, our method facilitates the modeling of time-varying correlations across thousands of time series. Through empirical evaluations on diverse real-world datasets, we demonstrate that our proposed technique outperforms leading benchmarks in terms of accuracy. Additionally, we conduct an ablation study to elucidate the individual contributions of the model's components."}
{"text": "In the realm of data exploration, a typical process involves acquiring a reduced-dimension portrayal of the data, pinpointing clusters of data points within this representation, and subsequently analyzing the disparities among these clusters to discern their underlying significance. By framing this procedure as an interpretable machine learning challenge, we harness the model responsible for the dimensionality reduction to aid in discerning the salient contrasts between the identified clusters. To address this issue, we propose a novel explanatory concept, termed Global Counterfactual Explanations (GCEs), alongside our computational method, Transitive Global Translations (TGT), designed to calculate GCEs. TGT employs compressed sensing to delineate the discrepancies between every pair of clusters, while ensuring these pairwise differences adhere to a consistent pattern across all clusters. Through empirical validation, we showcase TGT's capability to uncover explanations that not only faithfully elucidate the model's behavior but also exhibit sparsity, aligning with genuine patterns embedded within the data."}
{"text": "As the integration of AI becomes more widespread, the inherent security and privacy weaknesses in machine learning systems are increasingly being exposed. Among these vulnerabilities is the potential for adversaries to extract confidential information about the nature of the data used to train a specific machine learning model. This threat, known as a model inversion attack, exploits classification scores in a sequential manner to generate high-confidence representations for different classes. Yet, for deep learning architectures, this approach often yields representations that are indiscernible and of little value to the attacker. In this thesis, we propose a more practical interpretation of model inversion, where the attacker has knowledge of the model's intended function, such as whether it is designed for OCR or facial recognition, and aims to uncover plausible representations within the relevant lower-dimensional manifold, which could be that of general symbols or faces, respectively. To achieve this, we utilize the characteristics of generative adversarial networks to create a connected lower-dimensional space, and we showcase the effectiveness of our model inversion technique when executed within this manifold."}
{"text": "In this work, we introduce a novel analytical framework for addressing a category of low-rank matrix recovery challenges on Riemannian manifolds. Our investigation centers on the global dynamics of Riemannian optimization algorithms initialized randomly. Employing the Riemannian gradient descent method, we aim to minimize a least squares cost function, delving into both the asymptotic characteristics and the precise convergence velocity. A hitherto undiscovered geometric trait of the low-rank matrix manifold is unveiled, specifically, the presence of deceptive critical points for the straightforward least squares function on this manifold. Under certain conditions, we demonstrate that Riemannian gradient descent, when initiated randomly, is highly likely to bypass these misleading critical points, converging exclusively to the true solution at an almost linear rate, i.e., (\\text(\\frac)+ \\text(n)) iterations to attain an \\epsilon-accurate solution. To illustrate our global analysis, we consider two instances: the first is a rank-1 matrix recovery scenario, and the second is an extension of the Gaussian phase retrieval problem, which, while only adhering to the weak isometry property, exhibits a comparable behavior save for an additional saddle set. Our convergence assurance is nearly optimal and largely independent of dimension, aligning closely with empirical observations. This global analysis holds promise for extension to other data-related issues featuring random measurement architectures and empirical least squares loss functions."}
{"text": "A novel unsupervised, iterative algorithm is introduced for registering point sets in N-dimensional Euclidean space, where the points are unlabeled, implying that the correspondence between them is initially unknown. The algorithm employs linear least squares methodology, systematically evaluating all potential pairings of points. Through an iterative process, it progressively aligns the two point sets until the quantity of point pairs no longer surpasses the threshold for the maximum feasible one-to-one correspondences."}
{"text": "The process of preparing and scanning histopathology slides involves multiple stages, each characterized by a range of variables. These variables can fluctuate not only across different pathology laboratories but also within the same facility over time, leading to substantial variations in tissue appearance. This variability poses a significant challenge to the universal applicability of automated image analysis techniques. Traditionally, such issues are tackled through specific methods, such as staining normalization, designed to minimize appearance inconsistencies. In this work, we introduce a comprehensive strategy that employs domain-adversarial neural networks to address this problem. Our underlying assumption is that eliminating domain-specific information from the model's representation will enhance its generalization capabilities. To validate this hypothesis, we focused on the task of identifying mitoses in breast cancer histopathology images and conducted a comparative evaluation against two alternative methods. Our findings demonstrate that the integration of color augmentation with domain-adversarial training outperforms conventional approaches, offering a superior means to augment the generalization of deep learning algorithms."}
{"text": "The proposed approach in this thesis advocates for the utilization of boosted regression trees as a means to derive solutions to reinforcement learning problems that are comprehensible to humans. By aggregating multiple regression trees, boosting enhances predictive accuracy while preserving the interpretability of individual trees. Previous research has largely addressed reinforcement learning and interpretable machine learning as separate domains, with scant attention paid to the intersection of these fields, particularly in terms of interpretability in reinforcement learning. The empirical findings presented herein demonstrate that boosted regression trees are capable of generating solutions that are not only interpretable but also rival the performance of state-of-the-art reinforcement learning techniques in terms of solution quality."}
{"text": "Image interpolation, also known as image morphing, involves creating a visually appealing transition between two or more input images. For the transition to be aesthetically pleasing, it should exhibit three key characteristics: smoothness, minimal alteration, and realism, ensuring that each intermediate image lacks unnatural artifacts. To achieve a seamless and simple transition, the Wasserstein Barycenter Problem (WBP) is often employed, as it ensures minimal changes under the Wasserstein metric. However, the resulting images from this method can sometimes appear artificial. In this thesis, we introduce a new approach to image morphing that fulfills all three desirable properties. We achieve this by formulating a constrained version of the WBP, which imposes an image prior on the intermediate images. We present an algorithm to solve this problem and showcase its effectiveness using both the sparse prior and generative adversarial networks."}
{"text": "Deep clustering methodologies integrate representation learning and clustering through the concurrent optimization of a clustering loss function and an additional non-clustering loss. Typically, this involves utilizing a deep neural network for representation learning in conjunction with a clustering network. Contrary to this conventional approach, we introduce a more straightforward method that focuses on enhancing the entanglement of the latent code representation learned by an autoencoder. Entanglement is characterized by the proximity of point pairs belonging to the same class or structure, in comparison to those from different classes or structures. To quantify this entanglement, we employ the soft nearest neighbor loss, augmenting it with an annealing temperature parameter. Our proposed technique achieved superior test clustering accuracies of 96.2% on the MNIST dataset, 85.6% on the Fashion-MNIST dataset, and 79.2% on the EMNIST Balanced dataset, surpassing the performance of our baseline models."}
{"text": "The objective of this paper is to develop a hierarchical compositional AND-OR model for image synthesis that is both interpretable and achieved through the sparsification of the generator network. This methodology is grounded in a hierarchical representation of images, progressing from scenes to objects, parts, subparts, and ultimately to primitives, akin to wavelet-like bases. The AND-OR structure is characterized by scenes that can be of various types (OR), each encompassing multiple objects (AND), a pattern that recurs throughout the hierarchy until reaching the primitive level. To operationalize this hierarchy in the context of image synthesis, the proposed framework involves two key elements: (i) Each hierarchical level is embodied by an over-complete collection of convolutional basis functions, with standard convolutional neural network architectures serving as the implementation vehicle for this hierarchy. (ii) The training process incorporates sparsity-inducing constraints, which transform the initially densely connected generator network into a sparsely activated and sparsely connected AND-OR model. This is achieved through a simple sparsity-inducing mechanism that restricts the activation of basis functions to the top-k at each layer (with k being a tunable hyperparameter). The learned basis functions not only facilitate image synthesis but also enable image reconstruction, providing insights into the input images. The efficacy of the proposed method is demonstrated through experiments on four established datasets, where it is shown to learn meaningful and interpretable hierarchical representations, outperforming baselines in terms of image synthesis and reconstruction quality."}
{"text": "The evaluation of optical flow algorithms through current benchmarks involves direct comparison of estimated flow fields with ground truth or indirect assessment via frame interpolation, followed by comparison with actual frames, often utilizing metrics like mean squared error (MSE). However, these objective measures are insufficient for accurately reflecting user-perceived image quality. To address this, we initiated a subjective quality assessment through a crowdsourcing study, focusing on frames interpolated by the Middlebury benchmark. Participants were asked to conduct forced-choice comparisons between interpolated images and their ground truth counterparts. To enhance observers' sensitivity to subtle differences, we introduced a novel technique, artefact amplification, to the full-reference quality assessment domain. Using the collected data, we applied Thurstone's model to derive absolute quality scale values, leading to a re-evaluation of 155 algorithms based on the visual quality of interpolated frames. This re-ranking underscores the importance of visual quality as an additional metric for optical flow and frame interpolation benchmarks. Moreover, the results serve as a foundation for developing new image quality assessment (IQA) methods tailored to the perceptual quality of interpolated images. As an initial effort, we introduced WAE-IQA, a full-reference method that outperformed the leading FR-IQA approach by slightly improving the weighting of local differences between interpolated images and their ground truth."}
{"text": "The advancement of automated vehicles capable of navigating complex driving situations and effectively communicating with other road users necessitates a semantic understanding of the driving environment, typically derived from the analysis of extensive naturalistic driving datasets. A key approach to enable automated vehicles to learn from human drivers and gain insights is through the identification of the fundamental components of traffic, known as traffic primitives. However, the exponential growth of data presents a significant challenge in extracting these primitives from high-dimensional time-series traffic data involving diverse road users. Consequently, automating the extraction of primitives has emerged as a cost-effective strategy to assist autonomous vehicles in comprehending and forecasting intricate traffic scenarios. Moreover, the primitives derived from raw data should not only be suitable for automated driving applications but also readily utilized for the generation of novel traffic scenarios. Yet, the current literature lacks a method for automatically learning these primitives from voluminous traffic data. This paper makes two significant contributions. Firstly, it introduces a novel framework for creating new traffic scenarios from a limited set of traffic data. Secondly, it presents a nonparametric Bayesian learning technique, specifically a sticky hierarchical Dirichlet process hidden Markov model, to autonomously extract primitives from multidimensional traffic data without prior assumptions about the primitive configurations. The proposed method is evaluated using a day's worth of naturalistic driving data, with the results demonstrating the efficacy of the nonparametric Bayesian learning technique in extracting primitives from traffic scenarios characterized by the co-occurrence of binary and continuous events."}
{"text": "A suite of datasets derived from core physics investigations, encompassing particle physics, astroparticle physics, and hadron- and nuclear physics, is introduced for supervised machine learning explorations. This compilation, featuring hadronic top quarks, cosmic-ray instigated air showers, phase transitions in hadronic matter, and generator-level chronologies, is made accessible to the public to facilitate future endeavors in interdisciplinary machine learning and transfer learning within fundamental physics. Grounded on these datasets, we propose a straightforward yet adaptable graph-based neural network design that can be readily applied to a broad spectrum of supervised learning tasks in these scientific areas. Our methodology demonstrates near-state-of-the-art performance compared to specialized techniques on all datasets. To ease the application to diverse challenges, we offer clear guidelines on constructing graph-based representations of data structures pertinent to fundamental physics, alongside code implementations for a variety of these structures. Code for our suggested approach and all comparative algorithms is also made available."}
{"text": "The challenge of deriving a precise, dense depth map from a solitary RGB image is tackled in this work. Initially, we employ a fundamental encoder-decoder convolutional neural network structure and inquire into the potential benefits of global information processing in enhancing the accuracy of depth estimation. In pursuit of this, we introduce a novel architecture, based on transformers, that segments the depth range into adjustable bins, with each bin's central value being determined on a per-image basis. The ultimate depth values are then calculated as a linear amalgamation of these bin centers. This innovative component is designated as AdaBins. Our findings demonstrate a significant enhancement over existing methods across various depth datasets and all evaluation metrics. The efficacy of the proposed block is corroborated through an ablation study, and we furnish the source code alongside the pre-trained weights of our cutting-edge model."}
{"text": "Time-lagged autoencoders (TAEs) have emerged as a promising deep learning technique for identifying slow modes in complex dynamical systems through regression. Despite their potential, a comprehensive evaluation of nonlinear TAEs is currently absent. This study aims to explore the strengths and weaknesses of TAEs through a combination of theoretical and empirical investigations. Theoretically, we establish performance limits for nonlinear TAEs in the context of slow mode detection, demonstrating that TAEs typically learn a blend of slow and high-variance modes. Through numerical experiments, we highlight scenarios in which TAEs either accurately pinpoint or fail to recognize the dominant slowest mode in two distinct systems: a 2D \"Washington beltway\" potential and the alanine dipeptide molecule submerged in explicit water. Furthermore, we contrast the outcomes of TAEs with those achieved by state-free reversible VAMPnets (SRVs), a variational neural network method for slow mode discovery, revealing that SRVs are capable of accurately uncovering slow modes in instances where TAEs prove inadequate."}
{"text": "Two novel evaluation metrics for generative models in the class-conditional image generation context are introduced. These metrics are derived by extending the widely-used unconditional metrics: the Inception Score (IS) and the Fre'chet Inception Distance (FID). A theoretical examination elucidates the rationale for each proposed metric and establishes a connection to their unconditional equivalents, manifesting as a product for IS and an upper limit for FID. An exhaustive empirical assessment is conducted, juxtaposing the new metrics with their unconditional versions and alternative metrics, and employing them to scrutinize existing generative models. This analysis yields deeper insights into model performance, encompassing issues from unlearned classes to mode collapse."}
{"text": "The growing accessibility of electronic health records (EHR) has facilitated extensive research into a multitude of medical inquiries. A critical aspect of utilizing EHR data is the selection of appropriate cohorts, particularly for the investigation of rare diseases, where the scarcity of records can significantly undermine the reliability of analyses. To mitigate this challenge, data augmentation techniques have been effectively employed in various fields, primarily through the generation of simulated records. This paper introduces ODVICE, an innovative data augmentation framework that harnesses the structure of medical concept ontologies to systematically enhance datasets through a unique ontologically guided Monte-Carlo graph spanning algorithm. The framework empowers users with a set of interactive parameters to tailor the augmentation process according to their specific needs. The efficacy of ODVICE is evaluated through experiments on the MIMIC-III dataset, focusing on two distinct learning tasks. The outcomes reveal a substantial enhancement in predictive performance, with an approximate 30% increase in the area under the curve (AUC) compared to non-augmented datasets and alternative augmentation methods."}
{"text": "This paper's objective is to present two versatile regularization techniques that directly alter weight matrices. The first technique, Weight Reinitialization, employs a simplified Bayesian framework by partially resetting a sparse set of parameters. The second method, Weight Shuffling, adds a non-white noise that is entropy- and weight distribution-invariant to the parameters, which can also be viewed as an ensemble strategy. The efficacy of these proposed methods is assessed on standard datasets including MNIST, CIFAR-10, and the JSB Chorales database, as well as on time series modeling tasks. Improvements in both performance and network entropy are reported. The code for these methods is accessible through a GitHub repository (https://github.com/rpatrik96/lod-wmm-2019)."}
{"text": "In various fields including healthcare, environmental science, economics, and others, multivariate time series data are often encountered in a sparse and irregularly sampled format. The majority of contemporary methodologies concentrate on executing classification, regression, or prediction operations on these datasets. When it comes to prediction, it is crucial not just to estimate the correct value, but also to pinpoint the moment in the irregular time series when this value will materialize. This study introduces a novel method capable of not only predicting the values themselves, but also the precise time points at which these values are anticipated to manifest."}
{"text": "Recent advancements in image captioning models have predominantly harnessed Convolutional Neural Network (CNN) features to generate textual descriptions through recurrent neural networks. To enrich the semantic understanding of these models, scene graphs, which encapsulate object entities, relationships, and attributes, have been integrated. However, the direct incorporation of scene graphs generated by a black-box system has been shown to degrade captioning performance, and existing models that utilize scene graphs often require the additional processing of image features to maintain caption quality. In response to these limitations, we introduce \\textbf, a novel framework that achieves competitive image captioning performance using solely scene graph labels. The core concept is to bridge the semantic discrepancy between the scene graph extracted from the input image and the one inferred from the corresponding caption. This is accomplished by incorporating the spatial positioning of objects and Human-Object-Interaction (HOI) labels into an auxiliary HOI graph. Our proposed SG2Caps framework significantly outperforms current scene graph-based captioning models, underscoring the potential of scene graphs as a valuable representation for enhancing image captioning. By directly utilizing scene graph labels, the need for computationally intensive graph convolutions over high-dimensional CNN features is obviated, leading to a 49% reduction in trainable parameters. The source code for SG2Caps is accessible at: https://github.com/Kien085/SG2Caps."}
{"text": "The utilization of fine-resolution remotely sensed images for semantic segmentation is pivotal in various practical domains, including urban planning, environmental conservation, and the monitoring of both natural and human-altered landscapes. Despite its significance, automating semantic segmentation, which involves the automatic classification and segmentation of images, remains a formidable challenge, especially when dealing with the intricate spatial and spectral complexities inherent in high-resolution imagery. Tackling this issue opens up a promising research avenue, facilitating advanced scene-level analysis of landscape patterns and informed decision-making. This work introduces an automated land segmentation methodology anchored in the Feature Pyramid Network (FPN), a well-established architecture that excels in constructing a feature pyramid rich in high-level semantics. Nonetheless, limitations in feature extraction and integration restrict FPN's capacity to accumulate more distinctive features. To surmount these limitations, we introduce the Attention Aggregation Module (AAM), designed to refine multi-scale feature learning through attention-driven feature amalgamation. Integrating AAM with FPN, we have developed the Attention Aggregation Feature Pyramid Network (A2-FPN), a groundbreaking framework for the semantic segmentation of fine-resolution remotely sensed images. Comprehensive evaluations on three datasets affirm the superior segmentation accuracy of our A2-FPN approach. The source code for A2-FPN is accessible at https://github.com/lironui/A2-FPN."}
{"text": "Enhancing the efficiency of data utilization is a pivotal challenge in the field of reinforcement learning (RL), and CURL, an algorithm that employs contrastive learning to distill high-level features from the raw pixel data of individual video frames, has demonstrated notable effectiveness~\\citep. However, we identify that CURL processes each frame independently, despite the inherent high correlation between consecutive frames in gaming scenarios. To address this limitation and enhance data efficiency, we introduce a novel approach, termed masked contrastive representation learning for RL, which explicitly accounts for the interdependencies among sequential inputs. Our method builds upon CURL's architecture, which includes a CNN encoder and a policy network, by incorporating an additional Transformer module designed to exploit the correlations across video frames. In the training phase, we selectively mask the features of certain frames and employ the CNN encoder in conjunction with the Transformer to infer these masked features based on the surrounding context frames. The CNN encoder and Transformer are co-trained through contrastive learning, ensuring that the reconstructed features are closely aligned with the ground-truth features while being distinct from others. For inference, actions are determined using the CNN encoder and the policy network, with the Transformer module being excluded from this process. Our proposed method outperforms CURL consistently across 14 of 16 environments from the DMControl suite and 21 of 26 environments from Atari 2600 Games. The source code for our implementation is accessible at https://github.com/teslacool/m-curl."}
{"text": "In recent times, there has been a growing fascination with semi-supervised classification techniques that incorporate graphical data. This has led to the development of a novel category of learning models, which fundamentally hinge on the application of a graph convolution prior to classifying the data. To evaluate the efficacy of this methodology, we delve into the classification of a Gaussian mixture, where the dataset is represented by the node attributes within a stochastic block model. Our findings reveal that graph convolution significantly broadens the scope for linear separability by approximately 1/, with D denoting the anticipated node degree, in comparison to the standalone mixture model data. Additionally, we observe that the linear classifier, derived by minimizing the cross-entropy loss post graph convolution, exhibits superior generalization capabilities. This is particularly evident when dealing with out-of-distribution data, characterized by distinct intra- and inter-class edge probabilities that diverge from those observed in the training dataset."}
{"text": "A novel variational technique for multi-view shape-from-shading under natural lighting conditions is presented. The central concept involves integrating Partial Differential Equation (PDE)-based solutions for single-image shape-from-shading tasks across various images and color channels through a variational framework. Contrary to the traditional approach of alternately addressing each shape-from-shading problem and then optimizing inter-image and inter-channel consistency, which can result in suboptimal outcomes, we advocate for a direct solution to the combined problem using an Alternating Direction Method of Multipliers (ADMM) algorithm. Extensive experimental evaluations on both synthetic and real-world images showcase the efficacy of our proposed method, which merges multi-view reconstruction with shape-from-shading to yield highly precise, dense reconstructions without the necessity for dense correspondence calculations. This variational amalgamation across multiple views enables shape-from-shading methodologies to tackle complex real-world reconstruction challenges, producing exquisitely detailed geometries even in regions characterized by smooth brightness transitions and minimal texture."}
{"text": "The emergence of deep learning has revolutionized image classification, enabling the development of intricate discriminative models. Nonetheless, these sophisticated models necessitate a vast collection of labeled data to ensure effective generalization during training. In the realm of medical image analysis, this poses a significant challenge due to the scarcity of training data, leading to a common issue of overfitting in classification models. This study introduces and explores a novel classifier designed to enhance generalization capabilities, particularly when faced with a limited dataset. Inspired by reinforcement learning principles, the proposed classifier innovates by incorporating generalization feedback from a portion of the training data to refine its parameters, diverging from the exclusive reliance on conventional cross-entropy loss. To validate the efficacy of this approach, the classifier is applied to three distinct classification tasks, benchmarked against standard deep learning classifiers augmented with current overfitting prevention strategies. The results demonstrate not only an overall enhancement in classification accuracy but also a notable aptitude for generalized learning, holding substantial promise for medical classification applications."}
{"text": "FPN-based detectors have demonstrated substantial advancements in general object detection tasks, notably on benchmarks such as MS COCO and PASCAL VOC. Nonetheless, they exhibit limitations in specialized scenarios, particularly in the detection of very small objects. This paper posits that the top-down connections within the Feature Pyramid Network (FPN) architecture have dual implications for tiny object detection, not exclusively beneficial. To address this, we introduce the concept of a fusion factor, a parameter designed to regulate the information flow from deeper to shallower layers, thereby optimizing FPN for the detection of tiny objects. Through a series of experiments and analyses, we develop a statistical method to determine an effective fusion factor value for a given dataset, which is contingent upon the distribution of object counts across different layers. Extensive experimentation on datasets tailored for tiny object detection, such as TinyPerson and Tiny CityPersons, corroborates our approach. The findings reveal that by appropriately setting the fusion factor, the network can markedly outperform the baseline on tiny object detection datasets. The source codes and models will be made publicly available."}
{"text": "This work introduces a new image descriptor, termed face X-ray, aimed at identifying forgeries in facial images. The face X-ray representation is a grayscale image that discloses whether the input image is a composite of two distinct sources by visualizing the blending boundary in manipulated images, while genuine images exhibit no such boundary. We note that a common practice in current face manipulation techniques is the blending of the modified face with a pre-existing background image. Consequently, the face X-ray serves as a potent tool for discerning forgeries produced by the majority of contemporary face manipulation algorithms. The strength of the face X-ray lies in its generality, as it merely presupposes the presence of a blending operation and does not hinge on the specific artifacts of any particular manipulation method. Remarkably, the algorithm for generating the face X-ray can be trained without requiring any fake images produced by cutting-edge face manipulation approaches. Comprehensive evaluations demonstrate that the face X-ray maintains its efficacy when applied to forgeries created by unfamiliar manipulation techniques, a capability that eludes most existing face forgery detection and deepfake detection algorithms, which typically witness a notable decline in performance."}
{"text": "The widespread adoption of attention mechanisms in deep learning has sparked debates regarding the interpretability of the attention weights they generate. Despite offering insights into model operations, using attention as a direct explanation for predictions remains controversial. The research community is actively pursuing more transparent methods to accurately pinpoint the local active regions that significantly influence the final decision. In response, we introduce a new Bilinear Representative Non-Parametric Attention (BR-NPA) approach aimed at enhancing the interpretability of attention models by capturing task-relevant, human-understandable information. Initially, the target model is refined to generate higher-resolution intermediate feature maps. Subsequently, these features are clustered based on their local pairwise similarity, leading to the creation of more detailed and accurate attention maps that emphasize the input's task-relevant sections. The attention maps are prioritized according to the 'activity level' of the composite feature, indicating the significance of the highlighted areas. Our proposed model is versatile, compatible with a broad range of contemporary deep learning architectures involving classification tasks. It outperforms conventional neural attention modules in terms of accuracy, speed, and memory efficiency. Extensive experimental evaluations demonstrate superior visual explanations compared to leading visualization models across various tasks, such as few-shot classification, person re-identification, and fine-grained image classification. This innovative visualization model provides crucial insights into how neural networks allocate attention differently across diverse tasks."}
{"text": "In this study, we conduct an experimental exploration of various feature engineering methodologies, designed to be employed alongside a Convolutional Neural Network (CNN), for the classification of electroencephalogram (EEG) signals into eyes-open or eyes-closed states, utilizing the Bonn dataset. Our approach involves the application of Takens' embedding technique to create geometric representations of the EEG time-series, from which simplicial complexes are derived. We proceed to evaluate and contrast the \\epsilon-series of Betti-numbers and \\epsilon-series of graph spectra, both topological invariants that characterize the underlying geometry of these complexes, against the raw EEG time series. This comparison is aimed at addressing a gap in the literature concerning benchmarking techniques. Motivated by Topological Data Analysis, these methods are utilized for feature engineering to encapsulate the local geometry of the time-series data. Furthermore, we assess the resilience of these feature engineering pipelines to downsampling and data reduction, with the objective of this paper being to set clearer benchmarks for time-series classification using geometric features, as well as to understand the performance of CNNs with time-series data of reduced resolution."}
{"text": "Cervical spondylosis (CS), a widespread chronic condition affecting approximately two-thirds of the population, imposes a substantial burden on both individuals and society. Early detection is crucial for enhancing treatment efficacy and minimizing healthcare expenses. Yet, the intricate nature of the disease, coupled with its subtle symptoms in the initial stages, complicates early diagnosis. Moreover, the high costs and time demands of conventional medical evaluations deter proactive screening for CS. Consequently, there is a pressing need for a user-friendly, affordable, and intelligent method for CS identification. This paper introduces a novel deep learning-based approach for diagnosing CS using surface electromyography (sEMG) signals. Addressing the sEMG signal's complexity, high dimensionality, and limited usability, we have devised a multi-channel EasiCSDeep algorithm, anchored in convolutional neural networks. This algorithm integrates feature extraction, spatial relationship modeling, and classification. To our knowledge, EasiCSDeep represents the pioneering application of deep learning and sEMG data for CS identification. Our method outperforms existing state-of-the-art algorithms, marking a significant advancement in the field."}
{"text": "The natural gradient boasts a desirable characteristic: its invariance under arbitrary differentiable reparameterizations of the model. Yet, this property holds true only in the limit of infinitesimally small steps, a condition that is not met in practical applications where finite step sizes are employed. In this work, we explore invariance properties by merging insights from Riemannian geometry and the numerical resolution of differential equations. We introduce the concept of the order of invariance for a numerical method, defined as its rate of convergence towards a solution that is invariant. To achieve more invariant optimization paths, we advocate for the utilization of higher-order integrators and adjustments for geodesic deviations. We establish the numerical convergence attributes of updates with geodesic corrections and illustrate that these can match the computational efficiency of the standard natural gradient. Through empirical evaluations, we showcase that enhanced invariance accelerates optimization, and our methodologies outperform conventional natural gradient techniques in the training of deep neural networks and natural policy gradient for reinforcement learning scenarios."}
{"text": "The objective of space-time video super-resolution (STVSR) is to enhance the spatial and temporal qualities of videos with low resolution and frame rates. Recent approaches based on deformable convolution have shown significant promise in STVSR, yet they are limited to interpolating frames that are pre-specified during the training phase and overlook the valuable short-term motion information between neighboring frames. In this work, we introduce the Temporal Modulation Network (TMNet), designed to accurately reconstruct high-resolution, arbitrary intermediate frames. Central to our method is the Temporal Modulation Block (TMB), which enables controlled feature interpolation by modulating deformable convolution kernels. To fully leverage temporal data, we incorporate a Locally-temporal Feature Comparison (LFC) module and Bi-directional Deformable ConvLSTM, which together capture both short-term and long-term motion cues within the video sequence. Our TMNet's superiority over existing STVSR techniques is substantiated through evaluations on three standard datasets. The source code for TMNet is accessible at https://github.com/CS-GangXu/TMNet."}
{"text": "Dropout, a prevalent method for training deep neural networks, is conventionally employed to prevent overfitting in large models by promoting regularization. Its efficacy has been attributed to the prevention of co-adaptation among nodes. Yet, when scrutinizing the correlations between nodes post-training, with and without dropout, doubts emerge regarding whether co-adaptation avoidance fully elucidates the mechanism of dropout. This paper introduces an alternative rationale for the effectiveness of dropout and presents a novel strategy for enhancing activation functions. We demonstrate that dropout can be interpreted as a technique that optimizes the input to gravitate towards the saturation region of the activation function, thereby facilitating the propagation of gradient information even within this saturated zone during backpropagation. Inspired by this insight, we introduce a new approach for activation functions, designed to expedite gradient flow in the saturation area. Consequently, the input to the activation function is encouraged to ascend into the saturation region, leading to a more resilient network as the model stabilizes in a flatter region. Empirical findings validate our interpretation of dropout's function and substantiate the enhancement of image classification performance through the proposed Gradient Accelerated Activation Function (GAAF) technique, as anticipated."}
{"text": "The remarkable achievements of deep learning in complex perceptual tasks have led to its widespread integration into software for delivering intelligent services. Nonetheless, the susceptibility of deep neural networks to adversarial examples has been a critical concern, prompting the development of numerous defense strategies. Unfortunately, these defenses are often tailored to specific attacks or have proven ineffective against new threats. Moreover, many existing defense mechanisms are overly complex, leading to excessive computational overhead and latency, making them unsuitable for practical deployment in real-world software. In response to these challenges, we introduce DAFAR, a feedback framework designed to enhance the robustness of deep learning models against adversarial examples with high efficacy and broad applicability, while maintaining low resource consumption and latency. DAFAR's architecture is straightforward, comprising a victim model, a feedback network that can be seamlessly integrated, and a detector. The core concept is to leverage high-level features extracted from the victim model's layers to reconstruct the input within the feedback network, creating a feedback autoencoder. This approach transforms subtle attacks on the victim model into more detectable reconstruction errors on the feedback autoencoder for potent attacks, and for weaker attacks, it disrupts the structure of adversarial examples during the reconstruction process. Our experimental evaluation on the MNIST and CIFAR-10 datasets demonstrates that DAFAR effectively counters a range of popular and advanced attacks, without compromising the performance on genuine samples, showcasing its high efficacy and universality across various attack methodologies and parameters."}
{"text": "Our contribution involves a novel approach to unsupervised video object segmentation, achieved by leveraging the knowledge embedded within image-based instance embedding networks. These networks generate an embedding vector for every pixel, facilitating the identification of pixels that belong to the same object. Despite being trained on static images, the instance embeddings exhibit temporal stability across sequential video frames, enabling the temporal linking of objects. Consequently, we repurpose instance networks, originally trained on static images, for video object segmentation tasks. This is accomplished by integrating the embeddings with objectness and optical flow features, without necessitating model retraining or online fine-tuning. Our method surpasses existing state-of-the-art unsupervised segmentation techniques, as demonstrated by its superior performance on the DAVIS and FBMS datasets."}
{"text": "Reward shaping, a technique that integrates domain expertise into reinforcement learning (RL), has proven to be effective. Typically, methods like potential-based reward shaping leverage a predefined shaping reward function extensively. Yet, the conversion of human knowledge into numerical rewards can be flawed due to factors like cognitive biases, potentially hindering the enhancement of RL algorithm performance. This paper addresses the challenge of dynamically utilizing a provided shaping reward function. We cast the use of shaping rewards as a bi-level optimization problem: the lower level optimizes the policy using shaping rewards, while the upper level optimizes a parameterized shaping weight function to maximize the true reward. We derive the gradient of the expected true reward concerning the parameters of the shaping weight function and introduce three learning algorithms based on varying assumptions. Experimental results in sparse-reward cartpole and MuJoCo environments demonstrate that our algorithms can effectively harness advantageous shaping rewards, while simultaneously disregarding or converting detrimental shaping rewards into advantageous ones."}
{"text": "Normalizing Flows (NFs) excel in modeling complex distributions p(y) characterized by significant inter-dimensional correlations and high multimodality, achieved by transforming a straightforward base density p(z) via an invertible neural network, leveraging the change of variables formula. This capability is particularly advantageous in multivariate structured prediction tasks, where traditional per-pixel loss-based approaches fall short in accurately capturing the strong interdependencies among output dimensions. This study focuses on Conditional Normalizing Flows (CNFs), a variant of NFs where the transformation from base density to output space is conditioned on an input x, enabling the modeling of conditional densities p(y|x). CNFs offer efficient sampling and inference, can be trained using a likelihood-based objective, and, as generative flows, are immune to issues like mode collapse and training instabilities that plague other models. We introduce a robust training methodology for continuous CNFs tailored to binary problems, and specifically, we apply these CNFs to super-resolution and vessel segmentation tasks. Our results showcase competitive performance on established benchmark datasets, as measured by likelihood and conventional evaluation metrics."}
{"text": "A multitude of applications in the domains of robotics and human-computer interaction stand to gain significantly from the accurate comprehension of 3D motion of points within dynamic environments, a problem commonly referred to as scene flow estimation. Despite the predominance of stereo and RGB-D images as input modalities in existing methodologies, there is a notable scarcity of approaches that directly tackle scene flow estimation from point clouds. This research introduces a groundbreaking deep neural network, coined FlowNet3D, which is designed to learn scene flow directly from point clouds in a holistic, end-to-end manner. The network's architecture is characterized by its dual capability to learn deep hierarchical features of point clouds alongside flow embeddings, which encapsulate the motion of individual points. This dual learning is facilitated by the incorporation of two innovative learning layers specifically tailored for point sets. The efficacy of our proposed network is rigorously assessed on both complex synthetic data sourced from the FlyingThings3D dataset and real-world Lidar scans obtained from the KITTI dataset. Notably, when trained exclusively on synthetic data, our network exhibits a remarkable ability to generalize to real-world scans, surpassing a variety of baseline methods and demonstrating performance on par with state-of-the-art techniques. To further underscore the versatility and potential of our scene flow output, we showcase its application in two key areas: scan registration and motion segmentation, thereby illustrating its broad applicability across multiple use cases."}
{"text": "Aiming to achieve dense 3D reconstructions at a reduced computational expense, a key objective in the SLAM domain, this paper introduces a novel framework. Our approach integrates semi-dense SLAM with Multispectral Photometric Stereo techniques to construct detailed 3D models from monocular multispectral video inputs. The proposed method, SALM, initially generates a semi-dense 3D structure that is later refined for increased density. It also acquires a sparse depth map, which serves as an input for the optimization-based multispectral photometric stereo process, enhancing the accuracy of the dense surface normal recovery. Additionally, SALM determines the camera pose, which is essential for the transformation of views during the fusion phase. Here, the sparse point cloud and the dense surface normals are merged using an automated cross-scale fusion technique, developed in this work, to produce a dense point cloud enriched with fine-grained texture details. Experimental results validate the efficacy of our methodology in generating more detailed 3D reconstructions."}
{"text": "The proliferation of Generative Adversarial Networks (GANs) has witnessed a remarkable surge in recent times, with their most notable influence being in the realm of computer vision, where they have revolutionized the creation and manipulation of images and videos. Beyond this domain, GANs have expanded their reach, finding applications in diverse areas such as time series and sequence generation. This paper explores the burgeoning field of GANs applied to time series data, where efforts are concentrated on generating high-quality, varied, and secure datasets. A comprehensive review of GAN variants tailored for time series tasks is presented, alongside a classification scheme that distinguishes between discrete and continuous time series GANs. The review highlights recent and prominent studies, detailing their architectural designs, outcomes, and practical uses. Additionally, a compilation of prevalent evaluation metrics and their applicability across different scenarios is provided. The paper also delves into privacy considerations for GANs, exploring measures to safeguard sensitive data and suggesting future research directions. The objective is to succinctly encapsulate the cutting-edge research in this domain and its implications for real-world technological advancements."}
{"text": "Monocular 3D object detection, a critical component for autonomous driving due to its cost-effectiveness, presents a more formidable challenge than traditional 2D detection due to the inherent ambiguity associated with the absence of depth information. Despite the advancements in 2D detection, transitioning a generic 2D detector to handle 3D tasks is not straightforward. This paper addresses this issue by constructing a practical approach on a fully convolutional, single-stage detector, leading to the development of a comprehensive framework termed FCOS3D. Our methodology involves translating the standard 7-DoF 3D object definitions into the image domain, subsequently separating them into 2D and 3D attributes. Objects are then allocated to various feature levels based on their 2D scales, with training assignments made solely according to the projected 3D-center. Additionally, we redefine the concept of center-ness using a 2D Gaussian distribution centered on the 3D-center, aligning with the 3D target representation. This framework is straightforward yet potent, eschewing reliance on 2D detection or 2D-3D correspondence heuristics. Our proposed solution secured the top position among vision-only methods in the nuScenes 3D detection challenge at NeurIPS 2020. The code and models are publicly available at https://github.com/open-mmlab/mmdetection3d."}
{"text": "Video feedback serves as a comprehensive source of information for surgical operations and is the primary sensory input for surgeons, playing a pivotal role in computer-assisted interventions (CAI) and post-operative evaluations. A critical component of these systems is the ability to accurately identify and locate surgical instruments and anatomical features through semantic segmentation. While deep learning has significantly enhanced semantic segmentation methods in recent times, it fundamentally depends on the existence of annotated datasets for model training. This work presents a dataset designed for the semantic segmentation of videos from cataract surgeries. The annotated images are sourced from the publicly accessible CATARACTS challenge dataset. Furthermore, we evaluate the performance of various cutting-edge deep learning models for semantic segmentation on this dataset. The dataset can be accessed at https://cataracts.grand-challenge.org/CaDIS/ ."}
{"text": "Teaching holds a pivotal position in society, serving as a conduit for disseminating knowledge and nurturing future generations. An effective teacher tailors their approach by selecting suitable educational resources, employing fitting pedagogical methods, and designing targeted assessments, all in response to the learning dynamics of their students. Yet, within the realm of artificial intelligence, the significance of teaching has not been fully appreciated, with the majority of focus directed towards the learning capabilities of machines. This paper posits that teaching deserves equal, if not greater, consideration and advocates for the adoption of an optimization framework, rather than heuristic methods, to derive effective teaching strategies. We term this novel approach 'learning to teach', which involves an interactive process between two intelligent entities: a student model, akin to the learner in conventional machine learning algorithms, and a teacher model, tasked with determining the optimal data, loss function, and hypothesis space to enhance the student model's learning. The teacher model refines its teaching strategies through reinforcement learning, utilizing feedback from the student model, fostering a co-evolutionary relationship between teacher and student. To substantiate the practical efficacy of our proposed methodology, we utilize the training of deep neural networks (DNNs) as a case study. We illustrate that by harnessing learning to teach techniques, it is possible to achieve comparable accuracy with significantly reduced training data and fewer iterations for a variety of DNN models, encompassing multi-layer perceptrons, convolutional neural networks, and recurrent neural networks, across diverse machine learning tasks such as image classification and text comprehension."}
{"text": "Inspired by the human practice of targeted skill enhancement, we introduce an innovative adversarial sampling strategy, guided by a failure prediction model termed \"CoachNet\", to address the limitations of reinforcement learning in terms of sample efficiency and performance consistency. CoachNet, co-trained with the agent, forecasts the likelihood of failure in given scenarios. This prediction is utilized in a probabilistic sampling mechanism to steer the agent towards more demanding and less mastered situations. By doing so, the learning process is optimized to concentrate on the agent's areas of weakness, rather than repeatedly practicing already mastered skills. The design, theoretical foundations, and practical efficacy of CoachNet in boosting sample efficiency and enhancing robustness during testing are showcased through empirical evaluations in standard continuous control environments."}
